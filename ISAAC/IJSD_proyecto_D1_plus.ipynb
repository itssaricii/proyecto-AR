{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUehXgCyIRdq"
      },
      "source": [
        "# Actividad - Proyecto práctico\n",
        "\n",
        "\n",
        "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
        "*   Alumno 1: de Antón Santiago, Sara\n",
        "*   Alumno 2: Sánchez La O, Benjamín C.\n",
        "*   Alumno 3: Sánchez Díaz, Isaac José\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwpYlnjWJhS9"
      },
      "source": [
        "---\n",
        "## **PARTE 1** - Instalación y requisitos previos\n",
        "\n",
        "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
        "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
        "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
        "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
        "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU2BPrK2JkP0"
      },
      "source": [
        "---\n",
        "### 1.1. Preparar enviroment (solo local)\n",
        "\n",
        "\n",
        "\n",
        "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
        "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
        "2. Instalar Anaconda\n",
        "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
        "\n",
        "\n",
        "```\n",
        "conda create --name miar_rl python=3.8\n",
        "conda activate miar_rl\n",
        "cd \"PATH_TO_FOLDER\"\n",
        "conda install git\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "\n",
        "4. Abrir la notebook con *jupyter-notebook*.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "jupyter-notebook\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-kixNPiJqTc"
      },
      "source": [
        "---\n",
        "### 1.2. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S_YDFwZ-JscI",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dp_a1iBJ0tf"
      },
      "source": [
        "---\n",
        "### 1.3. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6n7MIefJ21i",
        "outputId": "bce44695-c427-4ed6-b1c1-a0df92594a1d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "/content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "Archivos en el directorio: \n",
            "['models']\n"
          ]
        }
      ],
      "source": [
        "# Switch to the directory on the Google Drive that you want to use\n",
        "import os\n",
        "if IN_COLAB:\n",
        "  print(\"We're running Colab\")\n",
        "\n",
        "  if IN_COLAB:\n",
        "    # Mount the Google Drive at mount\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "      os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "# Verify we're in the correct working directory\n",
        "%pwd\n",
        "print(\"Archivos en el directorio: \")\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ZSL5bpJ560"
      },
      "source": [
        "---\n",
        "### 1.4. Instalar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UbVRjvHCJ8UF",
        "outputId": "8bee6aeb-0bde-457b-dd3a-0761bb830498",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gym==0.17.3\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (2.0.2)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
            "Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=ca109cf0abb6f7bb0aa1926f17659222e2863f152c8cccbe192ff3a74399b8b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/8b/b7/570cb90b10f17e85ccb291ba1f04af41ec697745104a2263eb\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, cloudpickle, gym\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.1\n",
            "    Uninstalling cloudpickle-3.1.1:\n",
            "      Successfully uninstalled cloudpickle-3.1.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "distributed 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "bigframes 2.6.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "dask 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n",
            "Collecting git+https://github.com/Kojoley/atari-py.git\n",
            "  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-ip_kiwml\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-ip_kiwml\n",
            "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from atari-py==1.2.2) (2.0.2)\n",
            "Building wheels for collected packages: atari-py\n",
            "  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atari-py: filename=atari_py-1.2.2-cp311-cp311-linux_x86_64.whl size=4738734 sha256=433fdc30a4117f9973381b18ce3bdbf12a664dbd6c7183816c836a5a5b847b8c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-70y04z4m/wheels/1a/58/b3/3baab9d1509939ecce2dfd9ca349c222b7ee6590f4bd6097a1\n",
            "Successfully built atari-py\n",
            "Installing collected packages: atari-py\n",
            "Successfully installed atari-py-1.2.2\n",
            "Collecting keras-rl2==1.0.5\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl.metadata (304 bytes)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from keras-rl2==1.0.5) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->keras-rl2==1.0.5) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->keras-rl2==1.0.5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->keras-rl2==1.0.5) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->keras-rl2==1.0.5) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (0.1.2)\n",
            "Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n",
            "Collecting tensorflow==2.12\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.73.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.5.2)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.14.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.2,>=0.6.2 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.1,>=0.6.1 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "orbax-checkpoint 0.11.15 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "bigframes 2.6.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "bigframes 2.6.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.4.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 protobuf-4.25.8 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "465c88e2998345eaacd8b679612498ea",
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# NO EJECUTAR EN SAGEMAKER (MIRAR EL README PARA CONFIGURAR EL ENTORNO)\n",
        "# AUNQUE DE FALLOS DE INSTALACIÓN LOS IMPORTS FUNCIONAN\n",
        "if IN_COLAB:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install tensorflow==2.12  #2.8\n",
        "else:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install pyglet==1.5.0\n",
        "  %pip install h5py==3.1.0\n",
        "  %pip install Pillow==9.5.0\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install Keras==2.2.4\n",
        "  %pip install tensorflow==2.5.3\n",
        "  %pip install torch==2.0.1\n",
        "  %pip install agents==1.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hzP_5ZuGb2X"
      },
      "source": [
        "---\n",
        "## **PARTE 2**. Enunciado\n",
        "\n",
        "Consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
        "\n",
        "Este proyecto práctico consta de tres partes:\n",
        "\n",
        "1.   Implementar la red neuronal que se usará en la solución\n",
        "2.   Implementar las distintas piezas de la solución DQN\n",
        "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
        "\n",
        "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
        "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
        "* Cada alumno deberá de subir la solución de forma individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_b3mzw8IzJP"
      },
      "source": [
        "---\n",
        "## **PARTE 3**. Desarrollo y preguntas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "45BPUdIoB41o",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# DEFINIR AL PRINCIPIO\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duPmUNOVGb2a"
      },
      "source": [
        "#### Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j3eRhgI-Gb2a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "# AÑADIDO\n",
        "from tensorflow.keras.layers import Lambda, BatchNormalization\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import pickle\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4jgQjzoGb2a"
      },
      "source": [
        "#### Configuración base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwOE6I_KGb2a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9jGEZUcpGb2a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LVgXEu006eFy"
      },
      "outputs": [],
      "source": [
        "hiperparametros = {\n",
        "    \"MEMORY_SIZE\": 400000,        # 33% más que antes\n",
        "    \"WARMUP_STEPS\": 50000,        # Más exploración inicial\n",
        "    \"SCHEDULER_STEPS\": 1500000,   # Decaimiento más lento de ε\n",
        "    \"GAMMA\": 0.96,                # Balance recompensas inmediatas/medias\n",
        "    \"MODEL_UPDATE\": 3000,         # Target network se actualiza más frecuente\n",
        "    \"LEARNING_RATE\": 0.00015,     # LR más bajo pero con mejor inicialización\n",
        "    \"MODEL_CHECKPOINT_STEPS\": 20000,\n",
        "    \"TRAIN_STEPS\": 2000000,       # 10% más de entrenamiento\n",
        "    \"LOG_INTERVAL\": 5000,\n",
        "    \"DELTA_CLIP\": 0.5            # Gradient clipping más estricto\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WRL-ixOyPIo7"
      },
      "outputs": [],
      "source": [
        "# ROOT PATH PARA LOS MODELOS Y SUS PESOS\n",
        "if IN_COLAB:\n",
        "  mount='/content/gdrive'\n",
        "  drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "  MODELS_DIR=drive_root+\"/models\"\n",
        "else:\n",
        "  MODELS_DIR=\"./models\"\n",
        "\n",
        "def get_dirs(model_name=\"modelo1\"):\n",
        "    WEIGHTS_DIR = os.path.join(MODELS_DIR, model_name, \"weights\")\n",
        "    CHECKPOINTS_DIR = os.path.join(MODELS_DIR, model_name, \"checkpoints\")\n",
        "    MODEL_DIR = os.path.join(MODELS_DIR, model_name)\n",
        "    os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
        "    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
        "    return MODEL_DIR, WEIGHTS_DIR, CHECKPOINTS_DIR\n",
        "\n",
        "def save_hyperparams(modelo):\n",
        "    \"\"\"\n",
        "    Guarda los hiperparámetros actuales en el fichero JSON.\n",
        "    \"\"\"\n",
        "    hyper_file = os.path.join(MODELS_DIR, modelo, modelo + '.json')\n",
        "    with open(hyper_file, 'w') as f:\n",
        "        json.dump(hiperparametros, f, indent=4)\n",
        "    print(f\"[INFO] Hiperparámetros guardados en {hyper_file}\")\n",
        "\n",
        "\n",
        "def load_hyperparams(modelo):\n",
        "    \"\"\"\n",
        "    Actualiza los hiperparámetros definidos en memoria a los del fichero cargado.\n",
        "    Si el fichero no existe, lo crea con los valores por defecto (hiperparametros).\n",
        "    \"\"\"\n",
        "    hyper_file = os.path.join(MODELS_DIR, modelo, modelo + '.json')\n",
        "\n",
        "    # Crear el directorio del modelo si no existe\n",
        "    os.makedirs(os.path.dirname(hyper_file), exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(hyper_file):\n",
        "        # Guardar el fichero con los hiperparámetros por defecto\n",
        "        with open(hyper_file, 'w') as f:\n",
        "            json.dump(hiperparametros, f, indent=4)\n",
        "\n",
        "        print(f\"[INFO] Fichero de hiperparámetros no encontrado. Creado por defecto en: {hyper_file}\")\n",
        "        params = hiperparametros\n",
        "    else:\n",
        "        # El fichero existe: lo leemos\n",
        "        with open(hyper_file, 'r') as f:\n",
        "            params = json.load(f)\n",
        "\n",
        "    # Asignar dinámicamente los valores\n",
        "    for key, value in params.items():\n",
        "        globals()[key] = value\n",
        "        hiperparametros[key] = value  # Actualiza el diccionario en memoria\n",
        "\n",
        "    print(f\"[INFO] Hiperparámetros cargados desde {hyper_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9KxgJ090eku"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN PARA PLOTEAR LOGS DEL TRAINING *** SARA ***\n",
        "'''\n",
        "def graph_training_csv(csv_path, model_dir, model_name, save_clean_csv=False):\n",
        "    if not os.path.isfile(csv_path):\n",
        "        print(f\"[ERROR] El archivo '{csv_path}' no existe.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, usecols=[0, 1, 2])  # Usa encabezados del archivo\n",
        "        df.columns = ['episode_jump', 'episode_reward', 'nb_steps']\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] No se pudo leer el CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    # Ignorar la primera fila de datos (por ejemplo, episodio 1)\n",
        "    df = df.iloc[1:].reset_index(drop=True)\n",
        "\n",
        "    # Convertir nb_steps a entero seguro\n",
        "    df['nb_steps'] = pd.to_numeric(df['nb_steps'], errors='coerce')\n",
        "    df = df.dropna(subset=['nb_steps'])\n",
        "    df['nb_steps'] = df['nb_steps'].astype(int)\n",
        "\n",
        "    fixed_steps = []\n",
        "    accumulated_steps = 0\n",
        "    previous_step = df['nb_steps'].iloc[0]\n",
        "\n",
        "    for s in df['nb_steps']:\n",
        "      if s < previous_step:\n",
        "          accumulated_steps += previous_step\n",
        "          value= s + accumulated_steps\n",
        "      else:\n",
        "          if accumulated_steps == 0:\n",
        "              value = s\n",
        "          else:\n",
        "              value = s+ accumulated_steps\n",
        "      previous_step = s\n",
        "      fixed_steps.append(value)\n",
        "\n",
        "\n",
        "    df['fixed_steps'] = fixed_steps\n",
        "    print(f\"[INFO] Último valor en 'fixed_steps': {fixed_steps[-1]}\")\n",
        "\n",
        "    # Graficar\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(df['fixed_steps'], df['episode_reward'])\n",
        "    plt.title(f\"{model_name}: Episode Reward vs Steps\")\n",
        "    plt.xlabel(\"Steps\")\n",
        "    plt.ylabel(\"Episode Reward\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    output_path = os.path.join(model_dir, f\"{model_name}_episode_reward_fixed_steps.png\")\n",
        "    plt.savefig(output_path)\n",
        "    print(f\"[INFO] Gráfica guardada en: {output_path}\")\n",
        "    plt.show()\n",
        "\n",
        "    if save_clean_csv:\n",
        "        clean_csv_path = os.path.join(model_dir, f\"{model_name}_cleaned_log_fixed.csv\")\n",
        "        df.to_csv(clean_csv_path, index=False)\n",
        "        print(f\"[INFO] CSV corregido guardado en: {clean_csv_path}\")\n",
        "  '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Gd1JuAi9rCqr"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN PARA PLOTEAR LOGS DEL TRAINING *** BENJAMIN ***\n",
        "def analyze_training(model_name, window_size):\n",
        "    \"\"\"\n",
        "    Analiza el log de entrenamiento de un modelo de RL, genera gráficos de evolución\n",
        "    de métricas y un informe textual. Incluye un informe de métricas globales y otro\n",
        "    centrado en las métricas después de los window_size ultimos episodios.\n",
        "    Los graficos y datos son guardados en MODEL_DIR/graphs por fecha.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): El nombre del modelo.\n",
        "        window_size (int): Tamaño de la ventana para la media móvil.\n",
        "    \"\"\"\n",
        "    MODEL_DIR = os.path.join(\"./models\", model_name)\n",
        "    log_csv_path = os.path.join(MODEL_DIR, f'{model_name}_training_log.csv')\n",
        "\n",
        "    if not os.path.exists(log_csv_path):\n",
        "        print(f\"Error: El archivo de log '{log_csv_path}' no se encontró.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(log_csv_path)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"El DataFrame está vacío, no se puede continuar.\")\n",
        "        return\n",
        "\n",
        "    # Calcular medias móviles\n",
        "    df['reward_smooth'] = df['episode_reward'].rolling(window=window_size).mean()\n",
        "    df['loss_smooth'] = df['loss'].rolling(window=window_size).mean()\n",
        "    df['q_smooth'] = df['mean_q'].rolling(window=window_size).mean()\n",
        "    df['eps_smooth'] = df['mean_eps'].rolling(window=window_size).mean()\n",
        "\n",
        "    # Generar gráficos\n",
        "    _plot_metrics(df, model_name, window_size, MODEL_DIR)\n",
        "\n",
        "    # Informe global\n",
        "    print(\"\\nINFORME DEL TRAINING (Todas las métricas)\\n\" + \"-\" * 40)\n",
        "    _print_report(df)\n",
        "\n",
        "    # Informe últimos episodios\n",
        "    _analyze_training_last(df, model_name, window_size, MODEL_DIR)\n",
        "\n",
        "def _plot_metrics(df, model_name, window_size, model_dir):\n",
        "    sns.set(style=\"darkgrid\", font_scale=1.2)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "    plots_info = [\n",
        "        ('episode_reward', 'reward_smooth', 'Recompensa por episodio', 'Recompensa', axes[0, 0]),\n",
        "        ('loss', 'loss_smooth', 'Pérdida (loss) por episodio', 'Loss', axes[0, 1]),\n",
        "        ('mean_q', 'q_smooth', 'Q medio por episodio', 'Mean Q', axes[1, 0]),\n",
        "        ('mean_eps', 'eps_smooth', 'Epsilon medio por episodio', 'Mean Eps', axes[1, 1]),\n",
        "    ]\n",
        "\n",
        "    for orig_col, smooth_col, title, ylabel, ax in plots_info:\n",
        "        sns.lineplot(x='episode', y=orig_col, data=df, marker='o', markersize=4, label='Original', ax=ax)\n",
        "        sns.lineplot(x='episode', y=smooth_col, data=df, color='red', linewidth=2, label=f'Media móvil ({window_size})', ax=ax)\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel('Episodio')\n",
        "        ax.set_ylabel(ylabel)\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "        unique = dict(zip(labels, handles))\n",
        "        ax.legend(unique.values(), unique.keys(), loc='best')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    graphs_dir = os.path.join(model_dir, \"graphs\")\n",
        "    os.makedirs(graphs_dir, exist_ok=True)\n",
        "\n",
        "    datetime_stamp = pd.to_datetime('today').strftime('%Y%m%d%H%M%S')\n",
        "    graph_path = os.path.join(graphs_dir, f'{datetime_stamp}_{model_name}_training_analyze_graph.png')\n",
        "    csv_path = os.path.join(graphs_dir, f'{datetime_stamp}_{model_name}_training_analyze_log.csv')\n",
        "\n",
        "    plt.savefig(graph_path, dpi=300)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Gráfico guardado en: {graph_path}\")\n",
        "    print(f\"CSV de informe guardado en: {csv_path}\")\n",
        "\n",
        "def _print_report(df):\n",
        "    print(f\"Episodios totales: {df['episode'].max()}\")\n",
        "    print(f\"Recompensa media: {df['episode_reward'].mean():.2f}\")\n",
        "    print(f\"Recompensa máxima: {df['episode_reward'].max()}\")\n",
        "    print(f\"Recompensa mínima: {df['episode_reward'].min()}\")\n",
        "    print(f\"Loss medio: {df['loss'].mean(skipna=True):.6f}\")\n",
        "    print(f\"Mean Q medio: {df['mean_q'].mean(skipna=True):.6f}\")\n",
        "    print(f\"Epsilon medio: {df['mean_eps'].mean(skipna=True):.6f}\")\n",
        "    print(f\"Pasos medios por episodio: {df['nb_steps'].mean():.2f}\")\n",
        "\n",
        "    if len(df) > 1:\n",
        "        reward_diff = df['episode_reward'].iloc[-1] - df['episode_reward'].iloc[0]\n",
        "        if reward_diff > 0:\n",
        "            print(f\"La recompensa final ({df['episode_reward'].iloc[-1]:.2f}) es mayor que la inicial ({df['episode_reward'].iloc[0]:.2f}), indicando una mejora.\")\n",
        "        else:\n",
        "            print(f\"La recompensa final ({df['episode_reward'].iloc[-1]:.2f}) no ha mejorado significativamente respecto a la inicial ({df['episode_reward'].iloc[0]:.2f}).\")\n",
        "    else:\n",
        "        print(\"No hay suficientes episodios para evaluar la evolución de la recompensa global.\")\n",
        "\n",
        "def _analyze_training_last(df_full, model_name, window_size, model_dir):\n",
        "    latest_log_path = _get_latest_log_file(model_dir, model_name)\n",
        "    if not latest_log_path:\n",
        "        print(\"No se encontró un CSV de análisis previo para los últimos episodios.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(latest_log_path)\n",
        "    if df.empty:\n",
        "        print(\"El DataFrame de los últimos episodios está vacío.\")\n",
        "        return\n",
        "\n",
        "    df = df.tail(window_size + 1).iloc[:-1]\n",
        "\n",
        "    print(f\"\\nINFORME DEL TRAINING (últimos {window_size} episodios completados {df['episode'].min()} al {df['episode'].max()})\\n\" + \"-\" * 40)\n",
        "    _print_report(df)\n",
        "\n",
        "def _get_latest_log_file(model_dir, model_name):\n",
        "    pattern = os.path.join(model_dir, \"graphs\", f'*_{model_name}_training_analyze_log.csv')\n",
        "    files = glob.glob(pattern)\n",
        "    if not files:\n",
        "        return None\n",
        "    return max(files, key=os.path.basename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qUOajl31fsW7"
      },
      "outputs": [],
      "source": [
        "# CALLBACK CUSTOM DEL LOGGER PARA PODER GUARDAR TODA LA INFO EN UN MISMO FICHERO YA QUE ANTES SE SOBREESCRIBÍA\n",
        "# BSL: AÑADIDAS NUEVAS MÉTRICAS AL LOGGER\n",
        "class EpisodeLoggerCSV(Callback):\n",
        "    def __init__(self, filepath, verbose=False):\n",
        "        super().__init__()\n",
        "        self.filepath = filepath\n",
        "        self.verbose = verbose\n",
        "        self.fields = [\n",
        "            'episode', 'episode_reward', 'nb_steps', 'duration',\n",
        "            'loss', 'mae', 'mean_q', 'mean_eps', 'ale.lives',\n",
        "            'reward_min', 'reward_max'\n",
        "        ]\n",
        "        self.episode_count = 0\n",
        "        self.file = None\n",
        "        self.writer = None\n",
        "        # Acumuladores por episodio\n",
        "        self._reset_episode_stats()\n",
        "\n",
        "    def _reset_episode_stats(self):\n",
        "        self.losses = []\n",
        "        self.q_values = []\n",
        "        self.maes = []\n",
        "        self.epsilons = []\n",
        "        self.lives = []\n",
        "        self.reward_values = []\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        file_exists = os.path.exists(self.filepath)\n",
        "        self.file = open(self.filepath, mode='a', newline='')\n",
        "        self.writer = csv.DictWriter(self.file, fieldnames=self.fields)\n",
        "        if not file_exists:\n",
        "            self.writer.writeheader()\n",
        "\n",
        "    def on_step_end(self, step, logs=None):\n",
        "        logs = logs or {}\n",
        "        if 'metrics' in logs and logs['metrics'] is not None:\n",
        "            metrics = logs['metrics']\n",
        "            if len(metrics) >= 4:\n",
        "                loss, mae, mean_q, mean_eps = metrics\n",
        "                if not np.isnan(loss):\n",
        "                    self.losses.append(loss)\n",
        "                if not np.isnan(mae):\n",
        "                    self.maes.append(mae)\n",
        "                if not np.isnan(mean_q):\n",
        "                    self.q_values.append(mean_q)\n",
        "                if not np.isnan(mean_eps):\n",
        "                    self.epsilons.append(mean_eps)\n",
        "        if 'reward' in logs:\n",
        "            self.reward_values.append(logs['reward'])\n",
        "        if 'info' in logs and 'ale.lives' in logs['info']:\n",
        "            self.lives.append(float(logs['info']['ale.lives']))\n",
        "\n",
        "    def on_episode_end(self, episode, logs=None):\n",
        "        self.episode_count += 1\n",
        "        logs = logs or {}\n",
        "\n",
        "        row = {\n",
        "            'episode': self.episode_count,\n",
        "            'episode_reward': logs.get('episode_reward'),\n",
        "            'nb_steps': logs.get('nb_steps'),\n",
        "            'duration': logs.get('duration'),\n",
        "            'loss': np.mean(self.losses) if self.losses else None,\n",
        "            'mae': np.mean(self.maes) if self.maes else None,\n",
        "            'mean_q': np.mean(self.q_values) if self.q_values else None,\n",
        "            'mean_eps': np.mean(self.epsilons) if self.epsilons else None,\n",
        "            'ale.lives': np.mean(self.lives) if self.lives else None,\n",
        "            'reward_min': np.min(self.reward_values) if self.reward_values else None,\n",
        "            'reward_max': np.max(self.reward_values) if self.reward_values else None\n",
        "        }\n",
        "\n",
        "        self.writer.writerow(row)\n",
        "        self.file.flush()\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"[Log CSV] Episodio {row['episode']} - Recompensa: {row['episode_reward']} - \"\n",
        "                  f\"Loss: {row['loss']}, MAE: {row['mae']}, Mean Q: {row['mean_q']}\")\n",
        "\n",
        "        self._reset_episode_stats()\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.file:\n",
        "            self.file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jqpjeC-6J9xP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# CALLBACK CUSTOM PARA GUARDAR LOS CHECKPOINTS CON EL NOMBRE BIEN CUANDO SE REINICIA EL ENTRENAMIENTO Y LA MEMORIA\n",
        "class AccumulatedCheckpoint(Callback):\n",
        "    def __init__(self, base_path, env_name, interval, initial_step=0):\n",
        "        super().__init__()\n",
        "        self.base_path = base_path\n",
        "        self.weights_path = os.path.join(base_path, f'dqn_{env_name}_weights_{{step}}.h5f')\n",
        "        self.memory_path= os.path.join(base_path,'memory.pkl')\n",
        "        self.policy_state_path = os.path.join(base_path, 'policy.json')\n",
        "        self.interval = interval\n",
        "        self.total_steps = initial_step\n",
        "        print(f\"Callback iniciado desde paso {self.total_steps}\")\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        self.total_steps += 1\n",
        "        if self.total_steps % self.interval == 0:\n",
        "            base = self.base_path.format(step=self.total_steps)\n",
        "\n",
        "            # Guardar pesos del modelo\n",
        "            weights_path = self.weights_path\n",
        "            weights_path = self.weights_path.format(step=self.total_steps)\n",
        "            self.model.save_weights(weights_path, overwrite=True)\n",
        "            print(f\"\\n[Checkpoint] Pesos guardados en: {weights_path}\")\n",
        "\n",
        "            # Guardar memoria de repetición\n",
        "            memory_path = self.memory_path\n",
        "            temp_path = memory_path + \".tmp\"\n",
        "\n",
        "            with open(temp_path, \"wb\") as f:\n",
        "                pickle.dump(self.model.memory, f)\n",
        "\n",
        "            # Solo si guardar ha ido bien\n",
        "            os.replace(temp_path, memory_path)\n",
        "            print(f\"\\n[Checkpoint] Memoria guardada de forma segura en: {memory_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QEeBZCr1J9xP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN PARA CARGAR EL ÚLTMO CHECKPOINT DETECTADO\n",
        "def load_last_checkpoint(dqn):\n",
        "  # Buscar todos los checkpoints por su archivo .index\n",
        "  pattern = os.path.join(CHECKPOINTS_DIR, f'dqn_{env_name}_weights_*.h5f.index')\n",
        "  checkpoints = glob.glob(pattern)\n",
        "\n",
        "  # Valor por defecto si no se encuentra checkpoint\n",
        "  last_checkpoint_steps = 0\n",
        "\n",
        "  if checkpoints:\n",
        "      # Extraer el número de paso del nombre\n",
        "      def extract_step(filename):\n",
        "          try:\n",
        "              name = os.path.basename(filename)\n",
        "              step_part = name.split('_weights_')[1].replace('.h5f.index', '')\n",
        "              return int(step_part)\n",
        "          except:\n",
        "              return -1\n",
        "\n",
        "      # Seleccionar el checkpoint con mayor número de pasos\n",
        "      latest_index = max(checkpoints, key=extract_step)\n",
        "\n",
        "      # Quitar la extensión .index para obtener el nombre base\n",
        "      latest_checkpoint = latest_index.replace('.index', '')\n",
        "\n",
        "      print(f\"[DQN] Cargando último checkpoint: {latest_checkpoint}\")\n",
        "      dqn.load_weights(latest_checkpoint)\n",
        "\n",
        "      # Aquí extraemos los pasos acumulados\n",
        "      last_checkpoint_steps = extract_step(latest_index)\n",
        "  else:\n",
        "      print(\"[DQN] No se encontró ningún checkpoint, entrenamiento desde cero.\")\n",
        "  return dqn, last_checkpoint_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "o2PywN6Wsef_"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN QUE PERMITE AJUSTAR LOS PARÁMETROS DE LA POLICY PARA QUE EL ENTRENAMIENTO SE REAUNDE POR DONDE TOCA\n",
        "# SI NO POR DEFECTO COMENZARÍA EN EL VALOR MÁXIMO DE EPSILON\n",
        "def adjust_policy_params(scheduler_steps, train_steps, current_step, value_max=1.0, value_min=0.1):\n",
        "    \"\"\"\n",
        "    Ajusta dinámicamente los parámetros de LinearAnnealedPolicy para reanudar la exploración desde el punto correcto.\n",
        "\n",
        "    Args:\n",
        "        scheduler_steps (int): Número de pasos que define la duración del decaimiento de eps.\n",
        "        train_steps (int): Número total de pasos planeados para el entrenamiento.\n",
        "        current_step (int): Paso actual o recuperado del checkpoint.\n",
        "        value_max (float): Valor inicial deseado de eps.\n",
        "        value_min (float): Valor mínimo deseado de eps.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (value_max_ajustado, value_min, scheduler_steps_restantes)\n",
        "    \"\"\"\n",
        "    if current_step >= scheduler_steps:\n",
        "        # Ya se alcanzó el mínimo, mantenerlo constante\n",
        "        return value_min, value_min, 1  # eps se queda fijo\n",
        "\n",
        "    # Calcular epsilon actual desde el punto alcanzado\n",
        "    frac = current_step / scheduler_steps\n",
        "    current_eps = value_max - (value_max - value_min) * frac\n",
        "\n",
        "    # Pasos restantes para completar el scheduler\n",
        "    steps_remaining = scheduler_steps - current_step\n",
        "    return current_eps, value_min, steps_remaining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "88T-Q17-t1cw"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN PARA CARGAR LA MEMORIA Y EL NÚMERO DE PASOS DEL MODELO\n",
        "def get_memory_and_last_steps():\n",
        "  pattern = os.path.join(CHECKPOINTS_DIR, f'dqn_{env_name}_weights_*.h5f.index')\n",
        "  checkpoints = glob.glob(pattern)\n",
        "  last_checkpoint_steps = 0\n",
        "  memory = None\n",
        "\n",
        "  if checkpoints:\n",
        "      def extract_step(filename):\n",
        "          try:\n",
        "              name = os.path.basename(filename)\n",
        "              step_part = name.split('_weights_')[1].replace('.h5f.index', '')\n",
        "              return int(step_part)\n",
        "          except:\n",
        "              return -1\n",
        "\n",
        "      # Encontrar el último checkpoint\n",
        "      latest_index = max(checkpoints, key=extract_step)\n",
        "      latest_checkpoint = latest_index.replace('.index', '')\n",
        "      last_checkpoint_steps = extract_step(latest_index)\n",
        "      print(f\"Último step de checkpoint: {last_checkpoint_steps}\")\n",
        "\n",
        "      # Cargar memoria\n",
        "      memory_path = os.path.join(CHECKPOINTS_DIR, \"memory.pkl\")\n",
        "      if os.path.exists(memory_path):\n",
        "          with open(memory_path, \"rb\") as f:\n",
        "              memory = pickle.load(f)\n",
        "          print(f\"Memoria cargada desde: {memory_path}\")\n",
        "      else:\n",
        "          print(\"No se encontró memoria para este checkpoint.\")\n",
        "\n",
        "  else:\n",
        "      print(\"No se encontró ningún checkpoint, entrenamiento desde cero.\")\n",
        "  return memory, last_checkpoint_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpVqx7bnJLKg",
        "tags": []
      },
      "source": [
        "# MODELO D1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uEMcNQ45ISB"
      },
      "source": [
        "Aquí se presenta una variante del modelo D1\n",
        "\n",
        "Gaussian Noise en input:\n",
        "\n",
        "Mejora generalización simulando variaciones en pixels\n",
        "\n",
        "SpatialDropout2D:\n",
        "\n",
        "Mejor que Dropout para capas convolucionales (elimina canales completos)\n",
        "\n",
        "Mecanismo de Attention simple:\n",
        "\n",
        "Ayuda al modelo a enfocarse en enemigos y proyectiles\n",
        "\n",
        "Inicialización He Normal:\n",
        "\n",
        "Optimizada para LeakyReLU (mejor flujo de gradientes)\n",
        "\n",
        "Hiperparámetros ajustados:\n",
        "\n",
        "GAMMA=0.96: Prioriza disparar enemigos (recompensa inmediata) sin descuidar supervivencia\n",
        "\n",
        "MODEL_UPDATE=3000: Más estabilidad en aprendizaje profundo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT45Y6y25ISB"
      },
      "source": [
        "## DQN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yitXTADGb2b"
      },
      "source": [
        "1. Implementación de la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBvHXcEe6eF0",
        "outputId": "7e2f800a-aaef-47f5-e758-bab84eac6303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "gaussian_noise_1 (GaussianNo (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 21, 21, 64)        16448     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 21, 21, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 21, 21, 64)        0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_2 (Spatial (None, 21, 21, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 128)       131200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 11, 11, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 11, 11, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 11, 11, 256)       0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_3 (Spatial (None, 11, 11, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 30976)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               15860224  \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 1542      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 16,437,702\n",
            "Trainable params: 16,436,806\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LeakyReLU, GaussianNoise, SpatialDropout2D\n",
        "from tensorflow.keras.layers import Dropout, Conv2D, Multiply\n",
        "\n",
        "\n",
        "model_name = \"modelo_D1_Plus\"\n",
        "model = Sequential()\n",
        "\n",
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "\n",
        "# Preprocesamiento\n",
        "if K.image_data_format() == 'channels_last':\n",
        "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "else:\n",
        "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
        "\n",
        "model.add(Lambda(lambda x: x / 255.0))\n",
        "model.add(GaussianNoise(0.01))  # Regularización en input\n",
        "\n",
        "# Bloques convolucionales\n",
        "model.add(Conv2D(64, (8, 8), strides=(4, 4), padding='same', kernel_initializer='he_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(SpatialDropout2D(0.1))\n",
        "\n",
        "model.add(Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', kernel_initializer='he_normal'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(SpatialDropout2D(0.1))\n",
        "\n",
        "# Capas densas\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, kernel_initializer='he_normal'))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(256, kernel_initializer='he_normal'))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "# Salida\n",
        "model.add(Dense(nb_actions, kernel_initializer='glorot_uniform'))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB9-_5HPGb2b"
      },
      "source": [
        "2. Implementación de la solución DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxBLemXQQam3",
        "outputId": "d93f440f-33b4-492c-a056-bd0362ced1f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fichero de hiperparámetros no encontrado. Creado por defecto en: ./models\\modelo_D1_Plus\\modelo_D1_Plus.json\n",
            "[INFO] Hiperparámetros cargados desde ./models\\modelo_D1_Plus\\modelo_D1_Plus.json\n"
          ]
        }
      ],
      "source": [
        "# GENERACIÓN O CARGA MODELO\n",
        "MODEL_DIR, WEIGHTS_DIR, CHECKPOINTS_DIR = get_dirs(model_name)\n",
        "# save_hyperparams(HYPERPARAMETERS_DIR, filename=model_name+\".json\")\n",
        "load_hyperparams(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foSlxWH1Gb2b",
        "outputId": "79fca2b4-2b67-4bce-f0e9-602bca9f2597",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No se encontró ningún checkpoint, entrenamiento desde cero.\n",
            "Memoria inicializada de 0\n",
            "Valores de la policy: value_min=0.1, value_max=1.0, scheduler_steps=1500000\n"
          ]
        }
      ],
      "source": [
        "# DEFINICIÓN DE LA POLICY\n",
        "\n",
        "memory, last_checkpoint_steps=get_memory_and_last_steps()\n",
        "\n",
        "value_max, value_min, new_scheduler_steps = adjust_policy_params(\n",
        "    scheduler_steps=SCHEDULER_STEPS,\n",
        "    train_steps=TRAIN_STEPS,\n",
        "    current_step=last_checkpoint_steps\n",
        ")\n",
        "if not memory:\n",
        "  memory = SequentialMemory(limit=MEMORY_SIZE, window_length=WINDOW_LENGTH)\n",
        "  print(\"Memoria inicializada de 0\")\n",
        "\n",
        "print(f\"Valores de la policy: value_min={value_min}, value_max={value_max}, scheduler_steps={new_scheduler_steps}\")\n",
        "\n",
        "processor = AtariProcessor()\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                              value_max=value_max, value_min=value_min,\n",
        "                              value_test=.05,\n",
        "                              nb_steps=new_scheduler_steps)\n",
        "#policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "#                              value_max=1., value_min=.1,\n",
        "#                              value_test=.05,\n",
        "#                              nb_steps=SCHEDULER_STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "G6mWUNNi5ISE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# CREACIÓN DEL AGENTE DQN\n",
        "dqn = DQNAgent(model=model,\n",
        "               nb_actions=nb_actions,\n",
        "               policy=policy,\n",
        "               memory=memory,\n",
        "               processor=processor,\n",
        "               nb_steps_warmup=WARMUP_STEPS,\n",
        "               gamma=GAMMA,\n",
        "               target_model_update=MODEL_UPDATE,\n",
        "               train_interval=4,\n",
        "               delta_clip=DELTA_CLIP)\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Parche para evitar el error 'get_updates' que ya no existe\n",
        "    def patched_get_updates(self, loss, params):\n",
        "        return []\n",
        "    Adam.get_updates = patched_get_updates\n",
        "\n",
        "dqn.compile(Adam(learning_rate=LEARNING_RATE), metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "TxfFDyGzS3_K"
      },
      "outputs": [],
      "source": [
        "# DEFINICIÓN DE LOS NOMBRES DE FICHEROS DE SALIDA\n",
        "weights_filename = os.path.join(WEIGHTS_DIR, 'dqn_{}_weights_{}.h5f'.format(env_name, model_name))\n",
        "checkpoint_weights_filename = os.path.join(CHECKPOINTS_DIR, 'dqn_' + env_name + '_weights_{step}.h5f')\n",
        "log_filename =os.path.join(MODEL_DIR, 'dqn_{}_log_{}.json'.format(env_name, model_name))\n",
        "log_csv_path = os.path.join(MODEL_DIR, f'{model_name}_training_log.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgEZ4q9OB41r",
        "outputId": "3aafb8d1-291a-4430-d5d7-4aa0089759b9",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DQN] No se encontró ningún checkpoint, entrenamiento desde cero.\n",
            "Callback iniciado desde paso 0\n"
          ]
        }
      ],
      "source": [
        "# CARGAR PESOS DEL ÚLTIMO CHECKPOINT SI EXISTE\n",
        "dqn, last_checkpoint_steps = load_last_checkpoint(dqn)\n",
        "# CREAR CALLBACKS CUSTOMIZADOS QUE GUARDAN BIEN LOS CHECKPOINTS Y LOGS DEL TRAINING\n",
        "checkpoint_callback = AccumulatedCheckpoint(\n",
        "      base_path=CHECKPOINTS_DIR,\n",
        "      env_name=env_name,\n",
        "      interval=MODEL_CHECKPOINT_STEPS,\n",
        "      initial_step=last_checkpoint_steps\n",
        "  )\n",
        "callbacks = [checkpoint_callback]\n",
        "dqn.step = last_checkpoint_steps\n",
        "callbacks += [EpisodeLoggerCSV(log_csv_path)]\n",
        "#callbacks =  [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=MODEL_CHECKPOINT_STEPS)]\n",
        "#callbacks += [FileLogger(log_filename, interval=100)] # Gives some errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BSoqLO65ISF",
        "outputId": "e9df2221-2c03-4c5e-b807-e46f2eff1727",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 2000000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "x:\\08_MIAR\\actividades\\proyecto_practico\\venv_rl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5000/5000 [==============================] - 29s 5ms/step - reward: 0.0130\n",
            "7 episodes - episode_reward: 7.857 [6.000, 11.000] - ale.lives: 2.050\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "5000/5000 [==============================] - 22s 4ms/step - reward: 0.0116\n",
            "6 episodes - episode_reward: 8.833 [4.000, 13.000] - ale.lives: 2.022\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 23s 5ms/step - reward: 0.0124\n",
            "7 episodes - episode_reward: 10.714 [3.000, 23.000] - ale.lives: 2.122\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "4989/5000 [============================>.] - ETA: 0s - reward: 0.0136\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_20000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 24s 5ms/step - reward: 0.0136\n",
            "8 episodes - episode_reward: 8.500 [2.000, 17.000] - ale.lives: 2.124\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 22s 4ms/step - reward: 0.0156\n",
            "6 episodes - episode_reward: 12.500 [8.000, 18.000] - ale.lives: 2.027\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "5000/5000 [==============================] - 22s 4ms/step - reward: 0.0116\n",
            "7 episodes - episode_reward: 8.286 [4.000, 11.000] - ale.lives: 1.991\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            "5000/5000 [==============================] - 22s 4ms/step - reward: 0.0156\n",
            "8 episodes - episode_reward: 9.625 [3.000, 23.000] - ale.lives: 2.027\n",
            "\n",
            "Interval 8 (35000 steps performed)\n",
            "4989/5000 [============================>.] - ETA: 0s - reward: 0.0148\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_40000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 23s 5ms/step - reward: 0.0148\n",
            "8 episodes - episode_reward: 9.500 [7.000, 14.000] - ale.lives: 2.170\n",
            "\n",
            "Interval 9 (40000 steps performed)\n",
            "5000/5000 [==============================] - 22s 4ms/step - reward: 0.0106\n",
            "7 episodes - episode_reward: 7.429 [1.000, 20.000] - ale.lives: 2.185\n",
            "\n",
            "Interval 10 (45000 steps performed)\n",
            "5000/5000 [==============================] - 22s 4ms/step - reward: 0.0152\n",
            "7 episodes - episode_reward: 10.429 [3.000, 18.000] - ale.lives: 1.915\n",
            "\n",
            "Interval 11 (50000 steps performed)\n",
            "5000/5000 [==============================] - 141s 28ms/step - reward: 0.0174\n",
            "7 episodes - episode_reward: 13.000 [7.000, 23.000] - loss: 0.027 - mae: 0.137 - mean_q: 0.242 - mean_eps: 0.968 - ale.lives: 2.235\n",
            "\n",
            "Interval 12 (55000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0140\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_60000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 140s 28ms/step - reward: 0.0140\n",
            "7 episodes - episode_reward: 9.857 [7.000, 13.000] - loss: 0.006 - mae: 0.065 - mean_q: 0.125 - mean_eps: 0.966 - ale.lives: 1.971\n",
            "\n",
            "Interval 13 (60000 steps performed)\n",
            "5000/5000 [==============================] - 132s 26ms/step - reward: 0.0134\n",
            "8 episodes - episode_reward: 8.500 [3.000, 14.000] - loss: 0.006 - mae: 0.067 - mean_q: 0.118 - mean_eps: 0.963 - ale.lives: 1.980\n",
            "\n",
            "Interval 14 (65000 steps performed)\n",
            "5000/5000 [==============================] - 132s 26ms/step - reward: 0.0142\n",
            "9 episodes - episode_reward: 8.333 [4.000, 14.000] - loss: 0.006 - mae: 0.090 - mean_q: 0.141 - mean_eps: 0.960 - ale.lives: 2.115\n",
            "\n",
            "Interval 15 (70000 steps performed)\n",
            "5000/5000 [==============================] - 132s 26ms/step - reward: 0.0140\n",
            "6 episodes - episode_reward: 9.833 [5.000, 12.000] - loss: 0.006 - mae: 0.095 - mean_q: 0.140 - mean_eps: 0.957 - ale.lives: 2.006\n",
            "\n",
            "Interval 16 (75000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0138\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_80000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 134s 27ms/step - reward: 0.0138\n",
            "8 episodes - episode_reward: 9.625 [5.000, 17.000] - loss: 0.005 - mae: 0.091 - mean_q: 0.134 - mean_eps: 0.954 - ale.lives: 2.165\n",
            "\n",
            "Interval 17 (80000 steps performed)\n",
            "5000/5000 [==============================] - 132s 26ms/step - reward: 0.0142\n",
            "7 episodes - episode_reward: 9.429 [5.000, 18.000] - loss: 0.006 - mae: 0.085 - mean_q: 0.128 - mean_eps: 0.951 - ale.lives: 2.102\n",
            "\n",
            "Interval 18 (85000 steps performed)\n",
            "5000/5000 [==============================] - 132s 26ms/step - reward: 0.0148\n",
            "7 episodes - episode_reward: 10.571 [4.000, 18.000] - loss: 0.006 - mae: 0.070 - mean_q: 0.109 - mean_eps: 0.948 - ale.lives: 2.021\n",
            "\n",
            "Interval 19 (90000 steps performed)\n",
            "5000/5000 [==============================] - 133s 27ms/step - reward: 0.0148\n",
            "7 episodes - episode_reward: 9.857 [4.000, 20.000] - loss: 0.006 - mae: 0.060 - mean_q: 0.093 - mean_eps: 0.945 - ale.lives: 2.027\n",
            "\n",
            "Interval 20 (95000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0136\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_100000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 135s 27ms/step - reward: 0.0136\n",
            "8 episodes - episode_reward: 9.375 [5.000, 14.000] - loss: 0.005 - mae: 0.064 - mean_q: 0.100 - mean_eps: 0.942 - ale.lives: 2.126\n",
            "\n",
            "Interval 21 (100000 steps performed)\n",
            "5000/5000 [==============================] - 132s 26ms/step - reward: 0.0170\n",
            "6 episodes - episode_reward: 11.833 [7.000, 14.000] - loss: 0.005 - mae: 0.065 - mean_q: 0.097 - mean_eps: 0.939 - ale.lives: 1.967\n",
            "\n",
            "Interval 22 (105000 steps performed)\n",
            "5000/5000 [==============================] - 133s 27ms/step - reward: 0.0134\n",
            "7 episodes - episode_reward: 11.429 [3.000, 25.000] - loss: 0.005 - mae: 0.063 - mean_q: 0.096 - mean_eps: 0.936 - ale.lives: 1.906\n",
            "\n",
            "Interval 23 (110000 steps performed)\n",
            "5000/5000 [==============================] - 133s 27ms/step - reward: 0.0160\n",
            "7 episodes - episode_reward: 11.571 [7.000, 22.000] - loss: 0.005 - mae: 0.056 - mean_q: 0.087 - mean_eps: 0.933 - ale.lives: 2.056\n",
            "\n",
            "Interval 24 (115000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0132\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_120000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 137s 27ms/step - reward: 0.0134\n",
            "8 episodes - episode_reward: 8.875 [6.000, 14.000] - loss: 0.006 - mae: 0.073 - mean_q: 0.106 - mean_eps: 0.930 - ale.lives: 2.165\n",
            "\n",
            "Interval 25 (120000 steps performed)\n",
            "5000/5000 [==============================] - 134s 27ms/step - reward: 0.0126\n",
            "8 episodes - episode_reward: 7.750 [5.000, 12.000] - loss: 0.006 - mae: 0.152 - mean_q: 0.200 - mean_eps: 0.927 - ale.lives: 2.128\n",
            "\n",
            "Interval 26 (125000 steps performed)\n",
            "5000/5000 [==============================] - 134s 27ms/step - reward: 0.0134\n",
            "9 episodes - episode_reward: 7.444 [4.000, 14.000] - loss: 0.006 - mae: 0.224 - mean_q: 0.286 - mean_eps: 0.924 - ale.lives: 2.015\n",
            "\n",
            "Interval 27 (130000 steps performed)\n",
            "5000/5000 [==============================] - 134s 27ms/step - reward: 0.0132\n",
            "7 episodes - episode_reward: 9.286 [6.000, 15.000] - loss: 0.006 - mae: 0.298 - mean_q: 0.377 - mean_eps: 0.921 - ale.lives: 2.236\n",
            "\n",
            "Interval 28 (135000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0148\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_140000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 137s 27ms/step - reward: 0.0148\n",
            "10 episodes - episode_reward: 7.600 [4.000, 16.000] - loss: 0.006 - mae: 0.366 - mean_q: 0.459 - mean_eps: 0.918 - ale.lives: 2.145\n",
            "\n",
            "Interval 29 (140000 steps performed)\n",
            "5000/5000 [==============================] - 135s 27ms/step - reward: 0.0158\n",
            "7 episodes - episode_reward: 10.571 [8.000, 15.000] - loss: 0.007 - mae: 0.596 - mean_q: 0.741 - mean_eps: 0.915 - ale.lives: 1.955\n",
            "\n",
            "Interval 30 (145000 steps performed)\n",
            "5000/5000 [==============================] - 135s 27ms/step - reward: 0.0134\n",
            "7 episodes - episode_reward: 9.714 [3.000, 19.000] - loss: 0.009 - mae: 1.113 - mean_q: 1.368 - mean_eps: 0.912 - ale.lives: 2.051\n",
            "\n",
            "Interval 31 (150000 steps performed)\n",
            "5000/5000 [==============================] - 135s 27ms/step - reward: 0.0172\n",
            "7 episodes - episode_reward: 11.286 [4.000, 19.000] - loss: 0.010 - mae: 1.482 - mean_q: 1.813 - mean_eps: 0.909 - ale.lives: 2.116\n",
            "\n",
            "Interval 32 (155000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0122\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_160000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 140s 28ms/step - reward: 0.0122\n",
            "9 episodes - episode_reward: 8.000 [1.000, 14.000] - loss: 0.013 - mae: 1.757 - mean_q: 2.146 - mean_eps: 0.906 - ale.lives: 2.126\n",
            "\n",
            "Interval 33 (160000 steps performed)\n",
            "5000/5000 [==============================] - 135s 27ms/step - reward: 0.0138\n",
            "6 episodes - episode_reward: 11.833 [6.000, 17.000] - loss: 0.014 - mae: 2.039 - mean_q: 2.485 - mean_eps: 0.903 - ale.lives: 2.072\n",
            "\n",
            "Interval 34 (165000 steps performed)\n",
            "5000/5000 [==============================] - 135s 27ms/step - reward: 0.0124\n",
            "6 episodes - episode_reward: 9.500 [5.000, 14.000] - loss: 0.011 - mae: 1.656 - mean_q: 2.017 - mean_eps: 0.900 - ale.lives: 2.170\n",
            "\n",
            "Interval 35 (170000 steps performed)\n",
            "5000/5000 [==============================] - 141s 28ms/step - reward: 0.0132\n",
            "8 episodes - episode_reward: 8.500 [4.000, 20.000] - loss: 0.013 - mae: 1.960 - mean_q: 2.392 - mean_eps: 0.897 - ale.lives: 2.132\n",
            "\n",
            "Interval 36 (175000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0146\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_180000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 148s 29ms/step - reward: 0.0146\n",
            "7 episodes - episode_reward: 10.571 [5.000, 17.000] - loss: 0.014 - mae: 2.254 - mean_q: 2.745 - mean_eps: 0.894 - ale.lives: 2.094\n",
            "\n",
            "Interval 37 (180000 steps performed)\n",
            "5000/5000 [==============================] - 143s 28ms/step - reward: 0.0128\n",
            "7 episodes - episode_reward: 8.714 [3.000, 14.000] - loss: 0.010 - mae: 1.702 - mean_q: 2.070 - mean_eps: 0.891 - ale.lives: 2.134\n",
            "\n",
            "Interval 38 (185000 steps performed)\n",
            "5000/5000 [==============================] - 143s 29ms/step - reward: 0.0152\n",
            "7 episodes - episode_reward: 11.143 [7.000, 17.000] - loss: 0.013 - mae: 2.024 - mean_q: 2.465 - mean_eps: 0.888 - ale.lives: 2.219\n",
            "\n",
            "Interval 39 (190000 steps performed)\n",
            "5000/5000 [==============================] - 143s 29ms/step - reward: 0.0126\n",
            "8 episodes - episode_reward: 7.875 [4.000, 16.000] - loss: 0.010 - mae: 1.676 - mean_q: 2.039 - mean_eps: 0.885 - ale.lives: 2.216\n",
            "\n",
            "Interval 40 (195000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0118\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_200000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0118\n",
            "7 episodes - episode_reward: 8.143 [5.000, 14.000] - loss: 0.009 - mae: 1.547 - mean_q: 1.884 - mean_eps: 0.882 - ale.lives: 2.015\n",
            "\n",
            "Interval 41 (200000 steps performed)\n",
            "5000/5000 [==============================] - 140s 28ms/step - reward: 0.0144\n",
            "8 episodes - episode_reward: 9.625 [1.000, 19.000] - loss: 0.009 - mae: 1.515 - mean_q: 1.847 - mean_eps: 0.879 - ale.lives: 2.007\n",
            "\n",
            "Interval 42 (205000 steps performed)\n",
            "5000/5000 [==============================] - 139s 28ms/step - reward: 0.0140\n",
            "8 episodes - episode_reward: 8.125 [3.000, 15.000] - loss: 0.008 - mae: 1.404 - mean_q: 1.711 - mean_eps: 0.876 - ale.lives: 2.129\n",
            "\n",
            "Interval 43 (210000 steps performed)\n",
            "5000/5000 [==============================] - 138s 28ms/step - reward: 0.0150\n",
            "7 episodes - episode_reward: 9.714 [6.000, 16.000] - loss: 0.008 - mae: 1.377 - mean_q: 1.677 - mean_eps: 0.873 - ale.lives: 2.074\n",
            "\n",
            "Interval 44 (215000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0160\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_220000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 146s 29ms/step - reward: 0.0160\n",
            "7 episodes - episode_reward: 13.000 [7.000, 23.000] - loss: 0.008 - mae: 1.274 - mean_q: 1.555 - mean_eps: 0.870 - ale.lives: 2.035\n",
            "\n",
            "Interval 45 (220000 steps performed)\n",
            "5000/5000 [==============================] - 140s 28ms/step - reward: 0.0124\n",
            "7 episodes - episode_reward: 7.714 [4.000, 13.000] - loss: 0.008 - mae: 1.239 - mean_q: 1.510 - mean_eps: 0.867 - ale.lives: 2.023\n",
            "\n",
            "Interval 46 (225000 steps performed)\n",
            "5000/5000 [==============================] - 143s 29ms/step - reward: 0.0120\n",
            "7 episodes - episode_reward: 9.286 [6.000, 14.000] - loss: 0.007 - mae: 0.996 - mean_q: 1.215 - mean_eps: 0.864 - ale.lives: 2.033\n",
            "\n",
            "Interval 47 (230000 steps performed)\n",
            "5000/5000 [==============================] - 138s 28ms/step - reward: 0.0162\n",
            "6 episodes - episode_reward: 12.833 [7.000, 18.000] - loss: 0.007 - mae: 0.944 - mean_q: 1.152 - mean_eps: 0.861 - ale.lives: 1.894\n",
            "\n",
            "Interval 48 (235000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0158\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_240000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 145s 29ms/step - reward: 0.0158\n",
            "7 episodes - episode_reward: 11.143 [7.000, 17.000] - loss: 0.007 - mae: 1.039 - mean_q: 1.269 - mean_eps: 0.858 - ale.lives: 2.149\n",
            "\n",
            "Interval 49 (240000 steps performed)\n",
            "5000/5000 [==============================] - 141s 28ms/step - reward: 0.0160\n",
            "7 episodes - episode_reward: 12.429 [6.000, 24.000] - loss: 0.007 - mae: 0.993 - mean_q: 1.212 - mean_eps: 0.855 - ale.lives: 2.003\n",
            "\n",
            "Interval 50 (245000 steps performed)\n",
            "5000/5000 [==============================] - 145s 29ms/step - reward: 0.0144\n",
            "6 episodes - episode_reward: 10.333 [5.000, 16.000] - loss: 0.007 - mae: 0.873 - mean_q: 1.065 - mean_eps: 0.852 - ale.lives: 2.093\n",
            "\n",
            "Interval 51 (250000 steps performed)\n",
            "5000/5000 [==============================] - 147s 29ms/step - reward: 0.0138\n",
            "8 episodes - episode_reward: 9.625 [6.000, 21.000] - loss: 0.006 - mae: 0.693 - mean_q: 0.846 - mean_eps: 0.849 - ale.lives: 1.975\n",
            "\n",
            "Interval 52 (255000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0162\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_260000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0162\n",
            "8 episodes - episode_reward: 10.500 [4.000, 19.000] - loss: 0.006 - mae: 0.745 - mean_q: 0.910 - mean_eps: 0.846 - ale.lives: 2.053\n",
            "\n",
            "Interval 53 (260000 steps performed)\n",
            "5000/5000 [==============================] - 142s 28ms/step - reward: 0.0178\n",
            "6 episodes - episode_reward: 13.500 [5.000, 23.000] - loss: 0.006 - mae: 0.746 - mean_q: 0.912 - mean_eps: 0.843 - ale.lives: 2.020\n",
            "\n",
            "Interval 54 (265000 steps performed)\n",
            "5000/5000 [==============================] - 144s 29ms/step - reward: 0.0148\n",
            "7 episodes - episode_reward: 10.857 [6.000, 16.000] - loss: 0.006 - mae: 0.714 - mean_q: 0.872 - mean_eps: 0.840 - ale.lives: 2.012\n",
            "\n",
            "Interval 55 (270000 steps performed)\n",
            "5000/5000 [==============================] - 145s 29ms/step - reward: 0.0134\n",
            "8 episodes - episode_reward: 8.625 [5.000, 18.000] - loss: 0.006 - mae: 0.674 - mean_q: 0.823 - mean_eps: 0.837 - ale.lives: 2.134\n",
            "\n",
            "Interval 56 (275000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0172\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_280000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0172\n",
            "7 episodes - episode_reward: 12.429 [7.000, 27.000] - loss: 0.006 - mae: 0.672 - mean_q: 0.820 - mean_eps: 0.834 - ale.lives: 2.069\n",
            "\n",
            "Interval 57 (280000 steps performed)\n",
            "5000/5000 [==============================] - 144s 29ms/step - reward: 0.0126\n",
            "7 episodes - episode_reward: 8.714 [6.000, 14.000] - loss: 0.006 - mae: 0.681 - mean_q: 0.831 - mean_eps: 0.831 - ale.lives: 2.017\n",
            "\n",
            "Interval 58 (285000 steps performed)\n",
            "5000/5000 [==============================] - 145s 29ms/step - reward: 0.0142\n",
            "6 episodes - episode_reward: 11.167 [9.000, 16.000] - loss: 0.006 - mae: 0.639 - mean_q: 0.779 - mean_eps: 0.828 - ale.lives: 2.029\n",
            "\n",
            "Interval 59 (290000 steps performed)\n",
            "5000/5000 [==============================] - 145s 29ms/step - reward: 0.0130\n",
            "8 episodes - episode_reward: 9.375 [2.000, 16.000] - loss: 0.005 - mae: 0.620 - mean_q: 0.756 - mean_eps: 0.825 - ale.lives: 2.081\n",
            "\n",
            "Interval 60 (295000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0160\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_300000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 12.333 [9.000, 16.000] - loss: 0.006 - mae: 0.617 - mean_q: 0.753 - mean_eps: 0.822 - ale.lives: 2.346\n",
            "\n",
            "Interval 61 (300000 steps performed)\n",
            "5000/5000 [==============================] - 146s 29ms/step - reward: 0.0130\n",
            "7 episodes - episode_reward: 9.143 [4.000, 20.000] - loss: 0.006 - mae: 0.619 - mean_q: 0.755 - mean_eps: 0.819 - ale.lives: 1.960\n",
            "\n",
            "Interval 62 (305000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0174\n",
            "7 episodes - episode_reward: 12.714 [4.000, 31.000] - loss: 0.006 - mae: 0.612 - mean_q: 0.746 - mean_eps: 0.816 - ale.lives: 2.078\n",
            "\n",
            "Interval 63 (310000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 13.500 [8.000, 20.000] - loss: 0.006 - mae: 0.596 - mean_q: 0.726 - mean_eps: 0.813 - ale.lives: 1.841\n",
            "\n",
            "Interval 64 (315000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0140\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_320000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0140\n",
            "9 episodes - episode_reward: 8.222 [4.000, 14.000] - loss: 0.006 - mae: 0.588 - mean_q: 0.716 - mean_eps: 0.810 - ale.lives: 2.109\n",
            "\n",
            "Interval 65 (320000 steps performed)\n",
            "5000/5000 [==============================] - 148s 30ms/step - reward: 0.0138\n",
            "8 episodes - episode_reward: 7.500 [3.000, 15.000] - loss: 0.006 - mae: 0.585 - mean_q: 0.714 - mean_eps: 0.807 - ale.lives: 2.046\n",
            "\n",
            "Interval 66 (325000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0144\n",
            "8 episodes - episode_reward: 9.250 [5.000, 16.000] - loss: 0.006 - mae: 0.579 - mean_q: 0.706 - mean_eps: 0.804 - ale.lives: 2.068\n",
            "\n",
            "Interval 67 (330000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0130\n",
            "8 episodes - episode_reward: 8.500 [3.000, 18.000] - loss: 0.006 - mae: 0.567 - mean_q: 0.691 - mean_eps: 0.801 - ale.lives: 2.142\n",
            "\n",
            "Interval 68 (335000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0130\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_340000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0130\n",
            "8 episodes - episode_reward: 7.500 [5.000, 14.000] - loss: 0.006 - mae: 0.551 - mean_q: 0.671 - mean_eps: 0.798 - ale.lives: 2.070\n",
            "\n",
            "Interval 69 (340000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0148\n",
            "8 episodes - episode_reward: 10.000 [5.000, 17.000] - loss: 0.006 - mae: 0.542 - mean_q: 0.661 - mean_eps: 0.795 - ale.lives: 2.028\n",
            "\n",
            "Interval 70 (345000 steps performed)\n",
            "5000/5000 [==============================] - 148s 30ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 12.000 [8.000, 19.000] - loss: 0.006 - mae: 0.528 - mean_q: 0.644 - mean_eps: 0.792 - ale.lives: 1.970\n",
            "\n",
            "Interval 71 (350000 steps performed)\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0170\n",
            "7 episodes - episode_reward: 12.571 [4.000, 21.000] - loss: 0.005 - mae: 0.526 - mean_q: 0.642 - mean_eps: 0.789 - ale.lives: 2.039\n",
            "\n",
            "Interval 72 (355000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0150\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_360000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0150\n",
            "8 episodes - episode_reward: 10.375 [3.000, 19.000] - loss: 0.005 - mae: 0.524 - mean_q: 0.639 - mean_eps: 0.786 - ale.lives: 2.152\n",
            "\n",
            "Interval 73 (360000 steps performed)\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0138\n",
            "7 episodes - episode_reward: 9.429 [6.000, 12.000] - loss: 0.006 - mae: 0.514 - mean_q: 0.627 - mean_eps: 0.783 - ale.lives: 1.918\n",
            "\n",
            "Interval 74 (365000 steps performed)\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0152\n",
            "7 episodes - episode_reward: 10.857 [3.000, 21.000] - loss: 0.006 - mae: 0.520 - mean_q: 0.634 - mean_eps: 0.780 - ale.lives: 2.121\n",
            "\n",
            "Interval 75 (370000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0176\n",
            "6 episodes - episode_reward: 14.167 [5.000, 21.000] - loss: 0.006 - mae: 0.510 - mean_q: 0.622 - mean_eps: 0.777 - ale.lives: 2.026\n",
            "\n",
            "Interval 76 (375000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0134\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_380000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 163s 33ms/step - reward: 0.0134\n",
            "7 episodes - episode_reward: 9.571 [6.000, 22.000] - loss: 0.006 - mae: 0.508 - mean_q: 0.620 - mean_eps: 0.774 - ale.lives: 1.997\n",
            "\n",
            "Interval 77 (380000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0148\n",
            "8 episodes - episode_reward: 9.625 [7.000, 15.000] - loss: 0.005 - mae: 0.502 - mean_q: 0.611 - mean_eps: 0.771 - ale.lives: 2.025\n",
            "\n",
            "Interval 78 (385000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0118\n",
            "5 episodes - episode_reward: 11.600 [5.000, 15.000] - loss: 0.006 - mae: 0.494 - mean_q: 0.602 - mean_eps: 0.768 - ale.lives: 1.986\n",
            "\n",
            "Interval 79 (390000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0124\n",
            "8 episodes - episode_reward: 8.000 [5.000, 12.000] - loss: 0.006 - mae: 0.483 - mean_q: 0.589 - mean_eps: 0.765 - ale.lives: 2.031\n",
            "\n",
            "Interval 80 (395000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_400000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0144\n",
            "8 episodes - episode_reward: 8.375 [3.000, 14.000] - loss: 0.006 - mae: 0.482 - mean_q: 0.587 - mean_eps: 0.762 - ale.lives: 2.068\n",
            "\n",
            "Interval 81 (400000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0162\n",
            "8 episodes - episode_reward: 10.625 [4.000, 19.000] - loss: 0.006 - mae: 0.485 - mean_q: 0.590 - mean_eps: 0.759 - ale.lives: 1.938\n",
            "\n",
            "Interval 82 (405000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0144\n",
            "6 episodes - episode_reward: 10.833 [4.000, 18.000] - loss: 0.006 - mae: 0.493 - mean_q: 0.600 - mean_eps: 0.756 - ale.lives: 2.138\n",
            "\n",
            "Interval 83 (410000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0148\n",
            "6 episodes - episode_reward: 11.000 [6.000, 16.000] - loss: 0.006 - mae: 0.491 - mean_q: 0.599 - mean_eps: 0.753 - ale.lives: 2.027\n",
            "\n",
            "Interval 84 (415000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0170\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_420000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 169s 34ms/step - reward: 0.0170\n",
            "7 episodes - episode_reward: 14.429 [6.000, 26.000] - loss: 0.006 - mae: 0.479 - mean_q: 0.583 - mean_eps: 0.750 - ale.lives: 1.966\n",
            "\n",
            "Interval 85 (420000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0110\n",
            "7 episodes - episode_reward: 6.714 [3.000, 15.000] - loss: 0.005 - mae: 0.473 - mean_q: 0.575 - mean_eps: 0.747 - ale.lives: 2.098\n",
            "\n",
            "Interval 86 (425000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0156\n",
            "7 episodes - episode_reward: 12.571 [4.000, 27.000] - loss: 0.005 - mae: 0.466 - mean_q: 0.568 - mean_eps: 0.744 - ale.lives: 1.932\n",
            "\n",
            "Interval 87 (430000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0152\n",
            "7 episodes - episode_reward: 10.857 [8.000, 14.000] - loss: 0.005 - mae: 0.475 - mean_q: 0.578 - mean_eps: 0.741 - ale.lives: 2.129\n",
            "\n",
            "Interval 88 (435000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0178\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_440000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 168s 34ms/step - reward: 0.0178\n",
            "5 episodes - episode_reward: 16.600 [7.000, 24.000] - loss: 0.006 - mae: 0.484 - mean_q: 0.589 - mean_eps: 0.738 - ale.lives: 2.149\n",
            "\n",
            "Interval 89 (440000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0166\n",
            "7 episodes - episode_reward: 11.143 [9.000, 16.000] - loss: 0.006 - mae: 0.487 - mean_q: 0.592 - mean_eps: 0.735 - ale.lives: 2.183\n",
            "\n",
            "Interval 90 (445000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0170\n",
            "7 episodes - episode_reward: 12.857 [5.000, 18.000] - loss: 0.006 - mae: 0.480 - mean_q: 0.585 - mean_eps: 0.732 - ale.lives: 1.933\n",
            "\n",
            "Interval 91 (450000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0136\n",
            "8 episodes - episode_reward: 8.500 [6.000, 11.000] - loss: 0.006 - mae: 0.477 - mean_q: 0.580 - mean_eps: 0.729 - ale.lives: 2.101\n",
            "\n",
            "Interval 92 (455000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0146\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_460000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 169s 34ms/step - reward: 0.0146\n",
            "8 episodes - episode_reward: 9.250 [4.000, 21.000] - loss: 0.005 - mae: 0.470 - mean_q: 0.570 - mean_eps: 0.726 - ale.lives: 2.097\n",
            "\n",
            "Interval 93 (460000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0142\n",
            "7 episodes - episode_reward: 9.143 [4.000, 15.000] - loss: 0.005 - mae: 0.460 - mean_q: 0.560 - mean_eps: 0.723 - ale.lives: 1.974\n",
            "\n",
            "Interval 94 (465000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0126\n",
            "9 episodes - episode_reward: 7.778 [2.000, 18.000] - loss: 0.006 - mae: 0.465 - mean_q: 0.566 - mean_eps: 0.720 - ale.lives: 2.200\n",
            "\n",
            "Interval 95 (470000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0150\n",
            "7 episodes - episode_reward: 10.571 [5.000, 19.000] - loss: 0.006 - mae: 0.465 - mean_q: 0.565 - mean_eps: 0.717 - ale.lives: 1.952\n",
            "\n",
            "Interval 96 (475000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0130\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_480000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 169s 34ms/step - reward: 0.0130\n",
            "7 episodes - episode_reward: 10.000 [4.000, 19.000] - loss: 0.005 - mae: 0.458 - mean_q: 0.557 - mean_eps: 0.714 - ale.lives: 2.077\n",
            "\n",
            "Interval 97 (480000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0142\n",
            "8 episodes - episode_reward: 8.875 [5.000, 15.000] - loss: 0.006 - mae: 0.454 - mean_q: 0.551 - mean_eps: 0.711 - ale.lives: 2.042\n",
            "\n",
            "Interval 98 (485000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0164\n",
            "6 episodes - episode_reward: 11.833 [6.000, 22.000] - loss: 0.006 - mae: 0.449 - mean_q: 0.546 - mean_eps: 0.708 - ale.lives: 2.125\n",
            "\n",
            "Interval 99 (490000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0128\n",
            "8 episodes - episode_reward: 9.500 [3.000, 20.000] - loss: 0.006 - mae: 0.451 - mean_q: 0.549 - mean_eps: 0.705 - ale.lives: 2.237\n",
            "\n",
            "Interval 100 (495000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0198\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_500000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 169s 34ms/step - reward: 0.0198\n",
            "5 episodes - episode_reward: 17.400 [13.000, 25.000] - loss: 0.006 - mae: 0.441 - mean_q: 0.535 - mean_eps: 0.702 - ale.lives: 2.108\n",
            "\n",
            "Interval 101 (500000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0162\n",
            "7 episodes - episode_reward: 13.286 [5.000, 27.000] - loss: 0.006 - mae: 0.428 - mean_q: 0.521 - mean_eps: 0.699 - ale.lives: 2.142\n",
            "\n",
            "Interval 102 (505000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0140\n",
            "7 episodes - episode_reward: 9.000 [6.000, 12.000] - loss: 0.006 - mae: 0.426 - mean_q: 0.518 - mean_eps: 0.696 - ale.lives: 2.027\n",
            "\n",
            "Interval 103 (510000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0132\n",
            "9 episodes - episode_reward: 7.444 [4.000, 15.000] - loss: 0.006 - mae: 0.433 - mean_q: 0.525 - mean_eps: 0.693 - ale.lives: 2.105\n",
            "\n",
            "Interval 104 (515000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0154\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_520000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 170s 34ms/step - reward: 0.0154\n",
            "9 episodes - episode_reward: 9.222 [5.000, 19.000] - loss: 0.006 - mae: 0.427 - mean_q: 0.518 - mean_eps: 0.690 - ale.lives: 2.052\n",
            "\n",
            "Interval 105 (520000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0164\n",
            "7 episodes - episode_reward: 11.429 [5.000, 21.000] - loss: 0.006 - mae: 0.421 - mean_q: 0.512 - mean_eps: 0.687 - ale.lives: 2.218\n",
            "\n",
            "Interval 106 (525000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0158\n",
            "7 episodes - episode_reward: 11.429 [6.000, 26.000] - loss: 0.006 - mae: 0.420 - mean_q: 0.509 - mean_eps: 0.684 - ale.lives: 2.069\n",
            "\n",
            "Interval 107 (530000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0162\n",
            "6 episodes - episode_reward: 12.833 [7.000, 29.000] - loss: 0.006 - mae: 0.412 - mean_q: 0.500 - mean_eps: 0.681 - ale.lives: 2.159\n",
            "\n",
            "Interval 108 (535000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0174\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_540000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 171s 34ms/step - reward: 0.0174\n",
            "5 episodes - episode_reward: 13.600 [7.000, 20.000] - loss: 0.006 - mae: 0.409 - mean_q: 0.496 - mean_eps: 0.678 - ale.lives: 2.054\n",
            "\n",
            "Interval 109 (540000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0186\n",
            "5 episodes - episode_reward: 23.000 [10.000, 30.000] - loss: 0.006 - mae: 0.411 - mean_q: 0.498 - mean_eps: 0.675 - ale.lives: 2.146\n",
            "\n",
            "Interval 110 (545000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0192\n",
            "5 episodes - episode_reward: 18.600 [8.000, 29.000] - loss: 0.006 - mae: 0.407 - mean_q: 0.494 - mean_eps: 0.672 - ale.lives: 2.040\n",
            "\n",
            "Interval 111 (550000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0164\n",
            "7 episodes - episode_reward: 12.429 [5.000, 21.000] - loss: 0.006 - mae: 0.408 - mean_q: 0.495 - mean_eps: 0.669 - ale.lives: 2.026\n",
            "\n",
            "Interval 112 (555000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0146\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_560000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0146\n",
            "6 episodes - episode_reward: 9.000 [5.000, 18.000] - loss: 0.006 - mae: 0.405 - mean_q: 0.491 - mean_eps: 0.666 - ale.lives: 2.162\n",
            "\n",
            "Interval 113 (560000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0136\n",
            "7 episodes - episode_reward: 11.857 [5.000, 22.000] - loss: 0.006 - mae: 0.402 - mean_q: 0.488 - mean_eps: 0.663 - ale.lives: 1.986\n",
            "\n",
            "Interval 114 (565000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0148\n",
            "6 episodes - episode_reward: 12.833 [7.000, 19.000] - loss: 0.006 - mae: 0.402 - mean_q: 0.487 - mean_eps: 0.660 - ale.lives: 2.071\n",
            "\n",
            "Interval 115 (570000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0172\n",
            "5 episodes - episode_reward: 17.400 [3.000, 31.000] - loss: 0.006 - mae: 0.393 - mean_q: 0.477 - mean_eps: 0.657 - ale.lives: 1.921\n",
            "\n",
            "Interval 116 (575000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0138\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_580000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0138\n",
            "7 episodes - episode_reward: 9.429 [4.000, 18.000] - loss: 0.006 - mae: 0.388 - mean_q: 0.471 - mean_eps: 0.654 - ale.lives: 2.197\n",
            "\n",
            "Interval 117 (580000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0166\n",
            "7 episodes - episode_reward: 10.571 [7.000, 19.000] - loss: 0.006 - mae: 0.384 - mean_q: 0.465 - mean_eps: 0.651 - ale.lives: 2.067\n",
            "\n",
            "Interval 118 (585000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0144\n",
            "7 episodes - episode_reward: 11.000 [6.000, 20.000] - loss: 0.006 - mae: 0.383 - mean_q: 0.462 - mean_eps: 0.648 - ale.lives: 2.097\n",
            "\n",
            "Interval 119 (590000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0176\n",
            "6 episodes - episode_reward: 14.500 [8.000, 21.000] - loss: 0.006 - mae: 0.394 - mean_q: 0.477 - mean_eps: 0.645 - ale.lives: 2.084\n",
            "\n",
            "Interval 120 (595000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0148\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_600000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0148\n",
            "7 episodes - episode_reward: 10.714 [3.000, 18.000] - loss: 0.006 - mae: 0.396 - mean_q: 0.480 - mean_eps: 0.642 - ale.lives: 1.851\n",
            "\n",
            "Interval 121 (600000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0184\n",
            "7 episodes - episode_reward: 11.857 [4.000, 21.000] - loss: 0.006 - mae: 0.395 - mean_q: 0.478 - mean_eps: 0.639 - ale.lives: 2.191\n",
            "\n",
            "Interval 122 (605000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0146\n",
            "8 episodes - episode_reward: 10.625 [5.000, 20.000] - loss: 0.006 - mae: 0.392 - mean_q: 0.474 - mean_eps: 0.636 - ale.lives: 1.980\n",
            "\n",
            "Interval 123 (610000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0146\n",
            "6 episodes - episode_reward: 8.667 [3.000, 14.000] - loss: 0.006 - mae: 0.395 - mean_q: 0.478 - mean_eps: 0.633 - ale.lives: 2.077\n",
            "\n",
            "Interval 124 (615000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0168\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_620000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0168\n",
            "7 episodes - episode_reward: 14.429 [5.000, 25.000] - loss: 0.006 - mae: 0.394 - mean_q: 0.477 - mean_eps: 0.630 - ale.lives: 2.183\n",
            "\n",
            "Interval 125 (620000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0160\n",
            "7 episodes - episode_reward: 12.429 [6.000, 25.000] - loss: 0.006 - mae: 0.396 - mean_q: 0.479 - mean_eps: 0.627 - ale.lives: 1.960\n",
            "\n",
            "Interval 126 (625000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0138\n",
            "7 episodes - episode_reward: 9.714 [5.000, 16.000] - loss: 0.006 - mae: 0.425 - mean_q: 0.514 - mean_eps: 0.624 - ale.lives: 1.984\n",
            "\n",
            "Interval 127 (630000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0172\n",
            "6 episodes - episode_reward: 13.000 [8.000, 24.000] - loss: 0.006 - mae: 0.441 - mean_q: 0.533 - mean_eps: 0.621 - ale.lives: 2.068\n",
            "\n",
            "Interval 128 (635000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0140\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_640000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0140\n",
            "7 episodes - episode_reward: 11.286 [5.000, 17.000] - loss: 0.006 - mae: 0.429 - mean_q: 0.519 - mean_eps: 0.618 - ale.lives: 1.997\n",
            "\n",
            "Interval 129 (640000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0186\n",
            "6 episodes - episode_reward: 13.167 [8.000, 18.000] - loss: 0.006 - mae: 0.423 - mean_q: 0.511 - mean_eps: 0.615 - ale.lives: 2.042\n",
            "\n",
            "Interval 130 (645000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0174\n",
            "8 episodes - episode_reward: 12.125 [4.000, 19.000] - loss: 0.006 - mae: 0.424 - mean_q: 0.513 - mean_eps: 0.612 - ale.lives: 2.064\n",
            "\n",
            "Interval 131 (650000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0168\n",
            "7 episodes - episode_reward: 12.429 [6.000, 18.000] - loss: 0.006 - mae: 0.416 - mean_q: 0.503 - mean_eps: 0.609 - ale.lives: 2.039\n",
            "\n",
            "Interval 132 (655000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0170\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_660000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0170\n",
            "6 episodes - episode_reward: 13.833 [9.000, 22.000] - loss: 0.006 - mae: 0.406 - mean_q: 0.490 - mean_eps: 0.606 - ale.lives: 2.139\n",
            "\n",
            "Interval 133 (660000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0186\n",
            "6 episodes - episode_reward: 14.000 [4.000, 26.000] - loss: 0.006 - mae: 0.401 - mean_q: 0.484 - mean_eps: 0.603 - ale.lives: 2.072\n",
            "\n",
            "Interval 134 (665000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0160\n",
            "7 episodes - episode_reward: 11.714 [4.000, 18.000] - loss: 0.006 - mae: 0.399 - mean_q: 0.481 - mean_eps: 0.600 - ale.lives: 2.235\n",
            "\n",
            "Interval 135 (670000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0144\n",
            "7 episodes - episode_reward: 10.429 [5.000, 21.000] - loss: 0.005 - mae: 0.385 - mean_q: 0.464 - mean_eps: 0.597 - ale.lives: 2.042\n",
            "\n",
            "Interval 136 (675000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0142\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_680000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0142\n",
            "7 episodes - episode_reward: 9.429 [6.000, 14.000] - loss: 0.006 - mae: 0.383 - mean_q: 0.462 - mean_eps: 0.594 - ale.lives: 2.065\n",
            "\n",
            "Interval 137 (680000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0156\n",
            "8 episodes - episode_reward: 11.625 [6.000, 19.000] - loss: 0.005 - mae: 0.380 - mean_q: 0.458 - mean_eps: 0.591 - ale.lives: 1.993\n",
            "\n",
            "Interval 138 (685000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0178\n",
            "6 episodes - episode_reward: 14.833 [7.000, 27.000] - loss: 0.006 - mae: 0.379 - mean_q: 0.457 - mean_eps: 0.588 - ale.lives: 2.015\n",
            "\n",
            "Interval 139 (690000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0140\n",
            "6 episodes - episode_reward: 11.000 [7.000, 19.000] - loss: 0.006 - mae: 0.380 - mean_q: 0.458 - mean_eps: 0.585 - ale.lives: 2.026\n",
            "\n",
            "Interval 140 (695000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0164\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_700000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0164\n",
            "5 episodes - episode_reward: 13.800 [2.000, 21.000] - loss: 0.006 - mae: 0.378 - mean_q: 0.455 - mean_eps: 0.582 - ale.lives: 2.084\n",
            "\n",
            "Interval 141 (700000 steps performed)\n",
            "5000/5000 [==============================] - 158s 31ms/step - reward: 0.0130\n",
            "6 episodes - episode_reward: 13.000 [7.000, 17.000] - loss: 0.006 - mae: 0.378 - mean_q: 0.456 - mean_eps: 0.579 - ale.lives: 1.908\n",
            "\n",
            "Interval 142 (705000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0110\n",
            "9 episodes - episode_reward: 6.444 [3.000, 10.000] - loss: 0.006 - mae: 0.373 - mean_q: 0.450 - mean_eps: 0.576 - ale.lives: 2.091\n",
            "\n",
            "Interval 143 (710000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0176\n",
            "6 episodes - episode_reward: 11.333 [7.000, 20.000] - loss: 0.006 - mae: 0.368 - mean_q: 0.443 - mean_eps: 0.573 - ale.lives: 2.240\n",
            "\n",
            "Interval 144 (715000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0180\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_720000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0180\n",
            "7 episodes - episode_reward: 14.143 [2.000, 26.000] - loss: 0.006 - mae: 0.369 - mean_q: 0.445 - mean_eps: 0.570 - ale.lives: 2.051\n",
            "\n",
            "Interval 145 (720000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 13.333 [7.000, 18.000] - loss: 0.006 - mae: 0.374 - mean_q: 0.450 - mean_eps: 0.567 - ale.lives: 2.169\n",
            "\n",
            "Interval 146 (725000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0162\n",
            "7 episodes - episode_reward: 13.286 [5.000, 24.000] - loss: 0.006 - mae: 0.362 - mean_q: 0.436 - mean_eps: 0.564 - ale.lives: 2.195\n",
            "\n",
            "Interval 147 (730000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0180\n",
            "6 episodes - episode_reward: 13.333 [8.000, 20.000] - loss: 0.006 - mae: 0.362 - mean_q: 0.436 - mean_eps: 0.561 - ale.lives: 2.189\n",
            "\n",
            "Interval 148 (735000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0172\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_740000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 171s 34ms/step - reward: 0.0172\n",
            "8 episodes - episode_reward: 11.000 [5.000, 21.000] - loss: 0.006 - mae: 0.362 - mean_q: 0.436 - mean_eps: 0.558 - ale.lives: 2.112\n",
            "\n",
            "Interval 149 (740000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0168\n",
            "7 episodes - episode_reward: 12.429 [6.000, 20.000] - loss: 0.006 - mae: 0.359 - mean_q: 0.433 - mean_eps: 0.555 - ale.lives: 2.290\n",
            "\n",
            "Interval 150 (745000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0168\n",
            "6 episodes - episode_reward: 11.500 [4.000, 17.000] - loss: 0.006 - mae: 0.354 - mean_q: 0.426 - mean_eps: 0.552 - ale.lives: 1.941\n",
            "\n",
            "Interval 151 (750000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0154\n",
            "6 episodes - episode_reward: 16.000 [1.000, 24.000] - loss: 0.006 - mae: 0.354 - mean_q: 0.426 - mean_eps: 0.549 - ale.lives: 1.911\n",
            "\n",
            "Interval 152 (755000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0164\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_760000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 170s 34ms/step - reward: 0.0164\n",
            "9 episodes - episode_reward: 9.000 [2.000, 18.000] - loss: 0.006 - mae: 0.363 - mean_q: 0.437 - mean_eps: 0.546 - ale.lives: 2.128\n",
            "\n",
            "Interval 153 (760000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0196\n",
            "6 episodes - episode_reward: 16.500 [11.000, 21.000] - loss: 0.006 - mae: 0.362 - mean_q: 0.435 - mean_eps: 0.543 - ale.lives: 2.189\n",
            "\n",
            "Interval 154 (765000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0162\n",
            "6 episodes - episode_reward: 10.333 [7.000, 14.000] - loss: 0.006 - mae: 0.361 - mean_q: 0.434 - mean_eps: 0.540 - ale.lives: 2.172\n",
            "\n",
            "Interval 155 (770000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0160\n",
            "7 episodes - episode_reward: 13.429 [6.000, 27.000] - loss: 0.006 - mae: 0.359 - mean_q: 0.433 - mean_eps: 0.537 - ale.lives: 1.994\n",
            "\n",
            "Interval 156 (775000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0162\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_780000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 170s 34ms/step - reward: 0.0162\n",
            "7 episodes - episode_reward: 11.143 [6.000, 17.000] - loss: 0.006 - mae: 0.361 - mean_q: 0.435 - mean_eps: 0.534 - ale.lives: 1.954\n",
            "\n",
            "Interval 157 (780000 steps performed)\n",
            "5000/5000 [==============================] - 153s 30ms/step - reward: 0.0142\n",
            "7 episodes - episode_reward: 11.429 [6.000, 14.000] - loss: 0.006 - mae: 0.359 - mean_q: 0.432 - mean_eps: 0.531 - ale.lives: 1.940\n",
            "\n",
            "Interval 158 (785000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0134\n",
            "7 episodes - episode_reward: 9.429 [4.000, 15.000] - loss: 0.006 - mae: 0.363 - mean_q: 0.437 - mean_eps: 0.528 - ale.lives: 1.971\n",
            "\n",
            "Interval 159 (790000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0180\n",
            "7 episodes - episode_reward: 13.000 [5.000, 20.000] - loss: 0.006 - mae: 0.369 - mean_q: 0.444 - mean_eps: 0.525 - ale.lives: 1.977\n",
            "\n",
            "Interval 160 (795000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_800000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 34ms/step - reward: 0.0144\n",
            "6 episodes - episode_reward: 11.167 [7.000, 19.000] - loss: 0.006 - mae: 0.370 - mean_q: 0.446 - mean_eps: 0.522 - ale.lives: 1.996\n",
            "\n",
            "Interval 161 (800000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0174\n",
            "6 episodes - episode_reward: 11.333 [6.000, 19.000] - loss: 0.006 - mae: 0.373 - mean_q: 0.448 - mean_eps: 0.519 - ale.lives: 2.022\n",
            "\n",
            "Interval 162 (805000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0172\n",
            "6 episodes - episode_reward: 16.500 [9.000, 28.000] - loss: 0.006 - mae: 0.369 - mean_q: 0.444 - mean_eps: 0.516 - ale.lives: 1.838\n",
            "\n",
            "Interval 163 (810000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0200\n",
            "6 episodes - episode_reward: 16.333 [5.000, 30.000] - loss: 0.006 - mae: 0.368 - mean_q: 0.442 - mean_eps: 0.513 - ale.lives: 1.900\n",
            "\n",
            "Interval 164 (815000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0160\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_820000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 171s 34ms/step - reward: 0.0160\n",
            "8 episodes - episode_reward: 11.375 [5.000, 26.000] - loss: 0.006 - mae: 0.366 - mean_q: 0.440 - mean_eps: 0.510 - ale.lives: 2.066\n",
            "\n",
            "Interval 165 (820000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0112\n",
            "5 episodes - episode_reward: 10.800 [7.000, 16.000] - loss: 0.006 - mae: 0.359 - mean_q: 0.432 - mean_eps: 0.507 - ale.lives: 1.760\n",
            "\n",
            "Interval 166 (825000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0154\n",
            "7 episodes - episode_reward: 11.571 [6.000, 23.000] - loss: 0.006 - mae: 0.359 - mean_q: 0.432 - mean_eps: 0.504 - ale.lives: 2.026\n",
            "\n",
            "Interval 167 (830000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0180\n",
            "6 episodes - episode_reward: 12.667 [5.000, 30.000] - loss: 0.006 - mae: 0.355 - mean_q: 0.426 - mean_eps: 0.501 - ale.lives: 2.039\n",
            "\n",
            "Interval 168 (835000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0168\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_840000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 171s 34ms/step - reward: 0.0168\n",
            "6 episodes - episode_reward: 15.333 [4.000, 27.000] - loss: 0.006 - mae: 0.350 - mean_q: 0.421 - mean_eps: 0.498 - ale.lives: 1.946\n",
            "\n",
            "Interval 169 (840000 steps performed)\n",
            "5000/5000 [==============================] - 153s 30ms/step - reward: 0.0158\n",
            "6 episodes - episode_reward: 14.000 [5.000, 25.000] - loss: 0.006 - mae: 0.336 - mean_q: 0.403 - mean_eps: 0.495 - ale.lives: 1.977\n",
            "\n",
            "Interval 170 (845000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0174\n",
            "6 episodes - episode_reward: 13.667 [3.000, 26.000] - loss: 0.006 - mae: 0.335 - mean_q: 0.402 - mean_eps: 0.492 - ale.lives: 1.968\n",
            "\n",
            "Interval 171 (850000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0156\n",
            "6 episodes - episode_reward: 13.000 [10.000, 15.000] - loss: 0.006 - mae: 0.327 - mean_q: 0.392 - mean_eps: 0.489 - ale.lives: 2.063\n",
            "\n",
            "Interval 172 (855000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0152\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_860000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 170s 34ms/step - reward: 0.0152\n",
            "8 episodes - episode_reward: 8.750 [3.000, 17.000] - loss: 0.006 - mae: 0.321 - mean_q: 0.385 - mean_eps: 0.486 - ale.lives: 1.973\n",
            "\n",
            "Interval 173 (860000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0178\n",
            "6 episodes - episode_reward: 15.000 [9.000, 28.000] - loss: 0.006 - mae: 0.319 - mean_q: 0.384 - mean_eps: 0.483 - ale.lives: 1.872\n",
            "\n",
            "Interval 174 (865000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0156\n",
            "7 episodes - episode_reward: 12.143 [3.000, 30.000] - loss: 0.006 - mae: 0.310 - mean_q: 0.372 - mean_eps: 0.480 - ale.lives: 1.953\n",
            "\n",
            "Interval 175 (870000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0160\n",
            "8 episodes - episode_reward: 10.500 [3.000, 22.000] - loss: 0.006 - mae: 0.305 - mean_q: 0.366 - mean_eps: 0.477 - ale.lives: 1.957\n",
            "\n",
            "Interval 176 (875000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0178\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_880000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0178\n",
            "6 episodes - episode_reward: 14.833 [5.000, 25.000] - loss: 0.006 - mae: 0.305 - mean_q: 0.366 - mean_eps: 0.474 - ale.lives: 1.715\n",
            "\n",
            "Interval 177 (880000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0170\n",
            "6 episodes - episode_reward: 12.833 [3.000, 22.000] - loss: 0.006 - mae: 0.304 - mean_q: 0.365 - mean_eps: 0.471 - ale.lives: 2.086\n",
            "\n",
            "Interval 178 (885000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0166\n",
            "6 episodes - episode_reward: 12.833 [3.000, 26.000] - loss: 0.006 - mae: 0.301 - mean_q: 0.361 - mean_eps: 0.468 - ale.lives: 1.773\n",
            "\n",
            "Interval 179 (890000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0146\n",
            "8 episodes - episode_reward: 10.875 [6.000, 16.000] - loss: 0.006 - mae: 0.302 - mean_q: 0.362 - mean_eps: 0.465 - ale.lives: 2.104\n",
            "\n",
            "Interval 180 (895000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0174\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_900000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0174\n",
            "6 episodes - episode_reward: 14.167 [8.000, 23.000] - loss: 0.006 - mae: 0.300 - mean_q: 0.360 - mean_eps: 0.462 - ale.lives: 2.032\n",
            "\n",
            "Interval 181 (900000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0170\n",
            "6 episodes - episode_reward: 13.000 [5.000, 22.000] - loss: 0.006 - mae: 0.299 - mean_q: 0.359 - mean_eps: 0.459 - ale.lives: 1.909\n",
            "\n",
            "Interval 182 (905000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0170\n",
            "6 episodes - episode_reward: 14.000 [5.000, 22.000] - loss: 0.006 - mae: 0.301 - mean_q: 0.361 - mean_eps: 0.456 - ale.lives: 1.834\n",
            "\n",
            "Interval 183 (910000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0174\n",
            "7 episodes - episode_reward: 13.857 [6.000, 22.000] - loss: 0.007 - mae: 0.302 - mean_q: 0.363 - mean_eps: 0.453 - ale.lives: 1.994\n",
            "\n",
            "Interval 184 (915000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0182\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_920000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0182\n",
            "6 episodes - episode_reward: 14.167 [5.000, 30.000] - loss: 0.006 - mae: 0.302 - mean_q: 0.363 - mean_eps: 0.450 - ale.lives: 2.084\n",
            "\n",
            "Interval 185 (920000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0150\n",
            "6 episodes - episode_reward: 13.167 [6.000, 20.000] - loss: 0.006 - mae: 0.299 - mean_q: 0.359 - mean_eps: 0.447 - ale.lives: 2.159\n",
            "\n",
            "Interval 186 (925000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0128\n",
            "5 episodes - episode_reward: 10.800 [7.000, 16.000] - loss: 0.006 - mae: 0.298 - mean_q: 0.356 - mean_eps: 0.444 - ale.lives: 1.976\n",
            "\n",
            "Interval 187 (930000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0156\n",
            "6 episodes - episode_reward: 12.667 [4.000, 16.000] - loss: 0.006 - mae: 0.298 - mean_q: 0.357 - mean_eps: 0.441 - ale.lives: 1.913\n",
            "\n",
            "Interval 188 (935000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0188\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_940000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0188\n",
            "5 episodes - episode_reward: 18.600 [7.000, 23.000] - loss: 0.006 - mae: 0.298 - mean_q: 0.358 - mean_eps: 0.438 - ale.lives: 2.096\n",
            "\n",
            "Interval 189 (940000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0160\n",
            "7 episodes - episode_reward: 13.286 [5.000, 31.000] - loss: 0.006 - mae: 0.298 - mean_q: 0.358 - mean_eps: 0.435 - ale.lives: 1.957\n",
            "\n",
            "Interval 190 (945000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0154\n",
            "6 episodes - episode_reward: 12.833 [9.000, 19.000] - loss: 0.006 - mae: 0.296 - mean_q: 0.355 - mean_eps: 0.432 - ale.lives: 2.054\n",
            "\n",
            "Interval 191 (950000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0122\n",
            "8 episodes - episode_reward: 7.875 [5.000, 13.000] - loss: 0.006 - mae: 0.298 - mean_q: 0.357 - mean_eps: 0.429 - ale.lives: 2.191\n",
            "\n",
            "Interval 192 (955000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0166\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_960000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0166\n",
            "7 episodes - episode_reward: 9.571 [6.000, 19.000] - loss: 0.006 - mae: 0.301 - mean_q: 0.361 - mean_eps: 0.426 - ale.lives: 1.974\n",
            "\n",
            "Interval 193 (960000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0164\n",
            "7 episodes - episode_reward: 12.000 [6.000, 21.000] - loss: 0.006 - mae: 0.304 - mean_q: 0.364 - mean_eps: 0.423 - ale.lives: 1.924\n",
            "\n",
            "Interval 194 (965000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0142\n",
            "6 episodes - episode_reward: 13.500 [4.000, 19.000] - loss: 0.006 - mae: 0.310 - mean_q: 0.372 - mean_eps: 0.420 - ale.lives: 1.845\n",
            "\n",
            "Interval 195 (970000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0154\n",
            "6 episodes - episode_reward: 12.333 [3.000, 26.000] - loss: 0.006 - mae: 0.312 - mean_q: 0.374 - mean_eps: 0.417 - ale.lives: 1.859\n",
            "\n",
            "Interval 196 (975000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0152\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_980000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0152\n",
            "8 episodes - episode_reward: 9.125 [5.000, 18.000] - loss: 0.006 - mae: 0.311 - mean_q: 0.373 - mean_eps: 0.414 - ale.lives: 2.107\n",
            "\n",
            "Interval 197 (980000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0144\n",
            "6 episodes - episode_reward: 13.500 [8.000, 22.000] - loss: 0.006 - mae: 0.315 - mean_q: 0.378 - mean_eps: 0.411 - ale.lives: 2.063\n",
            "\n",
            "Interval 198 (985000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0192\n",
            "6 episodes - episode_reward: 15.500 [8.000, 27.000] - loss: 0.006 - mae: 0.315 - mean_q: 0.377 - mean_eps: 0.408 - ale.lives: 1.938\n",
            "\n",
            "Interval 199 (990000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0178\n",
            "7 episodes - episode_reward: 11.571 [5.000, 18.000] - loss: 0.006 - mae: 0.312 - mean_q: 0.374 - mean_eps: 0.405 - ale.lives: 2.150\n",
            "\n",
            "Interval 200 (995000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1000000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0144\n",
            "6 episodes - episode_reward: 13.167 [4.000, 27.000] - loss: 0.006 - mae: 0.315 - mean_q: 0.379 - mean_eps: 0.402 - ale.lives: 1.834\n",
            "\n",
            "Interval 201 (1000000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0166\n",
            "7 episodes - episode_reward: 11.571 [3.000, 30.000] - loss: 0.006 - mae: 0.317 - mean_q: 0.380 - mean_eps: 0.399 - ale.lives: 1.978\n",
            "\n",
            "Interval 202 (1005000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0190\n",
            "6 episodes - episode_reward: 16.167 [8.000, 24.000] - loss: 0.006 - mae: 0.314 - mean_q: 0.376 - mean_eps: 0.396 - ale.lives: 1.975\n",
            "\n",
            "Interval 203 (1010000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0156\n",
            "6 episodes - episode_reward: 13.667 [9.000, 20.000] - loss: 0.006 - mae: 0.311 - mean_q: 0.373 - mean_eps: 0.393 - ale.lives: 2.001\n",
            "\n",
            "Interval 204 (1015000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0190\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1020000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0190\n",
            "5 episodes - episode_reward: 17.400 [7.000, 25.000] - loss: 0.006 - mae: 0.310 - mean_q: 0.372 - mean_eps: 0.390 - ale.lives: 1.900\n",
            "\n",
            "Interval 205 (1020000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0136\n",
            "6 episodes - episode_reward: 10.333 [2.000, 16.000] - loss: 0.006 - mae: 0.311 - mean_q: 0.373 - mean_eps: 0.387 - ale.lives: 2.055\n",
            "\n",
            "Interval 206 (1025000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0158\n",
            "7 episodes - episode_reward: 13.429 [6.000, 19.000] - loss: 0.006 - mae: 0.316 - mean_q: 0.378 - mean_eps: 0.384 - ale.lives: 1.837\n",
            "\n",
            "Interval 207 (1030000 steps performed)\n",
            "5000/5000 [==============================] - 158s 31ms/step - reward: 0.0140\n",
            "6 episodes - episode_reward: 11.333 [3.000, 18.000] - loss: 0.006 - mae: 0.314 - mean_q: 0.376 - mean_eps: 0.381 - ale.lives: 2.040\n",
            "\n",
            "Interval 208 (1035000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0156\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1040000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 175s 35ms/step - reward: 0.0156\n",
            "7 episodes - episode_reward: 10.143 [6.000, 16.000] - loss: 0.006 - mae: 0.314 - mean_q: 0.376 - mean_eps: 0.378 - ale.lives: 2.041\n",
            "\n",
            "Interval 209 (1040000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0152\n",
            "7 episodes - episode_reward: 11.429 [5.000, 18.000] - loss: 0.007 - mae: 0.315 - mean_q: 0.377 - mean_eps: 0.375 - ale.lives: 2.066\n",
            "\n",
            "Interval 210 (1045000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0152\n",
            "4 episodes - episode_reward: 17.250 [7.000, 27.000] - loss: 0.006 - mae: 0.317 - mean_q: 0.380 - mean_eps: 0.372 - ale.lives: 1.828\n",
            "\n",
            "Interval 211 (1050000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0144\n",
            "6 episodes - episode_reward: 13.167 [6.000, 23.000] - loss: 0.006 - mae: 0.325 - mean_q: 0.389 - mean_eps: 0.369 - ale.lives: 1.822\n",
            "\n",
            "Interval 212 (1055000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0148\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1060000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 171s 34ms/step - reward: 0.0148\n",
            "6 episodes - episode_reward: 11.833 [6.000, 16.000] - loss: 0.006 - mae: 0.329 - mean_q: 0.394 - mean_eps: 0.366 - ale.lives: 1.919\n",
            "\n",
            "Interval 213 (1060000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0168\n",
            "7 episodes - episode_reward: 11.143 [7.000, 18.000] - loss: 0.006 - mae: 0.331 - mean_q: 0.397 - mean_eps: 0.363 - ale.lives: 2.141\n",
            "\n",
            "Interval 214 (1065000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0150\n",
            "6 episodes - episode_reward: 13.500 [6.000, 20.000] - loss: 0.006 - mae: 0.331 - mean_q: 0.397 - mean_eps: 0.360 - ale.lives: 2.120\n",
            "\n",
            "Interval 215 (1070000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0174\n",
            "7 episodes - episode_reward: 13.571 [8.000, 24.000] - loss: 0.006 - mae: 0.335 - mean_q: 0.402 - mean_eps: 0.357 - ale.lives: 2.009\n",
            "\n",
            "Interval 216 (1075000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0128\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1080000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0128\n",
            "5 episodes - episode_reward: 10.200 [7.000, 14.000] - loss: 0.006 - mae: 0.343 - mean_q: 0.412 - mean_eps: 0.354 - ale.lives: 2.020\n",
            "\n",
            "Interval 217 (1080000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0140\n",
            "6 episodes - episode_reward: 11.333 [7.000, 18.000] - loss: 0.006 - mae: 0.339 - mean_q: 0.406 - mean_eps: 0.351 - ale.lives: 2.096\n",
            "\n",
            "Interval 218 (1085000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0148\n",
            "7 episodes - episode_reward: 12.429 [5.000, 21.000] - loss: 0.006 - mae: 0.338 - mean_q: 0.404 - mean_eps: 0.348 - ale.lives: 1.942\n",
            "\n",
            "Interval 219 (1090000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0138\n",
            "7 episodes - episode_reward: 9.286 [3.000, 22.000] - loss: 0.006 - mae: 0.337 - mean_q: 0.402 - mean_eps: 0.345 - ale.lives: 2.028\n",
            "\n",
            "Interval 220 (1095000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0164\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1100000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 171s 34ms/step - reward: 0.0164\n",
            "5 episodes - episode_reward: 16.600 [6.000, 24.000] - loss: 0.006 - mae: 0.335 - mean_q: 0.402 - mean_eps: 0.342 - ale.lives: 1.985\n",
            "\n",
            "Interval 221 (1100000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0172\n",
            "5 episodes - episode_reward: 14.600 [5.000, 23.000] - loss: 0.006 - mae: 0.332 - mean_q: 0.398 - mean_eps: 0.339 - ale.lives: 1.806\n",
            "\n",
            "Interval 222 (1105000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0164\n",
            "6 episodes - episode_reward: 16.000 [5.000, 20.000] - loss: 0.007 - mae: 0.332 - mean_q: 0.397 - mean_eps: 0.336 - ale.lives: 1.871\n",
            "\n",
            "Interval 223 (1110000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0140\n",
            "6 episodes - episode_reward: 11.333 [7.000, 14.000] - loss: 0.006 - mae: 0.332 - mean_q: 0.397 - mean_eps: 0.333 - ale.lives: 2.112\n",
            "\n",
            "Interval 224 (1115000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0156\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1120000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0156\n",
            "5 episodes - episode_reward: 14.600 [10.000, 23.000] - loss: 0.006 - mae: 0.329 - mean_q: 0.393 - mean_eps: 0.330 - ale.lives: 1.864\n",
            "\n",
            "Interval 225 (1120000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0170\n",
            "6 episodes - episode_reward: 15.000 [11.000, 27.000] - loss: 0.006 - mae: 0.330 - mean_q: 0.395 - mean_eps: 0.327 - ale.lives: 2.038\n",
            "\n",
            "Interval 226 (1125000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0130\n",
            "4 episodes - episode_reward: 17.000 [12.000, 22.000] - loss: 0.006 - mae: 0.323 - mean_q: 0.388 - mean_eps: 0.324 - ale.lives: 2.053\n",
            "\n",
            "Interval 227 (1130000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0156\n",
            "5 episodes - episode_reward: 12.200 [6.000, 20.000] - loss: 0.006 - mae: 0.321 - mean_q: 0.385 - mean_eps: 0.321 - ale.lives: 2.159\n",
            "\n",
            "Interval 228 (1135000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0146\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1140000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 174s 35ms/step - reward: 0.0146\n",
            "8 episodes - episode_reward: 11.625 [3.000, 22.000] - loss: 0.006 - mae: 0.318 - mean_q: 0.382 - mean_eps: 0.318 - ale.lives: 1.955\n",
            "\n",
            "Interval 229 (1140000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0172\n",
            "5 episodes - episode_reward: 14.800 [6.000, 22.000] - loss: 0.006 - mae: 0.313 - mean_q: 0.375 - mean_eps: 0.315 - ale.lives: 2.141\n",
            "\n",
            "Interval 230 (1145000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0164\n",
            "6 episodes - episode_reward: 14.333 [6.000, 27.000] - loss: 0.006 - mae: 0.309 - mean_q: 0.370 - mean_eps: 0.312 - ale.lives: 1.948\n",
            "\n",
            "Interval 231 (1150000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0128\n",
            "6 episodes - episode_reward: 11.000 [8.000, 17.000] - loss: 0.006 - mae: 0.306 - mean_q: 0.367 - mean_eps: 0.309 - ale.lives: 1.996\n",
            "\n",
            "Interval 232 (1155000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0154\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1160000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0154\n",
            "7 episodes - episode_reward: 11.857 [2.000, 22.000] - loss: 0.006 - mae: 0.305 - mean_q: 0.364 - mean_eps: 0.306 - ale.lives: 2.165\n",
            "\n",
            "Interval 233 (1160000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0162\n",
            "5 episodes - episode_reward: 15.000 [10.000, 22.000] - loss: 0.006 - mae: 0.304 - mean_q: 0.364 - mean_eps: 0.303 - ale.lives: 1.848\n",
            "\n",
            "Interval 234 (1165000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0146\n",
            "5 episodes - episode_reward: 15.800 [7.000, 28.000] - loss: 0.006 - mae: 0.304 - mean_q: 0.365 - mean_eps: 0.300 - ale.lives: 1.785\n",
            "\n",
            "Interval 235 (1170000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0132\n",
            "4 episodes - episode_reward: 14.500 [6.000, 22.000] - loss: 0.006 - mae: 0.305 - mean_q: 0.364 - mean_eps: 0.297 - ale.lives: 1.845\n",
            "\n",
            "Interval 236 (1175000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0164\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1180000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 174s 35ms/step - reward: 0.0164\n",
            "6 episodes - episode_reward: 14.167 [6.000, 25.000] - loss: 0.006 - mae: 0.305 - mean_q: 0.365 - mean_eps: 0.294 - ale.lives: 1.872\n",
            "\n",
            "Interval 237 (1180000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0180\n",
            "5 episodes - episode_reward: 16.600 [14.000, 19.000] - loss: 0.006 - mae: 0.307 - mean_q: 0.368 - mean_eps: 0.291 - ale.lives: 2.070\n",
            "\n",
            "Interval 238 (1185000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0170\n",
            "6 episodes - episode_reward: 15.167 [6.000, 21.000] - loss: 0.006 - mae: 0.310 - mean_q: 0.371 - mean_eps: 0.288 - ale.lives: 1.963\n",
            "\n",
            "Interval 239 (1190000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0150\n",
            "7 episodes - episode_reward: 10.714 [5.000, 31.000] - loss: 0.006 - mae: 0.309 - mean_q: 0.369 - mean_eps: 0.285 - ale.lives: 1.871\n",
            "\n",
            "Interval 240 (1195000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0160\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1200000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 171s 34ms/step - reward: 0.0160\n",
            "5 episodes - episode_reward: 16.200 [9.000, 24.000] - loss: 0.006 - mae: 0.307 - mean_q: 0.368 - mean_eps: 0.282 - ale.lives: 1.917\n",
            "\n",
            "Interval 241 (1200000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0148\n",
            "5 episodes - episode_reward: 14.800 [9.000, 19.000] - loss: 0.006 - mae: 0.308 - mean_q: 0.369 - mean_eps: 0.279 - ale.lives: 1.950\n",
            "\n",
            "Interval 242 (1205000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0166\n",
            "6 episodes - episode_reward: 14.667 [9.000, 27.000] - loss: 0.006 - mae: 0.307 - mean_q: 0.367 - mean_eps: 0.276 - ale.lives: 2.080\n",
            "\n",
            "Interval 243 (1210000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0186\n",
            "6 episodes - episode_reward: 15.500 [9.000, 26.000] - loss: 0.006 - mae: 0.304 - mean_q: 0.364 - mean_eps: 0.273 - ale.lives: 2.194\n",
            "\n",
            "Interval 244 (1215000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1220000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0144\n",
            "6 episodes - episode_reward: 11.667 [5.000, 20.000] - loss: 0.006 - mae: 0.303 - mean_q: 0.362 - mean_eps: 0.270 - ale.lives: 2.128\n",
            "\n",
            "Interval 245 (1220000 steps performed)\n",
            "5000/5000 [==============================] - 153s 30ms/step - reward: 0.0152\n",
            "5 episodes - episode_reward: 14.400 [3.000, 27.000] - loss: 0.006 - mae: 0.299 - mean_q: 0.358 - mean_eps: 0.267 - ale.lives: 2.040\n",
            "\n",
            "Interval 246 (1225000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0156\n",
            "5 episodes - episode_reward: 13.800 [7.000, 25.000] - loss: 0.006 - mae: 0.296 - mean_q: 0.354 - mean_eps: 0.264 - ale.lives: 1.940\n",
            "\n",
            "Interval 247 (1230000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0172\n",
            "6 episodes - episode_reward: 15.167 [8.000, 18.000] - loss: 0.006 - mae: 0.290 - mean_q: 0.347 - mean_eps: 0.261 - ale.lives: 2.322\n",
            "\n",
            "Interval 248 (1235000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0142\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1240000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0142\n",
            "5 episodes - episode_reward: 15.600 [11.000, 21.000] - loss: 0.006 - mae: 0.284 - mean_q: 0.340 - mean_eps: 0.258 - ale.lives: 2.122\n",
            "\n",
            "Interval 249 (1240000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0150\n",
            "4 episodes - episode_reward: 15.500 [10.000, 25.000] - loss: 0.006 - mae: 0.281 - mean_q: 0.335 - mean_eps: 0.255 - ale.lives: 2.016\n",
            "\n",
            "Interval 250 (1245000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0176\n",
            "6 episodes - episode_reward: 16.000 [7.000, 21.000] - loss: 0.006 - mae: 0.279 - mean_q: 0.334 - mean_eps: 0.252 - ale.lives: 2.050\n",
            "\n",
            "Interval 251 (1250000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0148\n",
            "5 episodes - episode_reward: 14.400 [9.000, 19.000] - loss: 0.006 - mae: 0.279 - mean_q: 0.334 - mean_eps: 0.249 - ale.lives: 1.892\n",
            "\n",
            "Interval 252 (1255000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0150\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1260000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 34ms/step - reward: 0.0150\n",
            "7 episodes - episode_reward: 12.143 [7.000, 25.000] - loss: 0.006 - mae: 0.278 - mean_q: 0.332 - mean_eps: 0.246 - ale.lives: 1.875\n",
            "\n",
            "Interval 253 (1260000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0184\n",
            "4 episodes - episode_reward: 21.500 [14.000, 30.000] - loss: 0.006 - mae: 0.276 - mean_q: 0.331 - mean_eps: 0.243 - ale.lives: 2.119\n",
            "\n",
            "Interval 254 (1265000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0154\n",
            "5 episodes - episode_reward: 16.000 [9.000, 26.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.330 - mean_eps: 0.240 - ale.lives: 1.908\n",
            "\n",
            "Interval 255 (1270000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0162\n",
            "7 episodes - episode_reward: 11.857 [4.000, 23.000] - loss: 0.006 - mae: 0.276 - mean_q: 0.331 - mean_eps: 0.237 - ale.lives: 1.964\n",
            "\n",
            "Interval 256 (1275000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0190\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1280000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0190\n",
            "6 episodes - episode_reward: 16.000 [11.000, 26.000] - loss: 0.005 - mae: 0.275 - mean_q: 0.329 - mean_eps: 0.234 - ale.lives: 2.055\n",
            "\n",
            "Interval 257 (1280000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0158\n",
            "6 episodes - episode_reward: 11.000 [6.000, 15.000] - loss: 0.006 - mae: 0.277 - mean_q: 0.331 - mean_eps: 0.231 - ale.lives: 2.002\n",
            "\n",
            "Interval 258 (1285000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0138\n",
            "6 episodes - episode_reward: 12.667 [6.000, 19.000] - loss: 0.006 - mae: 0.278 - mean_q: 0.333 - mean_eps: 0.228 - ale.lives: 1.879\n",
            "\n",
            "Interval 259 (1290000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0150\n",
            "6 episodes - episode_reward: 11.333 [4.000, 17.000] - loss: 0.006 - mae: 0.278 - mean_q: 0.333 - mean_eps: 0.225 - ale.lives: 2.097\n",
            "\n",
            "Interval 260 (1295000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0172\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1300000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 171s 34ms/step - reward: 0.0172\n",
            "4 episodes - episode_reward: 18.500 [9.000, 27.000] - loss: 0.006 - mae: 0.278 - mean_q: 0.332 - mean_eps: 0.222 - ale.lives: 2.017\n",
            "\n",
            "Interval 261 (1300000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0166\n",
            "8 episodes - episode_reward: 12.750 [6.000, 25.000] - loss: 0.006 - mae: 0.272 - mean_q: 0.327 - mean_eps: 0.219 - ale.lives: 1.972\n",
            "\n",
            "Interval 262 (1305000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0156\n",
            "6 episodes - episode_reward: 13.833 [11.000, 24.000] - loss: 0.006 - mae: 0.272 - mean_q: 0.326 - mean_eps: 0.216 - ale.lives: 1.828\n",
            "\n",
            "Interval 263 (1310000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0180\n",
            "4 episodes - episode_reward: 21.750 [17.000, 29.000] - loss: 0.006 - mae: 0.270 - mean_q: 0.323 - mean_eps: 0.213 - ale.lives: 2.028\n",
            "\n",
            "Interval 264 (1315000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0142\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1320000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0142\n",
            "5 episodes - episode_reward: 15.000 [11.000, 19.000] - loss: 0.006 - mae: 0.268 - mean_q: 0.321 - mean_eps: 0.210 - ale.lives: 1.998\n",
            "\n",
            "Interval 265 (1320000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0146\n",
            "5 episodes - episode_reward: 13.200 [6.000, 20.000] - loss: 0.006 - mae: 0.264 - mean_q: 0.316 - mean_eps: 0.207 - ale.lives: 2.052\n",
            "\n",
            "Interval 266 (1325000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0168\n",
            "5 episodes - episode_reward: 14.800 [9.000, 22.000] - loss: 0.006 - mae: 0.262 - mean_q: 0.314 - mean_eps: 0.204 - ale.lives: 1.888\n",
            "\n",
            "Interval 267 (1330000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0178\n",
            "5 episodes - episode_reward: 19.400 [12.000, 29.000] - loss: 0.006 - mae: 0.265 - mean_q: 0.317 - mean_eps: 0.201 - ale.lives: 1.835\n",
            "\n",
            "Interval 268 (1335000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0134\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1340000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0134\n",
            "5 episodes - episode_reward: 12.800 [7.000, 23.000] - loss: 0.006 - mae: 0.264 - mean_q: 0.315 - mean_eps: 0.198 - ale.lives: 1.848\n",
            "\n",
            "Interval 269 (1340000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0188\n",
            "5 episodes - episode_reward: 17.000 [14.000, 22.000] - loss: 0.006 - mae: 0.263 - mean_q: 0.315 - mean_eps: 0.195 - ale.lives: 2.234\n",
            "\n",
            "Interval 270 (1345000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0148\n",
            "6 episodes - episode_reward: 15.333 [7.000, 29.000] - loss: 0.006 - mae: 0.264 - mean_q: 0.316 - mean_eps: 0.192 - ale.lives: 1.830\n",
            "\n",
            "Interval 271 (1350000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0192\n",
            "6 episodes - episode_reward: 16.500 [12.000, 19.000] - loss: 0.006 - mae: 0.265 - mean_q: 0.318 - mean_eps: 0.189 - ale.lives: 1.964\n",
            "\n",
            "Interval 272 (1355000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0162\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1360000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 175s 35ms/step - reward: 0.0162\n",
            "6 episodes - episode_reward: 12.333 [6.000, 16.000] - loss: 0.006 - mae: 0.265 - mean_q: 0.317 - mean_eps: 0.186 - ale.lives: 2.024\n",
            "\n",
            "Interval 273 (1360000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0162\n",
            "8 episodes - episode_reward: 10.625 [6.000, 25.000] - loss: 0.006 - mae: 0.267 - mean_q: 0.319 - mean_eps: 0.183 - ale.lives: 2.247\n",
            "\n",
            "Interval 274 (1365000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0170\n",
            "5 episodes - episode_reward: 13.800 [5.000, 25.000] - loss: 0.006 - mae: 0.270 - mean_q: 0.322 - mean_eps: 0.180 - ale.lives: 2.048\n",
            "\n",
            "Interval 275 (1370000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0184\n",
            "8 episodes - episode_reward: 13.125 [3.000, 23.000] - loss: 0.006 - mae: 0.270 - mean_q: 0.324 - mean_eps: 0.177 - ale.lives: 2.000\n",
            "\n",
            "Interval 276 (1375000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0166\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1380000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 174s 35ms/step - reward: 0.0166\n",
            "7 episodes - episode_reward: 12.714 [3.000, 20.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.328 - mean_eps: 0.174 - ale.lives: 2.161\n",
            "\n",
            "Interval 277 (1380000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0150\n",
            "6 episodes - episode_reward: 12.500 [4.000, 18.000] - loss: 0.006 - mae: 0.276 - mean_q: 0.330 - mean_eps: 0.171 - ale.lives: 2.022\n",
            "\n",
            "Interval 278 (1385000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0146\n",
            "4 episodes - episode_reward: 16.000 [12.000, 23.000] - loss: 0.006 - mae: 0.274 - mean_q: 0.328 - mean_eps: 0.168 - ale.lives: 2.089\n",
            "\n",
            "Interval 279 (1390000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0174\n",
            "6 episodes - episode_reward: 14.833 [6.000, 25.000] - loss: 0.006 - mae: 0.273 - mean_q: 0.328 - mean_eps: 0.165 - ale.lives: 2.167\n",
            "\n",
            "Interval 280 (1395000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0172\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1400000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 172s 34ms/step - reward: 0.0172\n",
            "5 episodes - episode_reward: 15.000 [9.000, 31.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.330 - mean_eps: 0.162 - ale.lives: 1.906\n",
            "\n",
            "Interval 281 (1400000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0156\n",
            "6 episodes - episode_reward: 14.000 [10.000, 29.000] - loss: 0.006 - mae: 0.276 - mean_q: 0.331 - mean_eps: 0.159 - ale.lives: 2.031\n",
            "\n",
            "Interval 282 (1405000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0130\n",
            "6 episodes - episode_reward: 12.667 [9.000, 22.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.329 - mean_eps: 0.156 - ale.lives: 2.005\n",
            "\n",
            "Interval 283 (1410000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0166\n",
            "5 episodes - episode_reward: 15.600 [7.000, 19.000] - loss: 0.006 - mae: 0.277 - mean_q: 0.332 - mean_eps: 0.153 - ale.lives: 2.125\n",
            "\n",
            "Interval 284 (1415000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0188\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1420000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0188\n",
            "7 episodes - episode_reward: 14.286 [4.000, 20.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.329 - mean_eps: 0.150 - ale.lives: 2.052\n",
            "\n",
            "Interval 285 (1420000 steps performed)\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0146\n",
            "3 episodes - episode_reward: 19.667 [16.000, 24.000] - loss: 0.006 - mae: 0.274 - mean_q: 0.329 - mean_eps: 0.147 - ale.lives: 1.878\n",
            "\n",
            "Interval 286 (1425000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0168\n",
            "6 episodes - episode_reward: 15.000 [6.000, 26.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.328 - mean_eps: 0.144 - ale.lives: 2.012\n",
            "\n",
            "Interval 287 (1430000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0160\n",
            "7 episodes - episode_reward: 12.143 [7.000, 21.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.329 - mean_eps: 0.141 - ale.lives: 2.232\n",
            "\n",
            "Interval 288 (1435000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0156\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1440000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 173s 35ms/step - reward: 0.0156\n",
            "4 episodes - episode_reward: 15.000 [11.000, 18.000] - loss: 0.006 - mae: 0.276 - mean_q: 0.330 - mean_eps: 0.138 - ale.lives: 1.788\n",
            "\n",
            "Interval 289 (1440000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0148\n",
            "5 episodes - episode_reward: 16.400 [10.000, 26.000] - loss: 0.006 - mae: 0.273 - mean_q: 0.328 - mean_eps: 0.135 - ale.lives: 2.133\n",
            "\n",
            "Interval 290 (1445000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0140\n",
            "6 episodes - episode_reward: 13.833 [5.000, 19.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.330 - mean_eps: 0.132 - ale.lives: 1.938\n",
            "\n",
            "Interval 291 (1450000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0146\n",
            "4 episodes - episode_reward: 13.500 [7.000, 18.000] - loss: 0.006 - mae: 0.277 - mean_q: 0.332 - mean_eps: 0.129 - ale.lives: 1.786\n",
            "\n",
            "Interval 292 (1455000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0160\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1460000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 175s 35ms/step - reward: 0.0160\n",
            "5 episodes - episode_reward: 18.600 [10.000, 34.000] - loss: 0.006 - mae: 0.279 - mean_q: 0.334 - mean_eps: 0.126 - ale.lives: 2.251\n",
            "\n",
            "Interval 293 (1460000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 13.167 [8.000, 21.000] - loss: 0.006 - mae: 0.278 - mean_q: 0.334 - mean_eps: 0.123 - ale.lives: 2.116\n",
            "\n",
            "Interval 294 (1465000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0168\n",
            "6 episodes - episode_reward: 15.000 [7.000, 26.000] - loss: 0.006 - mae: 0.277 - mean_q: 0.332 - mean_eps: 0.120 - ale.lives: 1.975\n",
            "\n",
            "Interval 295 (1470000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0154\n",
            "5 episodes - episode_reward: 14.800 [6.000, 23.000] - loss: 0.006 - mae: 0.278 - mean_q: 0.332 - mean_eps: 0.117 - ale.lives: 2.202\n",
            "\n",
            "Interval 296 (1475000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0184\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1480000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 175s 35ms/step - reward: 0.0184\n",
            "5 episodes - episode_reward: 19.000 [7.000, 48.000] - loss: 0.006 - mae: 0.274 - mean_q: 0.328 - mean_eps: 0.114 - ale.lives: 1.835\n",
            "\n",
            "Interval 297 (1480000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0122\n",
            "6 episodes - episode_reward: 9.333 [5.000, 22.000] - loss: 0.006 - mae: 0.274 - mean_q: 0.327 - mean_eps: 0.111 - ale.lives: 1.977\n",
            "\n",
            "Interval 298 (1485000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0166\n",
            "4 episodes - episode_reward: 20.250 [9.000, 24.000] - loss: 0.006 - mae: 0.271 - mean_q: 0.325 - mean_eps: 0.108 - ale.lives: 2.259\n",
            "\n",
            "Interval 299 (1490000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0134\n",
            "5 episodes - episode_reward: 12.400 [6.000, 20.000] - loss: 0.006 - mae: 0.269 - mean_q: 0.322 - mean_eps: 0.105 - ale.lives: 1.961\n",
            "\n",
            "Interval 300 (1495000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0178\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1500000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 178s 36ms/step - reward: 0.0178\n",
            "5 episodes - episode_reward: 13.400 [7.000, 26.000] - loss: 0.006 - mae: 0.267 - mean_q: 0.319 - mean_eps: 0.102 - ale.lives: 2.335\n",
            "\n",
            "Interval 301 (1500000 steps performed)\n",
            "5000/5000 [==============================] - 163s 33ms/step - reward: 0.0158\n",
            "6 episodes - episode_reward: 18.833 [9.000, 35.000] - loss: 0.006 - mae: 0.266 - mean_q: 0.320 - mean_eps: 0.100 - ale.lives: 2.118\n",
            "\n",
            "Interval 302 (1505000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0168\n",
            "6 episodes - episode_reward: 13.167 [6.000, 17.000] - loss: 0.006 - mae: 0.269 - mean_q: 0.322 - mean_eps: 0.100 - ale.lives: 2.062\n",
            "\n",
            "Interval 303 (1510000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0154\n",
            "6 episodes - episode_reward: 12.000 [5.000, 16.000] - loss: 0.006 - mae: 0.269 - mean_q: 0.323 - mean_eps: 0.100 - ale.lives: 2.116\n",
            "\n",
            "Interval 304 (1515000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0164\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1520000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 176s 35ms/step - reward: 0.0164\n",
            "6 episodes - episode_reward: 15.500 [4.000, 23.000] - loss: 0.006 - mae: 0.265 - mean_q: 0.317 - mean_eps: 0.100 - ale.lives: 1.772\n",
            "\n",
            "Interval 305 (1520000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0154\n",
            "5 episodes - episode_reward: 15.200 [12.000, 18.000] - loss: 0.006 - mae: 0.262 - mean_q: 0.315 - mean_eps: 0.100 - ale.lives: 1.931\n",
            "\n",
            "Interval 306 (1525000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0188\n",
            "4 episodes - episode_reward: 17.500 [10.000, 34.000] - loss: 0.006 - mae: 0.263 - mean_q: 0.315 - mean_eps: 0.100 - ale.lives: 1.914\n",
            "\n",
            "Interval 307 (1530000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0168\n",
            "4 episodes - episode_reward: 25.250 [18.000, 32.000] - loss: 0.006 - mae: 0.268 - mean_q: 0.321 - mean_eps: 0.100 - ale.lives: 1.798\n",
            "\n",
            "Interval 308 (1535000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0152\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1540000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 175s 35ms/step - reward: 0.0152\n",
            "6 episodes - episode_reward: 13.000 [5.000, 17.000] - loss: 0.006 - mae: 0.268 - mean_q: 0.322 - mean_eps: 0.100 - ale.lives: 1.963\n",
            "\n",
            "Interval 309 (1540000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0188\n",
            "5 episodes - episode_reward: 15.800 [8.000, 29.000] - loss: 0.006 - mae: 0.272 - mean_q: 0.326 - mean_eps: 0.100 - ale.lives: 1.940\n",
            "\n",
            "Interval 310 (1545000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 15.333 [10.000, 27.000] - loss: 0.006 - mae: 0.271 - mean_q: 0.325 - mean_eps: 0.100 - ale.lives: 1.977\n",
            "\n",
            "Interval 311 (1550000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0158\n",
            "6 episodes - episode_reward: 13.667 [6.000, 19.000] - loss: 0.006 - mae: 0.271 - mean_q: 0.325 - mean_eps: 0.100 - ale.lives: 1.999\n",
            "\n",
            "Interval 312 (1555000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0142\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1560000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 175s 35ms/step - reward: 0.0142\n",
            "5 episodes - episode_reward: 14.200 [7.000, 24.000] - loss: 0.006 - mae: 0.270 - mean_q: 0.324 - mean_eps: 0.100 - ale.lives: 1.925\n",
            "\n",
            "Interval 313 (1560000 steps performed)\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0138\n",
            "6 episodes - episode_reward: 11.667 [10.000, 13.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.331 - mean_eps: 0.100 - ale.lives: 1.990\n",
            "\n",
            "Interval 314 (1565000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0206\n",
            "5 episodes - episode_reward: 20.400 [14.000, 29.000] - loss: 0.006 - mae: 0.272 - mean_q: 0.327 - mean_eps: 0.100 - ale.lives: 1.856\n",
            "\n",
            "Interval 315 (1570000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0172\n",
            "6 episodes - episode_reward: 13.167 [6.000, 20.000] - loss: 0.006 - mae: 0.270 - mean_q: 0.324 - mean_eps: 0.100 - ale.lives: 2.030\n",
            "\n",
            "Interval 316 (1575000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0174\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1580000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 175s 35ms/step - reward: 0.0174\n",
            "7 episodes - episode_reward: 14.286 [6.000, 22.000] - loss: 0.006 - mae: 0.272 - mean_q: 0.326 - mean_eps: 0.100 - ale.lives: 2.044\n",
            "\n",
            "Interval 317 (1580000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0180\n",
            "5 episodes - episode_reward: 16.200 [11.000, 24.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.330 - mean_eps: 0.100 - ale.lives: 2.093\n",
            "\n",
            "Interval 318 (1585000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0166\n",
            "5 episodes - episode_reward: 16.200 [11.000, 20.000] - loss: 0.006 - mae: 0.276 - mean_q: 0.331 - mean_eps: 0.100 - ale.lives: 1.979\n",
            "\n",
            "Interval 319 (1590000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0140\n",
            "7 episodes - episode_reward: 10.571 [3.000, 17.000] - loss: 0.006 - mae: 0.274 - mean_q: 0.329 - mean_eps: 0.100 - ale.lives: 1.775\n",
            "\n",
            "Interval 320 (1595000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0194\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1600000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 176s 35ms/step - reward: 0.0194\n",
            "7 episodes - episode_reward: 14.000 [7.000, 20.000] - loss: 0.006 - mae: 0.273 - mean_q: 0.327 - mean_eps: 0.100 - ale.lives: 2.043\n",
            "\n",
            "Interval 321 (1600000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0188\n",
            "5 episodes - episode_reward: 15.600 [7.000, 20.000] - loss: 0.006 - mae: 0.274 - mean_q: 0.329 - mean_eps: 0.100 - ale.lives: 1.977\n",
            "\n",
            "Interval 322 (1605000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0174\n",
            "6 episodes - episode_reward: 17.167 [6.000, 28.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.330 - mean_eps: 0.100 - ale.lives: 1.996\n",
            "\n",
            "Interval 323 (1610000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0230\n",
            "6 episodes - episode_reward: 17.833 [8.000, 26.000] - loss: 0.006 - mae: 0.277 - mean_q: 0.333 - mean_eps: 0.100 - ale.lives: 2.243\n",
            "\n",
            "Interval 324 (1615000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0146\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1620000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 176s 35ms/step - reward: 0.0146\n",
            "6 episodes - episode_reward: 13.833 [9.000, 19.000] - loss: 0.006 - mae: 0.276 - mean_q: 0.332 - mean_eps: 0.100 - ale.lives: 2.051\n",
            "\n",
            "Interval 325 (1620000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0172\n",
            "5 episodes - episode_reward: 16.200 [12.000, 23.000] - loss: 0.006 - mae: 0.279 - mean_q: 0.335 - mean_eps: 0.100 - ale.lives: 2.120\n",
            "\n",
            "Interval 326 (1625000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0166\n",
            "8 episodes - episode_reward: 10.500 [3.000, 17.000] - loss: 0.007 - mae: 0.282 - mean_q: 0.338 - mean_eps: 0.100 - ale.lives: 1.962\n",
            "\n",
            "Interval 327 (1630000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0182\n",
            "6 episodes - episode_reward: 15.333 [6.000, 24.000] - loss: 0.006 - mae: 0.280 - mean_q: 0.336 - mean_eps: 0.100 - ale.lives: 2.187\n",
            "\n",
            "Interval 328 (1635000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0182\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1640000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 176s 35ms/step - reward: 0.0182\n",
            "7 episodes - episode_reward: 11.143 [5.000, 17.000] - loss: 0.006 - mae: 0.281 - mean_q: 0.338 - mean_eps: 0.100 - ale.lives: 2.185\n",
            "\n",
            "Interval 329 (1640000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0162\n",
            "6 episodes - episode_reward: 16.167 [9.000, 22.000] - loss: 0.006 - mae: 0.283 - mean_q: 0.339 - mean_eps: 0.100 - ale.lives: 2.080\n",
            "\n",
            "Interval 330 (1645000 steps performed)\n",
            "5000/5000 [==============================] - 158s 31ms/step - reward: 0.0176\n",
            "5 episodes - episode_reward: 16.800 [8.000, 30.000] - loss: 0.006 - mae: 0.282 - mean_q: 0.339 - mean_eps: 0.100 - ale.lives: 1.822\n",
            "\n",
            "Interval 331 (1650000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0178\n",
            "6 episodes - episode_reward: 15.333 [6.000, 24.000] - loss: 0.006 - mae: 0.278 - mean_q: 0.335 - mean_eps: 0.100 - ale.lives: 2.022\n",
            "\n",
            "Interval 332 (1655000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1660000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0144\n",
            "4 episodes - episode_reward: 17.750 [13.000, 23.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.330 - mean_eps: 0.100 - ale.lives: 1.894\n",
            "\n",
            "Interval 333 (1660000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0156\n",
            "5 episodes - episode_reward: 13.600 [11.000, 18.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.330 - mean_eps: 0.100 - ale.lives: 1.968\n",
            "\n",
            "Interval 334 (1665000 steps performed)\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 14.667 [7.000, 25.000] - loss: 0.006 - mae: 0.275 - mean_q: 0.331 - mean_eps: 0.100 - ale.lives: 1.942\n",
            "\n",
            "Interval 335 (1670000 steps performed)\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 14.667 [8.000, 19.000] - loss: 0.006 - mae: 0.282 - mean_q: 0.339 - mean_eps: 0.100 - ale.lives: 2.011\n",
            "\n",
            "Interval 336 (1675000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0178\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1680000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 176s 35ms/step - reward: 0.0178\n",
            "5 episodes - episode_reward: 15.800 [12.000, 21.000] - loss: 0.006 - mae: 0.287 - mean_q: 0.345 - mean_eps: 0.100 - ale.lives: 1.922\n",
            "\n",
            "Interval 337 (1680000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0156\n",
            "6 episodes - episode_reward: 14.000 [6.000, 23.000] - loss: 0.006 - mae: 0.291 - mean_q: 0.350 - mean_eps: 0.100 - ale.lives: 2.139\n",
            "\n",
            "Interval 338 (1685000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0156\n",
            "6 episodes - episode_reward: 12.167 [10.000, 16.000] - loss: 0.007 - mae: 0.294 - mean_q: 0.353 - mean_eps: 0.100 - ale.lives: 1.945\n",
            "\n",
            "Interval 339 (1690000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0142\n",
            "4 episodes - episode_reward: 16.500 [12.000, 23.000] - loss: 0.006 - mae: 0.296 - mean_q: 0.355 - mean_eps: 0.100 - ale.lives: 2.161\n",
            "\n",
            "Interval 340 (1695000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0152\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1700000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0152\n",
            "6 episodes - episode_reward: 12.833 [10.000, 17.000] - loss: 0.006 - mae: 0.299 - mean_q: 0.358 - mean_eps: 0.100 - ale.lives: 2.033\n",
            "\n",
            "Interval 341 (1700000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0146\n",
            "7 episodes - episode_reward: 11.429 [6.000, 17.000] - loss: 0.007 - mae: 0.303 - mean_q: 0.363 - mean_eps: 0.100 - ale.lives: 2.295\n",
            "\n",
            "Interval 342 (1705000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0164\n",
            "7 episodes - episode_reward: 11.143 [6.000, 18.000] - loss: 0.007 - mae: 0.302 - mean_q: 0.363 - mean_eps: 0.100 - ale.lives: 2.134\n",
            "\n",
            "Interval 343 (1710000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0144\n",
            "5 episodes - episode_reward: 16.400 [10.000, 23.000] - loss: 0.006 - mae: 0.301 - mean_q: 0.362 - mean_eps: 0.100 - ale.lives: 2.032\n",
            "\n",
            "Interval 344 (1715000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0170\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1720000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 176s 35ms/step - reward: 0.0170\n",
            "5 episodes - episode_reward: 15.800 [7.000, 24.000] - loss: 0.006 - mae: 0.303 - mean_q: 0.364 - mean_eps: 0.100 - ale.lives: 1.998\n",
            "\n",
            "Interval 345 (1720000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0136\n",
            "6 episodes - episode_reward: 12.167 [7.000, 26.000] - loss: 0.006 - mae: 0.306 - mean_q: 0.368 - mean_eps: 0.100 - ale.lives: 2.057\n",
            "\n",
            "Interval 346 (1725000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0130\n",
            "4 episodes - episode_reward: 14.750 [9.000, 19.000] - loss: 0.007 - mae: 0.307 - mean_q: 0.369 - mean_eps: 0.100 - ale.lives: 1.991\n",
            "\n",
            "Interval 347 (1730000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0176\n",
            "6 episodes - episode_reward: 15.667 [11.000, 20.000] - loss: 0.006 - mae: 0.309 - mean_q: 0.371 - mean_eps: 0.100 - ale.lives: 2.076\n",
            "\n",
            "Interval 348 (1735000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1740000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 176s 35ms/step - reward: 0.0144\n",
            "5 episodes - episode_reward: 12.600 [7.000, 24.000] - loss: 0.006 - mae: 0.310 - mean_q: 0.373 - mean_eps: 0.100 - ale.lives: 2.027\n",
            "\n",
            "Interval 349 (1740000 steps performed)\n",
            "5000/5000 [==============================] - 158s 31ms/step - reward: 0.0194\n",
            "5 episodes - episode_reward: 17.800 [8.000, 23.000] - loss: 0.006 - mae: 0.309 - mean_q: 0.371 - mean_eps: 0.100 - ale.lives: 1.933\n",
            "\n",
            "Interval 350 (1745000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 14.167 [8.000, 21.000] - loss: 0.007 - mae: 0.309 - mean_q: 0.372 - mean_eps: 0.100 - ale.lives: 2.048\n",
            "\n",
            "Interval 351 (1750000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0162\n",
            "5 episodes - episode_reward: 18.800 [10.000, 29.000] - loss: 0.006 - mae: 0.307 - mean_q: 0.370 - mean_eps: 0.100 - ale.lives: 1.904\n",
            "\n",
            "Interval 352 (1755000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0164\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1760000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0164\n",
            "6 episodes - episode_reward: 12.667 [5.000, 19.000] - loss: 0.006 - mae: 0.312 - mean_q: 0.376 - mean_eps: 0.100 - ale.lives: 2.094\n",
            "\n",
            "Interval 353 (1760000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0156\n",
            "6 episodes - episode_reward: 13.000 [6.000, 20.000] - loss: 0.006 - mae: 0.313 - mean_q: 0.375 - mean_eps: 0.100 - ale.lives: 1.952\n",
            "\n",
            "Interval 354 (1765000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0178\n",
            "4 episodes - episode_reward: 22.250 [12.000, 28.000] - loss: 0.006 - mae: 0.311 - mean_q: 0.374 - mean_eps: 0.100 - ale.lives: 2.162\n",
            "\n",
            "Interval 355 (1770000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0166\n",
            "5 episodes - episode_reward: 16.600 [12.000, 28.000] - loss: 0.006 - mae: 0.313 - mean_q: 0.376 - mean_eps: 0.100 - ale.lives: 2.006\n",
            "\n",
            "Interval 356 (1775000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0152\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1780000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0152\n",
            "6 episodes - episode_reward: 12.833 [3.000, 19.000] - loss: 0.007 - mae: 0.313 - mean_q: 0.377 - mean_eps: 0.100 - ale.lives: 1.908\n",
            "\n",
            "Interval 357 (1780000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0154\n",
            "5 episodes - episode_reward: 13.600 [9.000, 22.000] - loss: 0.006 - mae: 0.307 - mean_q: 0.369 - mean_eps: 0.100 - ale.lives: 2.014\n",
            "\n",
            "Interval 358 (1785000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0156\n",
            "7 episodes - episode_reward: 10.143 [5.000, 19.000] - loss: 0.007 - mae: 0.305 - mean_q: 0.366 - mean_eps: 0.100 - ale.lives: 2.030\n",
            "\n",
            "Interval 359 (1790000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0120\n",
            "6 episodes - episode_reward: 11.500 [6.000, 21.000] - loss: 0.007 - mae: 0.310 - mean_q: 0.373 - mean_eps: 0.100 - ale.lives: 2.005\n",
            "\n",
            "Interval 360 (1795000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0134\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1800000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0134\n",
            "5 episodes - episode_reward: 14.600 [11.000, 22.000] - loss: 0.006 - mae: 0.318 - mean_q: 0.382 - mean_eps: 0.100 - ale.lives: 1.979\n",
            "\n",
            "Interval 361 (1800000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0200\n",
            "7 episodes - episode_reward: 14.286 [6.000, 20.000] - loss: 0.006 - mae: 0.330 - mean_q: 0.396 - mean_eps: 0.100 - ale.lives: 2.052\n",
            "\n",
            "Interval 362 (1805000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0148\n",
            "5 episodes - episode_reward: 14.800 [12.000, 19.000] - loss: 0.006 - mae: 0.346 - mean_q: 0.417 - mean_eps: 0.100 - ale.lives: 2.104\n",
            "\n",
            "Interval 363 (1810000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0148\n",
            "6 episodes - episode_reward: 12.333 [10.000, 17.000] - loss: 0.006 - mae: 0.356 - mean_q: 0.429 - mean_eps: 0.100 - ale.lives: 2.099\n",
            "\n",
            "Interval 364 (1815000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0166\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1820000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 176s 35ms/step - reward: 0.0166\n",
            "6 episodes - episode_reward: 12.500 [6.000, 20.000] - loss: 0.006 - mae: 0.362 - mean_q: 0.436 - mean_eps: 0.100 - ale.lives: 1.915\n",
            "\n",
            "Interval 365 (1820000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0160\n",
            "7 episodes - episode_reward: 12.286 [5.000, 18.000] - loss: 0.006 - mae: 0.363 - mean_q: 0.436 - mean_eps: 0.100 - ale.lives: 1.883\n",
            "\n",
            "Interval 366 (1825000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0152\n",
            "6 episodes - episode_reward: 13.000 [6.000, 17.000] - loss: 0.007 - mae: 0.363 - mean_q: 0.438 - mean_eps: 0.100 - ale.lives: 1.900\n",
            "\n",
            "Interval 367 (1830000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0168\n",
            "6 episodes - episode_reward: 15.000 [10.000, 21.000] - loss: 0.006 - mae: 0.370 - mean_q: 0.446 - mean_eps: 0.100 - ale.lives: 2.048\n",
            "\n",
            "Interval 368 (1835000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0164\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1840000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0164\n",
            "5 episodes - episode_reward: 16.400 [8.000, 21.000] - loss: 0.006 - mae: 0.378 - mean_q: 0.455 - mean_eps: 0.100 - ale.lives: 1.929\n",
            "\n",
            "Interval 369 (1840000 steps performed)\n",
            "5000/5000 [==============================] - 158s 31ms/step - reward: 0.0150\n",
            "6 episodes - episode_reward: 12.500 [10.000, 16.000] - loss: 0.007 - mae: 0.383 - mean_q: 0.461 - mean_eps: 0.100 - ale.lives: 1.974\n",
            "\n",
            "Interval 370 (1845000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0150\n",
            "4 episodes - episode_reward: 16.250 [8.000, 27.000] - loss: 0.006 - mae: 0.391 - mean_q: 0.471 - mean_eps: 0.100 - ale.lives: 1.999\n",
            "\n",
            "Interval 371 (1850000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0156\n",
            "6 episodes - episode_reward: 14.167 [12.000, 20.000] - loss: 0.006 - mae: 0.396 - mean_q: 0.476 - mean_eps: 0.100 - ale.lives: 2.111\n",
            "\n",
            "Interval 372 (1855000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0170\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1860000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0170\n",
            "5 episodes - episode_reward: 16.000 [8.000, 26.000] - loss: 0.006 - mae: 0.391 - mean_q: 0.471 - mean_eps: 0.100 - ale.lives: 2.091\n",
            "\n",
            "Interval 373 (1860000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0128\n",
            "6 episodes - episode_reward: 11.000 [5.000, 22.000] - loss: 0.006 - mae: 0.392 - mean_q: 0.472 - mean_eps: 0.100 - ale.lives: 2.036\n",
            "\n",
            "Interval 374 (1865000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0194\n",
            "5 episodes - episode_reward: 18.800 [6.000, 30.000] - loss: 0.006 - mae: 0.391 - mean_q: 0.470 - mean_eps: 0.100 - ale.lives: 2.379\n",
            "\n",
            "Interval 375 (1870000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0198\n",
            "6 episodes - episode_reward: 15.833 [6.000, 26.000] - loss: 0.007 - mae: 0.389 - mean_q: 0.468 - mean_eps: 0.100 - ale.lives: 2.204\n",
            "\n",
            "Interval 376 (1875000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0136\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1880000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0136\n",
            "6 episodes - episode_reward: 12.500 [3.000, 18.000] - loss: 0.006 - mae: 0.390 - mean_q: 0.469 - mean_eps: 0.100 - ale.lives: 2.042\n",
            "\n",
            "Interval 377 (1880000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0124\n",
            "7 episodes - episode_reward: 7.857 [1.000, 14.000] - loss: 0.006 - mae: 0.388 - mean_q: 0.467 - mean_eps: 0.100 - ale.lives: 1.918\n",
            "\n",
            "Interval 378 (1885000 steps performed)\n",
            "5000/5000 [==============================] - 158s 31ms/step - reward: 0.0130\n",
            "6 episodes - episode_reward: 10.667 [5.000, 22.000] - loss: 0.007 - mae: 0.385 - mean_q: 0.464 - mean_eps: 0.100 - ale.lives: 2.228\n",
            "\n",
            "Interval 379 (1890000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0156\n",
            "6 episodes - episode_reward: 12.667 [6.000, 19.000] - loss: 0.007 - mae: 0.397 - mean_q: 0.478 - mean_eps: 0.100 - ale.lives: 1.956\n",
            "\n",
            "Interval 380 (1895000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0132\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1900000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0132\n",
            "7 episodes - episode_reward: 11.143 [1.000, 22.000] - loss: 0.007 - mae: 0.405 - mean_q: 0.488 - mean_eps: 0.100 - ale.lives: 1.902\n",
            "\n",
            "Interval 381 (1900000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0144\n",
            "4 episodes - episode_reward: 17.000 [11.000, 28.000] - loss: 0.007 - mae: 0.416 - mean_q: 0.500 - mean_eps: 0.100 - ale.lives: 2.134\n",
            "\n",
            "Interval 382 (1905000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0150\n",
            "6 episodes - episode_reward: 13.833 [12.000, 17.000] - loss: 0.007 - mae: 0.427 - mean_q: 0.514 - mean_eps: 0.100 - ale.lives: 1.862\n",
            "\n",
            "Interval 383 (1910000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0132\n",
            "7 episodes - episode_reward: 9.000 [0.000, 19.000] - loss: 0.007 - mae: 0.432 - mean_q: 0.521 - mean_eps: 0.100 - ale.lives: 1.936\n",
            "\n",
            "Interval 384 (1915000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0156\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1920000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0156\n",
            "5 episodes - episode_reward: 13.000 [3.000, 23.000] - loss: 0.007 - mae: 0.442 - mean_q: 0.533 - mean_eps: 0.100 - ale.lives: 2.174\n",
            "\n",
            "Interval 385 (1920000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0126\n",
            "7 episodes - episode_reward: 9.714 [4.000, 18.000] - loss: 0.007 - mae: 0.454 - mean_q: 0.546 - mean_eps: 0.100 - ale.lives: 2.038\n",
            "\n",
            "Interval 386 (1925000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0146\n",
            "6 episodes - episode_reward: 14.000 [8.000, 20.000] - loss: 0.006 - mae: 0.463 - mean_q: 0.558 - mean_eps: 0.100 - ale.lives: 2.054\n",
            "\n",
            "Interval 387 (1930000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0140\n",
            "4 episodes - episode_reward: 17.250 [14.000, 21.000] - loss: 0.006 - mae: 0.470 - mean_q: 0.565 - mean_eps: 0.100 - ale.lives: 1.927\n",
            "\n",
            "Interval 388 (1935000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0130\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1940000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0130\n",
            "5 episodes - episode_reward: 12.000 [10.000, 16.000] - loss: 0.006 - mae: 0.466 - mean_q: 0.561 - mean_eps: 0.100 - ale.lives: 1.915\n",
            "\n",
            "Interval 389 (1940000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0142\n",
            "5 episodes - episode_reward: 12.800 [1.000, 24.000] - loss: 0.006 - mae: 0.460 - mean_q: 0.553 - mean_eps: 0.100 - ale.lives: 2.073\n",
            "\n",
            "Interval 390 (1945000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0146\n",
            "6 episodes - episode_reward: 12.667 [7.000, 17.000] - loss: 0.006 - mae: 0.452 - mean_q: 0.544 - mean_eps: 0.100 - ale.lives: 1.837\n",
            "\n",
            "Interval 391 (1950000 steps performed)\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0128\n",
            "4 episodes - episode_reward: 18.500 [15.000, 23.000] - loss: 0.006 - mae: 0.451 - mean_q: 0.543 - mean_eps: 0.100 - ale.lives: 2.175\n",
            "\n",
            "Interval 392 (1955000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0150\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1960000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 178s 35ms/step - reward: 0.0150\n",
            "6 episodes - episode_reward: 11.333 [7.000, 16.000] - loss: 0.007 - mae: 0.454 - mean_q: 0.547 - mean_eps: 0.100 - ale.lives: 1.951\n",
            "\n",
            "Interval 393 (1960000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0136\n",
            "6 episodes - episode_reward: 10.500 [8.000, 17.000] - loss: 0.006 - mae: 0.457 - mean_q: 0.550 - mean_eps: 0.100 - ale.lives: 1.911\n",
            "\n",
            "Interval 394 (1965000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0154\n",
            "5 episodes - episode_reward: 15.800 [7.000, 22.000] - loss: 0.007 - mae: 0.457 - mean_q: 0.550 - mean_eps: 0.100 - ale.lives: 2.039\n",
            "\n",
            "Interval 395 (1970000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0160\n",
            "7 episodes - episode_reward: 12.143 [10.000, 15.000] - loss: 0.006 - mae: 0.458 - mean_q: 0.552 - mean_eps: 0.100 - ale.lives: 2.087\n",
            "\n",
            "Interval 396 (1975000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0152\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_1980000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0152\n",
            "5 episodes - episode_reward: 15.400 [12.000, 22.000] - loss: 0.006 - mae: 0.461 - mean_q: 0.554 - mean_eps: 0.100 - ale.lives: 1.763\n",
            "\n",
            "Interval 397 (1980000 steps performed)\n",
            "5000/5000 [==============================] - 158s 31ms/step - reward: 0.0150\n",
            "5 episodes - episode_reward: 12.400 [9.000, 16.000] - loss: 0.006 - mae: 0.455 - mean_q: 0.547 - mean_eps: 0.100 - ale.lives: 2.025\n",
            "\n",
            "Interval 398 (1985000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0122\n",
            "4 episodes - episode_reward: 17.000 [9.000, 23.000] - loss: 0.006 - mae: 0.452 - mean_q: 0.543 - mean_eps: 0.100 - ale.lives: 1.796\n",
            "\n",
            "Interval 399 (1990000 steps performed)\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0162\n",
            "7 episodes - episode_reward: 12.857 [8.000, 19.000] - loss: 0.006 - mae: 0.444 - mean_q: 0.534 - mean_eps: 0.100 - ale.lives: 1.865\n",
            "\n",
            "Interval 400 (1995000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0134\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1_Plus\\checkpoints\\dqn_SpaceInvaders-v0_weights_2000000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1_Plus\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 177s 35ms/step - reward: 0.0134\n",
            "done, took 61541.391 seconds\n"
          ]
        }
      ],
      "source": [
        "# ENTRENAR DQN\n",
        "if TRAIN_STEPS>0:\n",
        "  dqn.fit(env, callbacks=callbacks, nb_steps=TRAIN_STEPS-last_checkpoint_steps, log_interval=LOG_INTERVAL, visualize=False)\n",
        "  dqn.save_weights(weights_filename, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHYryKd1Gb2b",
        "outputId": "cf7d86ae-ab74-4723-8367-ffadc582e464",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 10.000, steps: 1259\n",
            "Episode 2: reward: 11.000, steps: 1087\n",
            "Episode 3: reward: 1.000, steps: 576\n",
            "Episode 4: reward: 12.000, steps: 1474\n",
            "Episode 5: reward: 6.000, steps: 704\n",
            "Episode 6: reward: 13.000, steps: 1247\n",
            "Episode 7: reward: 5.000, steps: 384\n",
            "Episode 8: reward: 9.000, steps: 717\n",
            "Episode 9: reward: 14.000, steps: 1228\n",
            "Episode 10: reward: 13.000, steps: 1243\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x22234d48708>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing part to calculate the mean reward\n",
        "weights_filename =os.path.join(WEIGHTS_DIR, 'dqn_{}_weights_{}.h5f'.format(env_name, model_name))\n",
        "#weights_filename=latest_checkpoint\n",
        "#print(weights_filename)\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1ne1WCO5ISG"
      },
      "outputs": [],
      "source": [
        "# graph_training_csv(log_csv_path, MODEL_DIR, model_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv_rl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
