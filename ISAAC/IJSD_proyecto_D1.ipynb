{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUehXgCyIRdq"
      },
      "source": [
        "# Actividad - Proyecto práctico\n",
        "\n",
        "\n",
        "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
        "*   Alumno 1: de Antón Santiago, Sara\n",
        "*   Alumno 2: Sánchez La O, Benjamín C.\n",
        "*   Alumno 3: Sánchez Díaz, Isaac José\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwpYlnjWJhS9"
      },
      "source": [
        "---\n",
        "## **PARTE 1** - Instalación y requisitos previos\n",
        "\n",
        "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
        "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
        "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
        "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
        "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU2BPrK2JkP0"
      },
      "source": [
        "---\n",
        "### 1.1. Preparar enviroment (solo local)\n",
        "\n",
        "\n",
        "\n",
        "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
        "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
        "2. Instalar Anaconda\n",
        "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
        "\n",
        "\n",
        "```\n",
        "conda create --name miar_rl python=3.8\n",
        "conda activate miar_rl\n",
        "cd \"PATH_TO_FOLDER\"\n",
        "conda install git\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "\n",
        "4. Abrir la notebook con *jupyter-notebook*.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "jupyter-notebook\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-kixNPiJqTc"
      },
      "source": [
        "---\n",
        "### 1.2. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S_YDFwZ-JscI",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dp_a1iBJ0tf"
      },
      "source": [
        "---\n",
        "### 1.3. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6n7MIefJ21i",
        "outputId": "bce44695-c427-4ed6-b1c1-a0df92594a1d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "/content/gdrive/My Drive/08_MIAR/actividades/proyecto practico\n",
            "Archivos en el directorio: \n",
            "['models']\n"
          ]
        }
      ],
      "source": [
        "# Switch to the directory on the Google Drive that you want to use\n",
        "import os\n",
        "if IN_COLAB:\n",
        "  print(\"We're running Colab\")\n",
        "\n",
        "  if IN_COLAB:\n",
        "    # Mount the Google Drive at mount\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "      os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "# Verify we're in the correct working directory\n",
        "%pwd\n",
        "print(\"Archivos en el directorio: \")\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ZSL5bpJ560"
      },
      "source": [
        "---\n",
        "### 1.4. Instalar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UbVRjvHCJ8UF",
        "outputId": "8bee6aeb-0bde-457b-dd3a-0761bb830498",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gym==0.17.3\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.17.3) (2.0.2)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
            "Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654617 sha256=ca109cf0abb6f7bb0aa1926f17659222e2863f152c8cccbe192ff3a74399b8b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/8b/b7/570cb90b10f17e85ccb291ba1f04af41ec697745104a2263eb\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, cloudpickle, gym\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.1\n",
            "    Uninstalling cloudpickle-3.1.1:\n",
            "      Successfully uninstalled cloudpickle-3.1.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "distributed 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "bigframes 2.6.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "dask 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0\n",
            "Collecting git+https://github.com/Kojoley/atari-py.git\n",
            "  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-ip_kiwml\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-ip_kiwml\n",
            "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from atari-py==1.2.2) (2.0.2)\n",
            "Building wheels for collected packages: atari-py\n",
            "  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atari-py: filename=atari_py-1.2.2-cp311-cp311-linux_x86_64.whl size=4738734 sha256=433fdc30a4117f9973381b18ce3bdbf12a664dbd6c7183816c836a5a5b847b8c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-70y04z4m/wheels/1a/58/b3/3baab9d1509939ecce2dfd9ca349c222b7ee6590f4bd6097a1\n",
            "Successfully built atari-py\n",
            "Installing collected packages: atari-py\n",
            "Successfully installed atari-py-1.2.2\n",
            "Collecting keras-rl2==1.0.5\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl.metadata (304 bytes)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from keras-rl2==1.0.5) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->keras-rl2==1.0.5) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2==1.0.5) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->keras-rl2==1.0.5) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->keras-rl2==1.0.5) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->keras-rl2==1.0.5) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->keras-rl2==1.0.5) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->keras-rl2==1.0.5) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->keras-rl2==1.0.5) (0.1.2)\n",
            "Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n",
            "Collecting tensorflow==2.12\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.73.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.5.2)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.14.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.2,>=0.6.2 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.1,>=0.6.1 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.15.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "orbax-checkpoint 0.11.15 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "bigframes 2.6.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.6.0 which is incompatible.\n",
            "bigframes 2.6.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.4.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 protobuf-4.25.8 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "465c88e2998345eaacd8b679612498ea",
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# NO EJECUTAR EN SAGEMAKER (MIRAR EL README PARA CONFIGURAR EL ENTORNO)\n",
        "# AUNQUE DE FALLOS DE INSTALACIÓN LOS IMPORTS FUNCIONAN\n",
        "if IN_COLAB:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install tensorflow==2.12  #2.8\n",
        "else:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install pyglet==1.5.0\n",
        "  %pip install h5py==3.1.0\n",
        "  %pip install Pillow==9.5.0\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install Keras==2.2.4\n",
        "  %pip install tensorflow==2.5.3\n",
        "  %pip install torch==2.0.1\n",
        "  %pip install agents==1.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hzP_5ZuGb2X"
      },
      "source": [
        "---\n",
        "## **PARTE 2**. Enunciado\n",
        "\n",
        "Consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
        "\n",
        "Este proyecto práctico consta de tres partes:\n",
        "\n",
        "1.   Implementar la red neuronal que se usará en la solución\n",
        "2.   Implementar las distintas piezas de la solución DQN\n",
        "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
        "\n",
        "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
        "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
        "* Cada alumno deberá de subir la solución de forma individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_b3mzw8IzJP"
      },
      "source": [
        "---\n",
        "## **PARTE 3**. Desarrollo y preguntas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "45BPUdIoB41o",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# DEFINIR AL PRINCIPIO\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duPmUNOVGb2a"
      },
      "source": [
        "#### Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j3eRhgI-Gb2a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "# AÑADIDO\n",
        "from tensorflow.keras.layers import Lambda, BatchNormalization\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import pickle\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4jgQjzoGb2a"
      },
      "source": [
        "#### Configuración base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jwOE6I_KGb2a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9jGEZUcpGb2a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LVgXEu006eFy"
      },
      "outputs": [],
      "source": [
        "hiperparametros = {\n",
        "    \"MEMORY_SIZE\": 300000,\n",
        "    \"WARMUP_STEPS\": 30000,\n",
        "    \"SCHEDULER_STEPS\": 1000000,\n",
        "    \"GAMMA\": 0.97,\n",
        "    \"MODEL_UPDATE\": 5000,\n",
        "    \"LEARNING_RATE\": 0.00025,\n",
        "    \"MODEL_CHECKPOINT_STEPS\": 10000,\n",
        "    \"TRAIN_STEPS\": 1800000,\n",
        "    \"LOG_INTERVAL\": 5000,\n",
        "    \"DELTA_CLIP\": 0.7\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WRL-ixOyPIo7"
      },
      "outputs": [],
      "source": [
        "# ROOT PATH PARA LOS MODELOS Y SUS PESOS\n",
        "if IN_COLAB:\n",
        "  mount='/content/gdrive'\n",
        "  drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "  MODELS_DIR=drive_root+\"/models\"\n",
        "else:\n",
        "  MODELS_DIR=\"./models\"\n",
        "\n",
        "def get_dirs(model_name=\"modelo1\"):\n",
        "    WEIGHTS_DIR = os.path.join(MODELS_DIR, model_name, \"weights\")\n",
        "    CHECKPOINTS_DIR = os.path.join(MODELS_DIR, model_name, \"checkpoints\")\n",
        "    MODEL_DIR = os.path.join(MODELS_DIR, model_name)\n",
        "    os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
        "    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
        "    return MODEL_DIR, WEIGHTS_DIR, CHECKPOINTS_DIR\n",
        "\n",
        "def save_hyperparams(modelo):\n",
        "    \"\"\"\n",
        "    Guarda los hiperparámetros actuales en el fichero JSON.\n",
        "    \"\"\"\n",
        "    hyper_file = os.path.join(MODELS_DIR, modelo, modelo + '.json')\n",
        "    with open(hyper_file, 'w') as f:\n",
        "        json.dump(hiperparametros, f, indent=4)\n",
        "    print(f\"[INFO] Hiperparámetros guardados en {hyper_file}\")\n",
        "\n",
        "\n",
        "def load_hyperparams(modelo):\n",
        "    \"\"\"\n",
        "    Actualiza los hiperparámetros definidos en memoria a los del fichero cargado.\n",
        "    Si el fichero no existe, lo crea con los valores por defecto (hiperparametros).\n",
        "    \"\"\"\n",
        "    hyper_file = os.path.join(MODELS_DIR, modelo, modelo + '.json')\n",
        "\n",
        "    # Crear el directorio del modelo si no existe\n",
        "    os.makedirs(os.path.dirname(hyper_file), exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(hyper_file):\n",
        "        # Guardar el fichero con los hiperparámetros por defecto\n",
        "        with open(hyper_file, 'w') as f:\n",
        "            json.dump(hiperparametros, f, indent=4)\n",
        "\n",
        "        print(f\"[INFO] Fichero de hiperparámetros no encontrado. Creado por defecto en: {hyper_file}\")\n",
        "        params = hiperparametros\n",
        "    else:\n",
        "        # El fichero existe: lo leemos\n",
        "        with open(hyper_file, 'r') as f:\n",
        "            params = json.load(f)\n",
        "\n",
        "    # Asignar dinámicamente los valores\n",
        "    for key, value in params.items():\n",
        "        globals()[key] = value\n",
        "        hiperparametros[key] = value  # Actualiza el diccionario en memoria\n",
        "\n",
        "    print(f\"[INFO] Hiperparámetros cargados desde {hyper_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9KxgJ090eku"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN PARA PLOTEAR LOGS DEL TRAINING *** SARA ***\n",
        "'''\n",
        "def graph_training_csv(csv_path, model_dir, model_name, save_clean_csv=False):\n",
        "    if not os.path.isfile(csv_path):\n",
        "        print(f\"[ERROR] El archivo '{csv_path}' no existe.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, usecols=[0, 1, 2])  # Usa encabezados del archivo\n",
        "        df.columns = ['episode_jump', 'episode_reward', 'nb_steps']\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] No se pudo leer el CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    # Ignorar la primera fila de datos (por ejemplo, episodio 1)\n",
        "    df = df.iloc[1:].reset_index(drop=True)\n",
        "\n",
        "    # Convertir nb_steps a entero seguro\n",
        "    df['nb_steps'] = pd.to_numeric(df['nb_steps'], errors='coerce')\n",
        "    df = df.dropna(subset=['nb_steps'])\n",
        "    df['nb_steps'] = df['nb_steps'].astype(int)\n",
        "\n",
        "    fixed_steps = []\n",
        "    accumulated_steps = 0\n",
        "    previous_step = df['nb_steps'].iloc[0]\n",
        "\n",
        "    for s in df['nb_steps']:\n",
        "      if s < previous_step:\n",
        "          accumulated_steps += previous_step\n",
        "          value= s + accumulated_steps\n",
        "      else:\n",
        "          if accumulated_steps == 0:\n",
        "              value = s\n",
        "          else:\n",
        "              value = s+ accumulated_steps\n",
        "      previous_step = s\n",
        "      fixed_steps.append(value)\n",
        "\n",
        "\n",
        "    df['fixed_steps'] = fixed_steps\n",
        "    print(f\"[INFO] Último valor en 'fixed_steps': {fixed_steps[-1]}\")\n",
        "\n",
        "    # Graficar\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(df['fixed_steps'], df['episode_reward'])\n",
        "    plt.title(f\"{model_name}: Episode Reward vs Steps\")\n",
        "    plt.xlabel(\"Steps\")\n",
        "    plt.ylabel(\"Episode Reward\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    output_path = os.path.join(model_dir, f\"{model_name}_episode_reward_fixed_steps.png\")\n",
        "    plt.savefig(output_path)\n",
        "    print(f\"[INFO] Gráfica guardada en: {output_path}\")\n",
        "    plt.show()\n",
        "\n",
        "    if save_clean_csv:\n",
        "        clean_csv_path = os.path.join(model_dir, f\"{model_name}_cleaned_log_fixed.csv\")\n",
        "        df.to_csv(clean_csv_path, index=False)\n",
        "        print(f\"[INFO] CSV corregido guardado en: {clean_csv_path}\")\n",
        "  '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Gd1JuAi9rCqr"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN PARA PLOTEAR LOGS DEL TRAINING *** BENJAMIN ***\n",
        "def analyze_training(model_name, window_size):\n",
        "    \"\"\"\n",
        "    Analiza el log de entrenamiento de un modelo de RL, genera gráficos de evolución\n",
        "    de métricas y un informe textual. Incluye un informe de métricas globales y otro\n",
        "    centrado en las métricas después de los window_size ultimos episodios.\n",
        "    Los graficos y datos son guardados en MODEL_DIR/graphs por fecha.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): El nombre del modelo.\n",
        "        window_size (int): Tamaño de la ventana para la media móvil.\n",
        "    \"\"\"\n",
        "    MODEL_DIR = os.path.join(\"./models\", model_name)\n",
        "    log_csv_path = os.path.join(MODEL_DIR, f'{model_name}_training_log.csv')\n",
        "\n",
        "    if not os.path.exists(log_csv_path):\n",
        "        print(f\"Error: El archivo de log '{log_csv_path}' no se encontró.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(log_csv_path)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"El DataFrame está vacío, no se puede continuar.\")\n",
        "        return\n",
        "\n",
        "    # Calcular medias móviles\n",
        "    df['reward_smooth'] = df['episode_reward'].rolling(window=window_size).mean()\n",
        "    df['loss_smooth'] = df['loss'].rolling(window=window_size).mean()\n",
        "    df['q_smooth'] = df['mean_q'].rolling(window=window_size).mean()\n",
        "    df['eps_smooth'] = df['mean_eps'].rolling(window=window_size).mean()\n",
        "\n",
        "    # Generar gráficos\n",
        "    _plot_metrics(df, model_name, window_size, MODEL_DIR)\n",
        "\n",
        "    # Informe global\n",
        "    print(\"\\nINFORME DEL TRAINING (Todas las métricas)\\n\" + \"-\" * 40)\n",
        "    _print_report(df)\n",
        "\n",
        "    # Informe últimos episodios\n",
        "    _analyze_training_last(df, model_name, window_size, MODEL_DIR)\n",
        "\n",
        "def _plot_metrics(df, model_name, window_size, model_dir):\n",
        "    sns.set(style=\"darkgrid\", font_scale=1.2)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "    plots_info = [\n",
        "        ('episode_reward', 'reward_smooth', 'Recompensa por episodio', 'Recompensa', axes[0, 0]),\n",
        "        ('loss', 'loss_smooth', 'Pérdida (loss) por episodio', 'Loss', axes[0, 1]),\n",
        "        ('mean_q', 'q_smooth', 'Q medio por episodio', 'Mean Q', axes[1, 0]),\n",
        "        ('mean_eps', 'eps_smooth', 'Epsilon medio por episodio', 'Mean Eps', axes[1, 1]),\n",
        "    ]\n",
        "\n",
        "    for orig_col, smooth_col, title, ylabel, ax in plots_info:\n",
        "        sns.lineplot(x='episode', y=orig_col, data=df, marker='o', markersize=4, label='Original', ax=ax)\n",
        "        sns.lineplot(x='episode', y=smooth_col, data=df, color='red', linewidth=2, label=f'Media móvil ({window_size})', ax=ax)\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel('Episodio')\n",
        "        ax.set_ylabel(ylabel)\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "        unique = dict(zip(labels, handles))\n",
        "        ax.legend(unique.values(), unique.keys(), loc='best')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    graphs_dir = os.path.join(model_dir, \"graphs\")\n",
        "    os.makedirs(graphs_dir, exist_ok=True)\n",
        "\n",
        "    datetime_stamp = pd.to_datetime('today').strftime('%Y%m%d%H%M%S')\n",
        "    graph_path = os.path.join(graphs_dir, f'{datetime_stamp}_{model_name}_training_analyze_graph.png')\n",
        "    csv_path = os.path.join(graphs_dir, f'{datetime_stamp}_{model_name}_training_analyze_log.csv')\n",
        "\n",
        "    plt.savefig(graph_path, dpi=300)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Gráfico guardado en: {graph_path}\")\n",
        "    print(f\"CSV de informe guardado en: {csv_path}\")\n",
        "\n",
        "def _print_report(df):\n",
        "    print(f\"Episodios totales: {df['episode'].max()}\")\n",
        "    print(f\"Recompensa media: {df['episode_reward'].mean():.2f}\")\n",
        "    print(f\"Recompensa máxima: {df['episode_reward'].max()}\")\n",
        "    print(f\"Recompensa mínima: {df['episode_reward'].min()}\")\n",
        "    print(f\"Loss medio: {df['loss'].mean(skipna=True):.6f}\")\n",
        "    print(f\"Mean Q medio: {df['mean_q'].mean(skipna=True):.6f}\")\n",
        "    print(f\"Epsilon medio: {df['mean_eps'].mean(skipna=True):.6f}\")\n",
        "    print(f\"Pasos medios por episodio: {df['nb_steps'].mean():.2f}\")\n",
        "\n",
        "    if len(df) > 1:\n",
        "        reward_diff = df['episode_reward'].iloc[-1] - df['episode_reward'].iloc[0]\n",
        "        if reward_diff > 0:\n",
        "            print(f\"La recompensa final ({df['episode_reward'].iloc[-1]:.2f}) es mayor que la inicial ({df['episode_reward'].iloc[0]:.2f}), indicando una mejora.\")\n",
        "        else:\n",
        "            print(f\"La recompensa final ({df['episode_reward'].iloc[-1]:.2f}) no ha mejorado significativamente respecto a la inicial ({df['episode_reward'].iloc[0]:.2f}).\")\n",
        "    else:\n",
        "        print(\"No hay suficientes episodios para evaluar la evolución de la recompensa global.\")\n",
        "\n",
        "def _analyze_training_last(df_full, model_name, window_size, model_dir):\n",
        "    latest_log_path = _get_latest_log_file(model_dir, model_name)\n",
        "    if not latest_log_path:\n",
        "        print(\"No se encontró un CSV de análisis previo para los últimos episodios.\")\n",
        "        return\n",
        "\n",
        "    df = pd.read_csv(latest_log_path)\n",
        "    if df.empty:\n",
        "        print(\"El DataFrame de los últimos episodios está vacío.\")\n",
        "        return\n",
        "\n",
        "    df = df.tail(window_size + 1).iloc[:-1]\n",
        "\n",
        "    print(f\"\\nINFORME DEL TRAINING (últimos {window_size} episodios completados {df['episode'].min()} al {df['episode'].max()})\\n\" + \"-\" * 40)\n",
        "    _print_report(df)\n",
        "\n",
        "def _get_latest_log_file(model_dir, model_name):\n",
        "    pattern = os.path.join(model_dir, \"graphs\", f'*_{model_name}_training_analyze_log.csv')\n",
        "    files = glob.glob(pattern)\n",
        "    if not files:\n",
        "        return None\n",
        "    return max(files, key=os.path.basename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qUOajl31fsW7"
      },
      "outputs": [],
      "source": [
        "# CALLBACK CUSTOM DEL LOGGER PARA PODER GUARDAR TODA LA INFO EN UN MISMO FICHERO YA QUE ANTES SE SOBREESCRIBÍA\n",
        "# BSL: AÑADIDAS NUEVAS MÉTRICAS AL LOGGER\n",
        "class EpisodeLoggerCSV(Callback):\n",
        "    def __init__(self, filepath, verbose=False):\n",
        "        super().__init__()\n",
        "        self.filepath = filepath\n",
        "        self.verbose = verbose\n",
        "        self.fields = [\n",
        "            'episode', 'episode_reward', 'nb_steps', 'duration',\n",
        "            'loss', 'mae', 'mean_q', 'mean_eps', 'ale.lives',\n",
        "            'reward_min', 'reward_max'\n",
        "        ]\n",
        "        self.episode_count = 0\n",
        "        self.file = None\n",
        "        self.writer = None\n",
        "        # Acumuladores por episodio\n",
        "        self._reset_episode_stats()\n",
        "\n",
        "    def _reset_episode_stats(self):\n",
        "        self.losses = []\n",
        "        self.q_values = []\n",
        "        self.maes = []\n",
        "        self.epsilons = []\n",
        "        self.lives = []\n",
        "        self.reward_values = []\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        file_exists = os.path.exists(self.filepath)\n",
        "        self.file = open(self.filepath, mode='a', newline='')\n",
        "        self.writer = csv.DictWriter(self.file, fieldnames=self.fields)\n",
        "        if not file_exists:\n",
        "            self.writer.writeheader()\n",
        "\n",
        "    def on_step_end(self, step, logs=None):\n",
        "        logs = logs or {}\n",
        "        if 'metrics' in logs and logs['metrics'] is not None:\n",
        "            metrics = logs['metrics']\n",
        "            if len(metrics) >= 4:\n",
        "                loss, mae, mean_q, mean_eps = metrics\n",
        "                if not np.isnan(loss):\n",
        "                    self.losses.append(loss)\n",
        "                if not np.isnan(mae):\n",
        "                    self.maes.append(mae)\n",
        "                if not np.isnan(mean_q):\n",
        "                    self.q_values.append(mean_q)\n",
        "                if not np.isnan(mean_eps):\n",
        "                    self.epsilons.append(mean_eps)\n",
        "        if 'reward' in logs:\n",
        "            self.reward_values.append(logs['reward'])\n",
        "        if 'info' in logs and 'ale.lives' in logs['info']:\n",
        "            self.lives.append(float(logs['info']['ale.lives']))\n",
        "\n",
        "    def on_episode_end(self, episode, logs=None):\n",
        "        self.episode_count += 1\n",
        "        logs = logs or {}\n",
        "\n",
        "        row = {\n",
        "            'episode': self.episode_count,\n",
        "            'episode_reward': logs.get('episode_reward'),\n",
        "            'nb_steps': logs.get('nb_steps'),\n",
        "            'duration': logs.get('duration'),\n",
        "            'loss': np.mean(self.losses) if self.losses else None,\n",
        "            'mae': np.mean(self.maes) if self.maes else None,\n",
        "            'mean_q': np.mean(self.q_values) if self.q_values else None,\n",
        "            'mean_eps': np.mean(self.epsilons) if self.epsilons else None,\n",
        "            'ale.lives': np.mean(self.lives) if self.lives else None,\n",
        "            'reward_min': np.min(self.reward_values) if self.reward_values else None,\n",
        "            'reward_max': np.max(self.reward_values) if self.reward_values else None\n",
        "        }\n",
        "\n",
        "        self.writer.writerow(row)\n",
        "        self.file.flush()\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"[Log CSV] Episodio {row['episode']} - Recompensa: {row['episode_reward']} - \"\n",
        "                  f\"Loss: {row['loss']}, MAE: {row['mae']}, Mean Q: {row['mean_q']}\")\n",
        "\n",
        "        self._reset_episode_stats()\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.file:\n",
        "            self.file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jqpjeC-6J9xP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# CALLBACK CUSTOM PARA GUARDAR LOS CHECKPOINTS CON EL NOMBRE BIEN CUANDO SE REINICIA EL ENTRENAMIENTO Y LA MEMORIA\n",
        "class AccumulatedCheckpoint(Callback):\n",
        "    def __init__(self, base_path, env_name, interval, initial_step=0):\n",
        "        super().__init__()\n",
        "        self.base_path = base_path\n",
        "        self.weights_path = os.path.join(base_path, f'dqn_{env_name}_weights_{{step}}.h5f')\n",
        "        self.memory_path= os.path.join(base_path,'memory.pkl')\n",
        "        self.policy_state_path = os.path.join(base_path, 'policy.json')\n",
        "        self.interval = interval\n",
        "        self.total_steps = initial_step\n",
        "        print(f\"Callback iniciado desde paso {self.total_steps}\")\n",
        "\n",
        "    def on_step_end(self, step, logs={}):\n",
        "        self.total_steps += 1\n",
        "        if self.total_steps % self.interval == 0:\n",
        "            base = self.base_path.format(step=self.total_steps)\n",
        "\n",
        "            # Guardar pesos del modelo\n",
        "            weights_path = self.weights_path\n",
        "            weights_path = self.weights_path.format(step=self.total_steps)\n",
        "            self.model.save_weights(weights_path, overwrite=True)\n",
        "            print(f\"\\n[Checkpoint] Pesos guardados en: {weights_path}\")\n",
        "\n",
        "            # Guardar memoria de repetición\n",
        "            memory_path = self.memory_path\n",
        "            temp_path = memory_path + \".tmp\"\n",
        "\n",
        "            with open(temp_path, \"wb\") as f:\n",
        "                pickle.dump(self.model.memory, f)\n",
        "\n",
        "            # Solo si guardar ha ido bien\n",
        "            os.replace(temp_path, memory_path)\n",
        "            print(f\"\\n[Checkpoint] Memoria guardada de forma segura en: {memory_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QEeBZCr1J9xP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN PARA CARGAR EL ÚLTMO CHECKPOINT DETECTADO\n",
        "def load_last_checkpoint(dqn):\n",
        "  # Buscar todos los checkpoints por su archivo .index\n",
        "  pattern = os.path.join(CHECKPOINTS_DIR, f'dqn_{env_name}_weights_*.h5f.index')\n",
        "  checkpoints = glob.glob(pattern)\n",
        "\n",
        "  # Valor por defecto si no se encuentra checkpoint\n",
        "  last_checkpoint_steps = 0\n",
        "\n",
        "  if checkpoints:\n",
        "      # Extraer el número de paso del nombre\n",
        "      def extract_step(filename):\n",
        "          try:\n",
        "              name = os.path.basename(filename)\n",
        "              step_part = name.split('_weights_')[1].replace('.h5f.index', '')\n",
        "              return int(step_part)\n",
        "          except:\n",
        "              return -1\n",
        "\n",
        "      # Seleccionar el checkpoint con mayor número de pasos\n",
        "      latest_index = max(checkpoints, key=extract_step)\n",
        "\n",
        "      # Quitar la extensión .index para obtener el nombre base\n",
        "      latest_checkpoint = latest_index.replace('.index', '')\n",
        "\n",
        "      print(f\"[DQN] Cargando último checkpoint: {latest_checkpoint}\")\n",
        "      dqn.load_weights(latest_checkpoint)\n",
        "\n",
        "      # Aquí extraemos los pasos acumulados\n",
        "      last_checkpoint_steps = extract_step(latest_index)\n",
        "  else:\n",
        "      print(\"[DQN] No se encontró ningún checkpoint, entrenamiento desde cero.\")\n",
        "  return dqn, last_checkpoint_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "o2PywN6Wsef_"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN QUE PERMITE AJUSTAR LOS PARÁMETROS DE LA POLICY PARA QUE EL ENTRENAMIENTO SE REAUNDE POR DONDE TOCA\n",
        "# SI NO POR DEFECTO COMENZARÍA EN EL VALOR MÁXIMO DE EPSILON\n",
        "def adjust_policy_params(scheduler_steps, train_steps, current_step, value_max=1.0, value_min=0.1):\n",
        "    \"\"\"\n",
        "    Ajusta dinámicamente los parámetros de LinearAnnealedPolicy para reanudar la exploración desde el punto correcto.\n",
        "\n",
        "    Args:\n",
        "        scheduler_steps (int): Número de pasos que define la duración del decaimiento de eps.\n",
        "        train_steps (int): Número total de pasos planeados para el entrenamiento.\n",
        "        current_step (int): Paso actual o recuperado del checkpoint.\n",
        "        value_max (float): Valor inicial deseado de eps.\n",
        "        value_min (float): Valor mínimo deseado de eps.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (value_max_ajustado, value_min, scheduler_steps_restantes)\n",
        "    \"\"\"\n",
        "    if current_step >= scheduler_steps:\n",
        "        # Ya se alcanzó el mínimo, mantenerlo constante\n",
        "        return value_min, value_min, 1  # eps se queda fijo\n",
        "\n",
        "    # Calcular epsilon actual desde el punto alcanzado\n",
        "    frac = current_step / scheduler_steps\n",
        "    current_eps = value_max - (value_max - value_min) * frac\n",
        "\n",
        "    # Pasos restantes para completar el scheduler\n",
        "    steps_remaining = scheduler_steps - current_step\n",
        "    return current_eps, value_min, steps_remaining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "88T-Q17-t1cw"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN PARA CARGAR LA MEMORIA Y EL NÚMERO DE PASOS DEL MODELO\n",
        "def get_memory_and_last_steps():\n",
        "  pattern = os.path.join(CHECKPOINTS_DIR, f'dqn_{env_name}_weights_*.h5f.index')\n",
        "  checkpoints = glob.glob(pattern)\n",
        "  last_checkpoint_steps = 0\n",
        "  memory = None\n",
        "\n",
        "  if checkpoints:\n",
        "      def extract_step(filename):\n",
        "          try:\n",
        "              name = os.path.basename(filename)\n",
        "              step_part = name.split('_weights_')[1].replace('.h5f.index', '')\n",
        "              return int(step_part)\n",
        "          except:\n",
        "              return -1\n",
        "\n",
        "      # Encontrar el último checkpoint\n",
        "      latest_index = max(checkpoints, key=extract_step)\n",
        "      latest_checkpoint = latest_index.replace('.index', '')\n",
        "      last_checkpoint_steps = extract_step(latest_index)\n",
        "      print(f\"Último step de checkpoint: {last_checkpoint_steps}\")\n",
        "\n",
        "      # Cargar memoria\n",
        "      memory_path = os.path.join(CHECKPOINTS_DIR, \"memory.pkl\")\n",
        "      if os.path.exists(memory_path):\n",
        "          with open(memory_path, \"rb\") as f:\n",
        "              memory = pickle.load(f)\n",
        "          print(f\"Memoria cargada desde: {memory_path}\")\n",
        "      else:\n",
        "          print(\"No se encontró memoria para este checkpoint.\")\n",
        "\n",
        "  else:\n",
        "      print(\"No se encontró ningún checkpoint, entrenamiento desde cero.\")\n",
        "  return memory, last_checkpoint_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpVqx7bnJLKg",
        "tags": []
      },
      "source": [
        "# MODELO D1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uEMcNQ45ISB"
      },
      "source": [
        "Aquí se presenta una variante del modelo 1 en la que introducimos pequeños cambios en la arquitectura que nos pueden ayudar a conseguir la puntuación mínima, ya que en el modelo 1 nos quedamos muy cerca.\n",
        "\n",
        "Los cambios consisten en un padding 'same' para prservar mejor las características espaciales en bordes, una leakyReLU para evitar neuronas muertas con un factor alpha de una décima (estandar en juegos), una capa extra de 128 filtros que captura patrones más complejos en el movimiento de los marcianitos, un dropout del 20% para reducir el overfitting y una reducción de las capas densas que es más eficiente y más que suficiente para el juego space invaders.\n",
        "\n",
        "Se espera que la combinación de la leakyReLU y el dropout mejoren el flujo de gradientes y la generalización y que la tercera capa convolucional detecte patrones más finos en los disparos y los movimientos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT45Y6y25ISB"
      },
      "source": [
        "## DQN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yitXTADGb2b"
      },
      "source": [
        "1. Implementación de la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBvHXcEe6eF0",
        "outputId": "7e2f800a-aaef-47f5-e758-bab84eac6303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From x:\\08_MIAR\\actividades\\proyecto_practico\\venv_rl\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute (Permute)            (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 21, 21, 32)        8224      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 21, 21, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 21, 21, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        32832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 11, 11, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 15488)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               3965184   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 774       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 4,114,662\n",
            "Trainable params: 4,114,214\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "\n",
        "\n",
        "model_name=\"modelo_D1\"\n",
        "model = Sequential()\n",
        "\n",
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "\n",
        "# Capa de preprocesamiento\n",
        "if K.image_data_format() == 'channels_last':\n",
        "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "else:\n",
        "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
        "\n",
        "model.add(Lambda(lambda x: x / 255.0))\n",
        "\n",
        "# Bloques convolucionales mejorados:\n",
        "model.add(Convolution2D(32, (8, 8), strides=(4, 4), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.1))  # LeakyReLU en lugar de ReLU\n",
        "\n",
        "model.add(Convolution2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Convolution2D(128, (3, 3), strides=(1, 1), padding='same'))  # Nueva capa\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "# Capas densas optimizadas:\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "model.add(Dropout(0.2))  # Regularización\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "# Salida:\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB9-_5HPGb2b"
      },
      "source": [
        "2. Implementación de la solución DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxBLemXQQam3",
        "outputId": "d93f440f-33b4-492c-a056-bd0362ced1f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Hiperparámetros cargados desde ./models\\modelo_D1\\modelo_D1.json\n"
          ]
        }
      ],
      "source": [
        "# GENERACIÓN O CARGA MODELO\n",
        "MODEL_DIR, WEIGHTS_DIR, CHECKPOINTS_DIR = get_dirs(model_name)\n",
        "# save_hyperparams(HYPERPARAMETERS_DIR, filename=model_name+\".json\")\n",
        "load_hyperparams(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foSlxWH1Gb2b",
        "outputId": "79fca2b4-2b67-4bce-f0e9-602bca9f2597",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Último step de checkpoint: 750000\n",
            "Memoria cargada desde: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "Valores de la policy: value_min=0.1, value_max=0.32499999999999996, scheduler_steps=250000\n"
          ]
        }
      ],
      "source": [
        "# DEFINICIÓN DE LA POLICY\n",
        "\n",
        "memory, last_checkpoint_steps=get_memory_and_last_steps()\n",
        "\n",
        "value_max, value_min, new_scheduler_steps = adjust_policy_params(\n",
        "    scheduler_steps=SCHEDULER_STEPS,\n",
        "    train_steps=TRAIN_STEPS,\n",
        "    current_step=last_checkpoint_steps\n",
        ")\n",
        "if not memory:\n",
        "  memory = SequentialMemory(limit=MEMORY_SIZE, window_length=WINDOW_LENGTH)\n",
        "  print(\"Memoria inicializada de 0\")\n",
        "\n",
        "print(f\"Valores de la policy: value_min={value_min}, value_max={value_max}, scheduler_steps={new_scheduler_steps}\")\n",
        "\n",
        "processor = AtariProcessor()\n",
        "\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                              value_max=value_max, value_min=value_min,\n",
        "                              value_test=.05,\n",
        "                              nb_steps=new_scheduler_steps)\n",
        "#policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "#                              value_max=1., value_min=.1,\n",
        "#                              value_test=.05,\n",
        "#                              nb_steps=SCHEDULER_STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "G6mWUNNi5ISE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# CREACIÓN DEL AGENTE DQN\n",
        "dqn = DQNAgent(model=model,\n",
        "               nb_actions=nb_actions,\n",
        "               policy=policy,\n",
        "               memory=memory,\n",
        "               processor=processor,\n",
        "               nb_steps_warmup=WARMUP_STEPS,\n",
        "               gamma=GAMMA,\n",
        "               target_model_update=MODEL_UPDATE,\n",
        "               train_interval=4,\n",
        "               delta_clip=DELTA_CLIP)\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Parche para evitar el error 'get_updates' que ya no existe\n",
        "    def patched_get_updates(self, loss, params):\n",
        "        return []\n",
        "    Adam.get_updates = patched_get_updates\n",
        "\n",
        "dqn.compile(Adam(learning_rate=LEARNING_RATE), metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TxfFDyGzS3_K"
      },
      "outputs": [],
      "source": [
        "# DEFINICIÓN DE LOS NOMBRES DE FICHEROS DE SALIDA\n",
        "weights_filename = os.path.join(WEIGHTS_DIR, 'dqn_{}_weights_{}.h5f'.format(env_name, model_name))\n",
        "checkpoint_weights_filename = os.path.join(CHECKPOINTS_DIR, 'dqn_' + env_name + '_weights_{step}.h5f')\n",
        "log_filename =os.path.join(MODEL_DIR, 'dqn_{}_log_{}.json'.format(env_name, model_name))\n",
        "log_csv_path = os.path.join(MODEL_DIR, f'{model_name}_training_log.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgEZ4q9OB41r",
        "outputId": "3aafb8d1-291a-4430-d5d7-4aa0089759b9",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DQN] Cargando último checkpoint: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_750000.h5f\n",
            "Callback iniciado desde paso 750000\n"
          ]
        }
      ],
      "source": [
        "# CARGAR PESOS DEL ÚLTIMO CHECKPOINT SI EXISTE\n",
        "dqn, last_checkpoint_steps = load_last_checkpoint(dqn)\n",
        "# CREAR CALLBACKS CUSTOMIZADOS QUE GUARDAN BIEN LOS CHECKPOINTS Y LOGS DEL TRAINING\n",
        "checkpoint_callback = AccumulatedCheckpoint(\n",
        "      base_path=CHECKPOINTS_DIR,\n",
        "      env_name=env_name,\n",
        "      interval=MODEL_CHECKPOINT_STEPS,\n",
        "      initial_step=last_checkpoint_steps\n",
        "  )\n",
        "callbacks = [checkpoint_callback]\n",
        "dqn.step = last_checkpoint_steps\n",
        "callbacks += [EpisodeLoggerCSV(log_csv_path)]\n",
        "#callbacks =  [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=MODEL_CHECKPOINT_STEPS)]\n",
        "#callbacks += [FileLogger(log_filename, interval=100)] # Gives some errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BSoqLO65ISF",
        "outputId": "e9df2221-2c03-4c5e-b807-e46f2eff1727",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 1050000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "x:\\08_MIAR\\actividades\\proyecto_practico\\venv_rl\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5000/5000 [==============================] - 31s 5ms/step - reward: 0.0196\n",
            "7 episodes - episode_reward: 13.286 [8.000, 17.000] - ale.lives: 1.965\n",
            "\n",
            "Interval 2 (5000 steps performed)\n",
            "4994/5000 [============================>.] - ETA: 0s - reward: 0.0168\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_760000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 32s 6ms/step - reward: 0.0168\n",
            "5 episodes - episode_reward: 15.400 [7.000, 23.000] - ale.lives: 1.895\n",
            "\n",
            "Interval 3 (10000 steps performed)\n",
            "5000/5000 [==============================] - 23s 5ms/step - reward: 0.0206\n",
            "8 episodes - episode_reward: 12.875 [5.000, 23.000] - ale.lives: 2.076\n",
            "\n",
            "Interval 4 (15000 steps performed)\n",
            "4995/5000 [============================>.] - ETA: 0s - reward: 0.0182\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_770000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 29s 6ms/step - reward: 0.0182\n",
            "7 episodes - episode_reward: 13.000 [8.000, 20.000] - ale.lives: 1.982\n",
            "\n",
            "Interval 5 (20000 steps performed)\n",
            "5000/5000 [==============================] - 22s 4ms/step - reward: 0.0184\n",
            "7 episodes - episode_reward: 13.000 [9.000, 18.000] - ale.lives: 1.975\n",
            "\n",
            "Interval 6 (25000 steps performed)\n",
            "4991/5000 [============================>.] - ETA: 0s - reward: 0.0182\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_780000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 28s 6ms/step - reward: 0.0182\n",
            "7 episodes - episode_reward: 13.714 [8.000, 18.000] - ale.lives: 1.987\n",
            "\n",
            "Interval 7 (30000 steps performed)\n",
            "5000/5000 [==============================] - 144s 29ms/step - reward: 0.0166\n",
            "5 episodes - episode_reward: 17.000 [8.000, 22.000] - loss: 3315.536 - mae: 113797.261 - mean_q: 136164.048 - mean_eps: 0.296 - ale.lives: 2.162\n",
            "\n",
            "Interval 8 (35000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0152\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_790000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0152\n",
            "6 episodes - episode_reward: 13.667 [2.000, 27.000] - loss: 2734.083 - mae: 108612.618 - mean_q: 130099.008 - mean_eps: 0.291 - ale.lives: 2.089\n",
            "\n",
            "Interval 9 (40000 steps performed)\n",
            "5000/5000 [==============================] - 142s 28ms/step - reward: 0.0144\n",
            "6 episodes - episode_reward: 10.333 [7.000, 16.000] - loss: 2273.353 - mae: 99797.706 - mean_q: 119577.827 - mean_eps: 0.287 - ale.lives: 1.978\n",
            "\n",
            "Interval 10 (45000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_800000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0146\n",
            "6 episodes - episode_reward: 13.000 [9.000, 17.000] - loss: 2732.004 - mae: 105718.210 - mean_q: 126652.447 - mean_eps: 0.282 - ale.lives: 2.008\n",
            "\n",
            "Interval 11 (50000 steps performed)\n",
            "5000/5000 [==============================] - 144s 29ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 13.167 [6.000, 21.000] - loss: 2470.246 - mae: 95318.728 - mean_q: 114161.022 - mean_eps: 0.278 - ale.lives: 2.036\n",
            "\n",
            "Interval 12 (55000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0154\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_810000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0154\n",
            "6 episodes - episode_reward: 11.833 [6.000, 27.000] - loss: 2402.406 - mae: 92112.989 - mean_q: 110316.921 - mean_eps: 0.273 - ale.lives: 2.201\n",
            "\n",
            "Interval 13 (60000 steps performed)\n",
            "5000/5000 [==============================] - 142s 28ms/step - reward: 0.0160\n",
            "7 episodes - episode_reward: 12.143 [7.000, 17.000] - loss: 2419.406 - mae: 86929.979 - mean_q: 104069.162 - mean_eps: 0.269 - ale.lives: 2.130\n",
            "\n",
            "Interval 14 (65000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0160\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_820000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0160\n",
            "5 episodes - episode_reward: 15.800 [10.000, 21.000] - loss: 2228.327 - mae: 84807.738 - mean_q: 101572.332 - mean_eps: 0.264 - ale.lives: 2.007\n",
            "\n",
            "Interval 15 (70000 steps performed)\n",
            "5000/5000 [==============================] - 143s 28ms/step - reward: 0.0136\n",
            "6 episodes - episode_reward: 12.667 [3.000, 30.000] - loss: 2340.763 - mae: 86824.446 - mean_q: 103982.057 - mean_eps: 0.260 - ale.lives: 2.040\n",
            "\n",
            "Interval 16 (75000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0190\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_830000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0190\n",
            "4 episodes - episode_reward: 22.000 [9.000, 34.000] - loss: 2268.499 - mae: 75301.893 - mean_q: 90095.949 - mean_eps: 0.255 - ale.lives: 1.934\n",
            "\n",
            "Interval 17 (80000 steps performed)\n",
            "5000/5000 [==============================] - 142s 28ms/step - reward: 0.0178\n",
            "5 episodes - episode_reward: 18.400 [9.000, 29.000] - loss: 2194.758 - mae: 76110.482 - mean_q: 91097.615 - mean_eps: 0.251 - ale.lives: 2.026\n",
            "\n",
            "Interval 18 (85000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0146\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_840000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0146\n",
            "5 episodes - episode_reward: 12.800 [6.000, 19.000] - loss: 2148.278 - mae: 69010.764 - mean_q: 82569.766 - mean_eps: 0.246 - ale.lives: 1.963\n",
            "\n",
            "Interval 19 (90000 steps performed)\n",
            "5000/5000 [==============================] - 143s 29ms/step - reward: 0.0146\n",
            "6 episodes - episode_reward: 11.000 [7.000, 17.000] - loss: 2342.301 - mae: 69805.553 - mean_q: 83497.803 - mean_eps: 0.242 - ale.lives: 2.036\n",
            "\n",
            "Interval 20 (95000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_850000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0144\n",
            "7 episodes - episode_reward: 11.857 [4.000, 22.000] - loss: 3242.941 - mae: 83232.177 - mean_q: 99401.265 - mean_eps: 0.237 - ale.lives: 2.013\n",
            "\n",
            "Interval 21 (100000 steps performed)\n",
            "5000/5000 [==============================] - 142s 28ms/step - reward: 0.0118\n",
            "6 episodes - episode_reward: 11.000 [7.000, 17.000] - loss: 3920.125 - mae: 109572.337 - mean_q: 131004.295 - mean_eps: 0.233 - ale.lives: 2.132\n",
            "\n",
            "Interval 22 (105000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0152\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_860000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0152\n",
            "7 episodes - episode_reward: 11.143 [6.000, 15.000] - loss: 4023.179 - mae: 124647.989 - mean_q: 149100.249 - mean_eps: 0.228 - ale.lives: 1.983\n",
            "\n",
            "Interval 23 (110000 steps performed)\n",
            "5000/5000 [==============================] - 143s 29ms/step - reward: 0.0152\n",
            "6 episodes - episode_reward: 11.000 [4.000, 15.000] - loss: 4430.304 - mae: 136187.194 - mean_q: 162829.543 - mean_eps: 0.224 - ale.lives: 2.067\n",
            "\n",
            "Interval 24 (115000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0156\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_870000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0156\n",
            "5 episodes - episode_reward: 14.600 [8.000, 23.000] - loss: 4100.339 - mae: 132446.500 - mean_q: 158439.124 - mean_eps: 0.219 - ale.lives: 1.793\n",
            "\n",
            "Interval 25 (120000 steps performed)\n",
            "5000/5000 [==============================] - 143s 29ms/step - reward: 0.0166\n",
            "5 episodes - episode_reward: 16.200 [6.000, 32.000] - loss: 4458.746 - mae: 142880.423 - mean_q: 170881.024 - mean_eps: 0.215 - ale.lives: 1.653\n",
            "\n",
            "Interval 26 (125000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_880000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0144\n",
            "7 episodes - episode_reward: 10.714 [4.000, 17.000] - loss: 6677.466 - mae: 196146.438 - mean_q: 234525.703 - mean_eps: 0.210 - ale.lives: 2.052\n",
            "\n",
            "Interval 27 (130000 steps performed)\n",
            "5000/5000 [==============================] - 146s 29ms/step - reward: 0.0196\n",
            "5 episodes - episode_reward: 20.800 [18.000, 25.000] - loss: 6295.751 - mae: 202240.283 - mean_q: 241830.967 - mean_eps: 0.206 - ale.lives: 2.131\n",
            "\n",
            "Interval 28 (135000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0148\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_890000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0148\n",
            "6 episodes - episode_reward: 12.000 [9.000, 17.000] - loss: 7090.864 - mae: 224038.834 - mean_q: 267960.046 - mean_eps: 0.201 - ale.lives: 2.096\n",
            "\n",
            "Interval 29 (140000 steps performed)\n",
            "5000/5000 [==============================] - 147s 29ms/step - reward: 0.0164\n",
            "6 episodes - episode_reward: 14.500 [7.000, 19.000] - loss: 8184.162 - mae: 245233.051 - mean_q: 293195.500 - mean_eps: 0.197 - ale.lives: 1.964\n",
            "\n",
            "Interval 30 (145000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_900000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0144\n",
            "5 episodes - episode_reward: 13.400 [2.000, 20.000] - loss: 8348.675 - mae: 255022.438 - mean_q: 304905.079 - mean_eps: 0.192 - ale.lives: 2.057\n",
            "\n",
            "Interval 31 (150000 steps performed)\n",
            "5000/5000 [==============================] - 147s 29ms/step - reward: 0.0132\n",
            "6 episodes - episode_reward: 11.333 [6.000, 17.000] - loss: 9920.020 - mae: 310933.726 - mean_q: 371860.463 - mean_eps: 0.188 - ale.lives: 1.987\n",
            "\n",
            "Interval 32 (155000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0118\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_910000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0118\n",
            "4 episodes - episode_reward: 14.750 [9.000, 20.000] - loss: 7747.213 - mae: 282585.669 - mean_q: 338230.832 - mean_eps: 0.183 - ale.lives: 1.940\n",
            "\n",
            "Interval 33 (160000 steps performed)\n",
            "5000/5000 [==============================] - 145s 29ms/step - reward: 0.0110\n",
            "5 episodes - episode_reward: 12.600 [8.000, 20.000] - loss: 7259.727 - mae: 274033.021 - mean_q: 328038.180 - mean_eps: 0.179 - ale.lives: 1.833\n",
            "\n",
            "Interval 34 (165000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0140\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_920000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0140\n",
            "6 episodes - episode_reward: 10.667 [6.000, 15.000] - loss: 6366.294 - mae: 248718.344 - mean_q: 297766.061 - mean_eps: 0.174 - ale.lives: 1.949\n",
            "\n",
            "Interval 35 (170000 steps performed)\n",
            "5000/5000 [==============================] - 147s 29ms/step - reward: 0.0142\n",
            "6 episodes - episode_reward: 12.333 [6.000, 19.000] - loss: 5677.602 - mae: 245467.952 - mean_q: 293968.204 - mean_eps: 0.170 - ale.lives: 2.074\n",
            "\n",
            "Interval 36 (175000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0118\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_930000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0118\n",
            "5 episodes - episode_reward: 11.400 [3.000, 16.000] - loss: 5710.197 - mae: 236427.440 - mean_q: 283026.290 - mean_eps: 0.165 - ale.lives: 2.101\n",
            "\n",
            "Interval 37 (180000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0138\n",
            "4 episodes - episode_reward: 16.750 [8.000, 36.000] - loss: 5112.192 - mae: 220569.162 - mean_q: 264147.028 - mean_eps: 0.161 - ale.lives: 1.934\n",
            "\n",
            "Interval 38 (185000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0150\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_940000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0150\n",
            "5 episodes - episode_reward: 13.600 [6.000, 21.000] - loss: 5615.598 - mae: 207032.193 - mean_q: 247711.051 - mean_eps: 0.156 - ale.lives: 2.187\n",
            "\n",
            "Interval 39 (190000 steps performed)\n",
            "5000/5000 [==============================] - 146s 29ms/step - reward: 0.0084\n",
            "5 episodes - episode_reward: 10.800 [5.000, 15.000] - loss: 7634.341 - mae: 261354.086 - mean_q: 312613.986 - mean_eps: 0.152 - ale.lives: 2.000\n",
            "\n",
            "Interval 40 (195000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0134\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_950000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0136\n",
            "7 episodes - episode_reward: 9.571 [2.000, 19.000] - loss: 6218.001 - mae: 219048.928 - mean_q: 262082.841 - mean_eps: 0.147 - ale.lives: 1.942\n",
            "\n",
            "Interval 41 (200000 steps performed)\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0142\n",
            "5 episodes - episode_reward: 12.000 [6.000, 23.000] - loss: 7600.664 - mae: 252045.860 - mean_q: 301419.725 - mean_eps: 0.143 - ale.lives: 1.822\n",
            "\n",
            "Interval 42 (205000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0158\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_960000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0158\n",
            "7 episodes - episode_reward: 12.857 [7.000, 17.000] - loss: 7895.386 - mae: 271264.914 - mean_q: 324565.446 - mean_eps: 0.138 - ale.lives: 2.125\n",
            "\n",
            "Interval 43 (210000 steps performed)\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0132\n",
            "6 episodes - episode_reward: 10.500 [5.000, 16.000] - loss: 11829.643 - mae: 364134.724 - mean_q: 435364.992 - mean_eps: 0.134 - ale.lives: 2.021\n",
            "\n",
            "Interval 44 (215000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0130\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_970000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0130\n",
            "6 episodes - episode_reward: 11.000 [6.000, 15.000] - loss: 9878.032 - mae: 308078.188 - mean_q: 368247.256 - mean_eps: 0.129 - ale.lives: 1.989\n",
            "\n",
            "Interval 45 (220000 steps performed)\n",
            "5000/5000 [==============================] - 145s 29ms/step - reward: 0.0108\n",
            "4 episodes - episode_reward: 14.250 [9.000, 25.000] - loss: 12841.825 - mae: 394876.516 - mean_q: 472215.164 - mean_eps: 0.125 - ale.lives: 1.940\n",
            "\n",
            "Interval 46 (225000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0134\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_980000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0134\n",
            "7 episodes - episode_reward: 9.000 [4.000, 13.000] - loss: 10234.742 - mae: 324648.407 - mean_q: 388122.458 - mean_eps: 0.120 - ale.lives: 2.027\n",
            "\n",
            "Interval 47 (230000 steps performed)\n",
            "5000/5000 [==============================] - 147s 29ms/step - reward: 0.0106\n",
            "5 episodes - episode_reward: 11.800 [9.000, 16.000] - loss: 9257.570 - mae: 348504.778 - mean_q: 417033.993 - mean_eps: 0.116 - ale.lives: 1.841\n",
            "\n",
            "Interval 48 (235000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0142\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_990000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0142\n",
            "4 episodes - episode_reward: 16.750 [15.000, 21.000] - loss: 10215.714 - mae: 399123.381 - mean_q: 477634.781 - mean_eps: 0.111 - ale.lives: 1.947\n",
            "\n",
            "Interval 49 (240000 steps performed)\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0104\n",
            "7 episodes - episode_reward: 7.143 [5.000, 14.000] - loss: 7816.901 - mae: 312621.321 - mean_q: 374226.955 - mean_eps: 0.107 - ale.lives: 1.968\n",
            "\n",
            "Interval 50 (245000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0122\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1000000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0122\n",
            "8 episodes - episode_reward: 8.375 [1.000, 12.000] - loss: 7344.267 - mae: 336626.298 - mean_q: 403149.514 - mean_eps: 0.102 - ale.lives: 1.868\n",
            "\n",
            "Interval 51 (250000 steps performed)\n",
            "5000/5000 [==============================] - 148s 30ms/step - reward: 0.0148\n",
            "5 episodes - episode_reward: 14.600 [8.000, 23.000] - loss: 8651.263 - mae: 373405.298 - mean_q: 447186.215 - mean_eps: 0.100 - ale.lives: 1.887\n",
            "\n",
            "Interval 52 (255000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0148\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1010000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0148\n",
            "6 episodes - episode_reward: 12.000 [3.000, 26.000] - loss: 8421.437 - mae: 381392.043 - mean_q: 456736.858 - mean_eps: 0.100 - ale.lives: 1.948\n",
            "\n",
            "Interval 53 (260000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0134\n",
            "4 episodes - episode_reward: 16.000 [6.000, 23.000] - loss: 8879.367 - mae: 376968.449 - mean_q: 451314.161 - mean_eps: 0.100 - ale.lives: 1.692\n",
            "\n",
            "Interval 54 (265000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0146\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1020000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0146\n",
            "4 episodes - episode_reward: 17.750 [12.000, 24.000] - loss: 8611.264 - mae: 367183.530 - mean_q: 439603.339 - mean_eps: 0.100 - ale.lives: 2.071\n",
            "\n",
            "Interval 55 (270000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0138\n",
            "6 episodes - episode_reward: 11.167 [7.000, 20.000] - loss: 9394.423 - mae: 375903.785 - mean_q: 449954.140 - mean_eps: 0.100 - ale.lives: 1.980\n",
            "\n",
            "Interval 56 (275000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0104\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1030000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 155s 31ms/step - reward: 0.0104\n",
            "7 episodes - episode_reward: 8.857 [5.000, 15.000] - loss: 14518.837 - mae: 467639.957 - mean_q: 558988.109 - mean_eps: 0.100 - ale.lives: 2.031\n",
            "\n",
            "Interval 57 (280000 steps performed)\n",
            "5000/5000 [==============================] - 148s 30ms/step - reward: 0.0150\n",
            "6 episodes - episode_reward: 12.167 [6.000, 23.000] - loss: 14764.960 - mae: 515472.866 - mean_q: 616491.183 - mean_eps: 0.100 - ale.lives: 2.204\n",
            "\n",
            "Interval 58 (285000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0132\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1040000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0132\n",
            "7 episodes - episode_reward: 9.714 [7.000, 13.000] - loss: 15375.877 - mae: 544867.806 - mean_q: 651701.210 - mean_eps: 0.100 - ale.lives: 1.881\n",
            "\n",
            "Interval 59 (290000 steps performed)\n",
            "5000/5000 [==============================] - 147s 29ms/step - reward: 0.0122\n",
            "5 episodes - episode_reward: 10.600 [6.000, 16.000] - loss: 25167.634 - mae: 815952.184 - mean_q: 975635.927 - mean_eps: 0.100 - ale.lives: 1.908\n",
            "\n",
            "Interval 60 (295000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0112\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1050000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0112\n",
            "6 episodes - episode_reward: 10.333 [5.000, 16.000] - loss: 32248.885 - mae: 1077020.348 - mean_q: 1287750.440 - mean_eps: 0.100 - ale.lives: 1.959\n",
            "\n",
            "Interval 61 (300000 steps performed)\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0128\n",
            "7 episodes - episode_reward: 7.714 [3.000, 15.000] - loss: 55061.557 - mae: 1709231.139 - mean_q: 2042631.181 - mean_eps: 0.100 - ale.lives: 2.033\n",
            "\n",
            "Interval 62 (305000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0126\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1060000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0126\n",
            "5 episodes - episode_reward: 14.600 [10.000, 21.000] - loss: 43827.929 - mae: 1627838.206 - mean_q: 1947771.222 - mean_eps: 0.100 - ale.lives: 2.131\n",
            "\n",
            "Interval 63 (310000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0150\n",
            "5 episodes - episode_reward: 15.400 [9.000, 24.000] - loss: 52593.740 - mae: 1878228.738 - mean_q: 2246301.322 - mean_eps: 0.100 - ale.lives: 2.019\n",
            "\n",
            "Interval 64 (315000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0148\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1070000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0148\n",
            "5 episodes - episode_reward: 11.200 [4.000, 22.000] - loss: 60450.005 - mae: 2176001.055 - mean_q: 2602313.399 - mean_eps: 0.100 - ale.lives: 2.070\n",
            "\n",
            "Interval 65 (320000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0144\n",
            "6 episodes - episode_reward: 12.833 [4.000, 32.000] - loss: 71116.945 - mae: 2582652.156 - mean_q: 3088596.688 - mean_eps: 0.100 - ale.lives: 1.908\n",
            "\n",
            "Interval 66 (325000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0106\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1080000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 156s 31ms/step - reward: 0.0106\n",
            "5 episodes - episode_reward: 11.800 [5.000, 21.000] - loss: 71259.104 - mae: 2733099.624 - mean_q: 3270086.473 - mean_eps: 0.100 - ale.lives: 1.653\n",
            "\n",
            "Interval 67 (330000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0120\n",
            "4 episodes - episode_reward: 12.750 [8.000, 19.000] - loss: 65802.847 - mae: 2498598.723 - mean_q: 2990207.869 - mean_eps: 0.100 - ale.lives: 2.040\n",
            "\n",
            "Interval 68 (335000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0128\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1090000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0128\n",
            "5 episodes - episode_reward: 13.600 [3.000, 28.000] - loss: 57401.835 - mae: 2353858.576 - mean_q: 2817227.867 - mean_eps: 0.100 - ale.lives: 1.818\n",
            "\n",
            "Interval 69 (340000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0144\n",
            "8 episodes - episode_reward: 10.500 [4.000, 16.000] - loss: 56320.518 - mae: 2327616.049 - mean_q: 2785936.189 - mean_eps: 0.100 - ale.lives: 1.996\n",
            "\n",
            "Interval 70 (345000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0118\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1100000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0118\n",
            "3 episodes - episode_reward: 15.667 [15.000, 16.000] - loss: 72104.663 - mae: 2791435.887 - mean_q: 3339060.999 - mean_eps: 0.100 - ale.lives: 2.080\n",
            "\n",
            "Interval 71 (350000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0150\n",
            "6 episodes - episode_reward: 11.833 [7.000, 21.000] - loss: 70114.668 - mae: 2881674.516 - mean_q: 3447524.063 - mean_eps: 0.100 - ale.lives: 1.947\n",
            "\n",
            "Interval 72 (355000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0126\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1110000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0126\n",
            "7 episodes - episode_reward: 10.286 [4.000, 17.000] - loss: 69689.613 - mae: 3039718.269 - mean_q: 3638087.692 - mean_eps: 0.100 - ale.lives: 2.112\n",
            "\n",
            "Interval 73 (360000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0148\n",
            "4 episodes - episode_reward: 16.250 [14.000, 18.000] - loss: 80427.801 - mae: 3334630.181 - mean_q: 3990140.180 - mean_eps: 0.100 - ale.lives: 2.055\n",
            "\n",
            "Interval 74 (365000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0128\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1120000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0128\n",
            "6 episodes - episode_reward: 12.167 [4.000, 23.000] - loss: 120894.989 - mae: 4299937.209 - mean_q: 5140785.085 - mean_eps: 0.100 - ale.lives: 2.077\n",
            "\n",
            "Interval 75 (370000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0112\n",
            "5 episodes - episode_reward: 11.400 [6.000, 23.000] - loss: 107122.127 - mae: 4345392.863 - mean_q: 5201692.580 - mean_eps: 0.100 - ale.lives: 1.922\n",
            "\n",
            "Interval 76 (375000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0140\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1130000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0140\n",
            "4 episodes - episode_reward: 18.000 [10.000, 26.000] - loss: 97210.732 - mae: 3959374.977 - mean_q: 4735580.601 - mean_eps: 0.100 - ale.lives: 2.114\n",
            "\n",
            "Interval 77 (380000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0162\n",
            "4 episodes - episode_reward: 15.500 [9.000, 22.000] - loss: 101099.668 - mae: 4056095.313 - mean_q: 4853536.996 - mean_eps: 0.100 - ale.lives: 1.797\n",
            "\n",
            "Interval 78 (385000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0120\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1140000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0120\n",
            "6 episodes - episode_reward: 13.000 [8.000, 23.000] - loss: 94293.533 - mae: 3958224.053 - mean_q: 4735416.634 - mean_eps: 0.100 - ale.lives: 1.952\n",
            "\n",
            "Interval 79 (390000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0152\n",
            "5 episodes - episode_reward: 14.800 [10.000, 22.000] - loss: 111387.234 - mae: 4464339.301 - mean_q: 5342011.866 - mean_eps: 0.100 - ale.lives: 1.848\n",
            "\n",
            "Interval 80 (395000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0162\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1150000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0162\n",
            "5 episodes - episode_reward: 14.800 [9.000, 20.000] - loss: 148162.681 - mae: 5483004.743 - mean_q: 6560518.763 - mean_eps: 0.100 - ale.lives: 1.887\n",
            "\n",
            "Interval 81 (400000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0128\n",
            "4 episodes - episode_reward: 17.000 [7.000, 24.000] - loss: 309008.815 - mae: 8791189.905 - mean_q: 10493625.796 - mean_eps: 0.100 - ale.lives: 1.900\n",
            "\n",
            "Interval 82 (405000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0128\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1160000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0128\n",
            "5 episodes - episode_reward: 12.000 [9.000, 13.000] - loss: 228108.149 - mae: 8241309.866 - mean_q: 9856925.253 - mean_eps: 0.100 - ale.lives: 1.828\n",
            "\n",
            "Interval 83 (410000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0146\n",
            "6 episodes - episode_reward: 12.167 [6.000, 16.000] - loss: 243261.660 - mae: 8902485.976 - mean_q: 10644088.934 - mean_eps: 0.100 - ale.lives: 2.038\n",
            "\n",
            "Interval 84 (415000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0124\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1170000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0124\n",
            "6 episodes - episode_reward: 12.000 [9.000, 17.000] - loss: 253525.152 - mae: 9420549.377 - mean_q: 11263415.438 - mean_eps: 0.100 - ale.lives: 1.766\n",
            "\n",
            "Interval 85 (420000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0118\n",
            "5 episodes - episode_reward: 10.400 [7.000, 13.000] - loss: 306318.607 - mae: 10383403.138 - mean_q: 12414814.752 - mean_eps: 0.100 - ale.lives: 1.963\n",
            "\n",
            "Interval 86 (425000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0120\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1180000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0120\n",
            "6 episodes - episode_reward: 10.833 [6.000, 19.000] - loss: 280782.256 - mae: 9904660.704 - mean_q: 11842145.346 - mean_eps: 0.100 - ale.lives: 1.903\n",
            "\n",
            "Interval 87 (430000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0174\n",
            "6 episodes - episode_reward: 13.167 [8.000, 20.000] - loss: 337934.903 - mae: 11856654.195 - mean_q: 14173099.463 - mean_eps: 0.100 - ale.lives: 1.963\n",
            "\n",
            "Interval 88 (435000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0142\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1190000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 31ms/step - reward: 0.0142\n",
            "5 episodes - episode_reward: 15.800 [7.000, 23.000] - loss: 287699.793 - mae: 10316688.744 - mean_q: 12331091.143 - mean_eps: 0.100 - ale.lives: 1.907\n",
            "\n",
            "Interval 89 (440000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0108\n",
            "6 episodes - episode_reward: 10.000 [5.000, 15.000] - loss: 285233.339 - mae: 11261203.818 - mean_q: 13469144.619 - mean_eps: 0.100 - ale.lives: 1.799\n",
            "\n",
            "Interval 90 (445000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0176\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1200000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0176\n",
            "6 episodes - episode_reward: 13.500 [8.000, 20.000] - loss: 263578.197 - mae: 10383527.637 - mean_q: 12414576.167 - mean_eps: 0.100 - ale.lives: 2.095\n",
            "\n",
            "Interval 91 (450000 steps performed)\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0126\n",
            "5 episodes - episode_reward: 11.800 [6.000, 18.000] - loss: 280912.272 - mae: 10536609.494 - mean_q: 12597707.921 - mean_eps: 0.100 - ale.lives: 1.901\n",
            "\n",
            "Interval 92 (455000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0116\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1210000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0116\n",
            "5 episodes - episode_reward: 13.800 [11.000, 17.000] - loss: 228409.220 - mae: 8182720.119 - mean_q: 9776821.522 - mean_eps: 0.100 - ale.lives: 1.669\n",
            "\n",
            "Interval 93 (460000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0148\n",
            "7 episodes - episode_reward: 10.143 [5.000, 19.000] - loss: 233901.662 - mae: 9214066.723 - mean_q: 11020821.186 - mean_eps: 0.100 - ale.lives: 2.039\n",
            "\n",
            "Interval 94 (465000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0108\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1220000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 31ms/step - reward: 0.0108\n",
            "4 episodes - episode_reward: 13.750 [7.000, 19.000] - loss: 191467.339 - mae: 7538473.304 - mean_q: 9014030.668 - mean_eps: 0.100 - ale.lives: 1.978\n",
            "\n",
            "Interval 95 (470000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0094\n",
            "5 episodes - episode_reward: 8.600 [5.000, 15.000] - loss: 187375.672 - mae: 7673924.929 - mean_q: 9178724.216 - mean_eps: 0.100 - ale.lives: 1.916\n",
            "\n",
            "Interval 96 (475000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0140\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1230000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0140\n",
            "5 episodes - episode_reward: 14.000 [10.000, 20.000] - loss: 164159.996 - mae: 6541770.704 - mean_q: 7823328.948 - mean_eps: 0.100 - ale.lives: 1.792\n",
            "\n",
            "Interval 97 (480000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0138\n",
            "6 episodes - episode_reward: 10.500 [7.000, 14.000] - loss: 148592.641 - mae: 6273171.237 - mean_q: 7505902.622 - mean_eps: 0.100 - ale.lives: 1.929\n",
            "\n",
            "Interval 98 (485000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0124\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1240000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 31ms/step - reward: 0.0124\n",
            "5 episodes - episode_reward: 14.600 [5.000, 22.000] - loss: 149412.547 - mae: 6459215.025 - mean_q: 7728011.753 - mean_eps: 0.100 - ale.lives: 1.838\n",
            "\n",
            "Interval 99 (490000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0144\n",
            "5 episodes - episode_reward: 13.200 [2.000, 23.000] - loss: 167356.415 - mae: 6950537.972 - mean_q: 8316898.547 - mean_eps: 0.100 - ale.lives: 1.989\n",
            "\n",
            "Interval 100 (495000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0146\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1250000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0146\n",
            "6 episodes - episode_reward: 11.667 [9.000, 16.000] - loss: 201121.679 - mae: 7977744.104 - mean_q: 9538878.448 - mean_eps: 0.100 - ale.lives: 1.887\n",
            "\n",
            "Interval 101 (500000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0130\n",
            "7 episodes - episode_reward: 10.714 [5.000, 19.000] - loss: 214260.790 - mae: 8389358.262 - mean_q: 10034031.515 - mean_eps: 0.100 - ale.lives: 1.880\n",
            "\n",
            "Interval 102 (505000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0166\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1260000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0166\n",
            "5 episodes - episode_reward: 16.000 [10.000, 23.000] - loss: 193831.476 - mae: 8341917.032 - mean_q: 9981616.190 - mean_eps: 0.100 - ale.lives: 2.165\n",
            "\n",
            "Interval 103 (510000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0158\n",
            "5 episodes - episode_reward: 16.000 [7.000, 21.000] - loss: 172413.860 - mae: 7100102.955 - mean_q: 8489202.168 - mean_eps: 0.100 - ale.lives: 1.795\n",
            "\n",
            "Interval 104 (515000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0154\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1270000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0154\n",
            "6 episodes - episode_reward: 13.000 [6.000, 20.000] - loss: 174091.441 - mae: 7401293.271 - mean_q: 8854199.904 - mean_eps: 0.100 - ale.lives: 2.100\n",
            "\n",
            "Interval 105 (520000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0120\n",
            "5 episodes - episode_reward: 11.000 [5.000, 17.000] - loss: 167938.375 - mae: 6957912.760 - mean_q: 8323048.454 - mean_eps: 0.100 - ale.lives: 2.291\n",
            "\n",
            "Interval 106 (525000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0114\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1280000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0114\n",
            "6 episodes - episode_reward: 7.833 [2.000, 14.000] - loss: 173163.440 - mae: 7281080.342 - mean_q: 8710436.819 - mean_eps: 0.100 - ale.lives: 2.170\n",
            "\n",
            "Interval 107 (530000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0148\n",
            "5 episodes - episode_reward: 17.000 [9.000, 32.000] - loss: 189664.083 - mae: 7594177.630 - mean_q: 9082432.563 - mean_eps: 0.100 - ale.lives: 1.777\n",
            "\n",
            "Interval 108 (535000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0136\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1290000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 31ms/step - reward: 0.0136\n",
            "5 episodes - episode_reward: 13.600 [7.000, 18.000] - loss: 249345.438 - mae: 8943595.309 - mean_q: 10689877.042 - mean_eps: 0.100 - ale.lives: 1.904\n",
            "\n",
            "Interval 109 (540000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0110\n",
            "5 episodes - episode_reward: 12.000 [6.000, 23.000] - loss: 206628.976 - mae: 7581062.712 - mean_q: 9064434.391 - mean_eps: 0.100 - ale.lives: 2.043\n",
            "\n",
            "Interval 110 (545000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0140\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1300000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0140\n",
            "6 episodes - episode_reward: 11.667 [6.000, 16.000] - loss: 278405.663 - mae: 9540197.196 - mean_q: 11397691.850 - mean_eps: 0.100 - ale.lives: 2.160\n",
            "\n",
            "Interval 111 (550000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0140\n",
            "4 episodes - episode_reward: 11.500 [7.000, 16.000] - loss: 351505.243 - mae: 11655672.369 - mean_q: 13924075.658 - mean_eps: 0.100 - ale.lives: 2.144\n",
            "\n",
            "Interval 112 (555000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0132\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1310000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0132\n",
            "6 episodes - episode_reward: 14.833 [3.000, 28.000] - loss: 308945.877 - mae: 11440861.939 - mean_q: 13673848.182 - mean_eps: 0.100 - ale.lives: 2.063\n",
            "\n",
            "Interval 113 (560000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0138\n",
            "5 episodes - episode_reward: 12.800 [9.000, 19.000] - loss: 317214.210 - mae: 11780289.480 - mean_q: 14083701.410 - mean_eps: 0.100 - ale.lives: 1.856\n",
            "\n",
            "Interval 114 (565000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0110\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1320000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0110\n",
            "5 episodes - episode_reward: 10.600 [5.000, 21.000] - loss: 271641.034 - mae: 9850286.352 - mean_q: 11776746.048 - mean_eps: 0.100 - ale.lives: 1.963\n",
            "\n",
            "Interval 115 (570000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0116\n",
            "5 episodes - episode_reward: 11.800 [7.000, 19.000] - loss: 236393.953 - mae: 8783468.384 - mean_q: 10496109.018 - mean_eps: 0.100 - ale.lives: 1.970\n",
            "\n",
            "Interval 116 (575000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0110\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1330000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0110\n",
            "5 episodes - episode_reward: 10.400 [3.000, 14.000] - loss: 215195.555 - mae: 8186680.764 - mean_q: 9792239.325 - mean_eps: 0.100 - ale.lives: 2.034\n",
            "\n",
            "Interval 117 (580000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0100\n",
            "4 episodes - episode_reward: 12.750 [7.000, 17.000] - loss: 217744.896 - mae: 7783622.249 - mean_q: 9304783.998 - mean_eps: 0.100 - ale.lives: 1.862\n",
            "\n",
            "Interval 118 (585000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0128\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1340000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 157s 31ms/step - reward: 0.0128\n",
            "5 episodes - episode_reward: 14.600 [4.000, 31.000] - loss: 294319.726 - mae: 9395142.862 - mean_q: 11224967.617 - mean_eps: 0.100 - ale.lives: 1.695\n",
            "\n",
            "Interval 119 (590000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0140\n",
            "4 episodes - episode_reward: 13.500 [5.000, 21.000] - loss: 328781.438 - mae: 11130172.560 - mean_q: 13302816.445 - mean_eps: 0.100 - ale.lives: 2.010\n",
            "\n",
            "Interval 120 (595000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1350000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0144\n",
            "6 episodes - episode_reward: 14.667 [11.000, 20.000] - loss: 315219.458 - mae: 11106431.661 - mean_q: 13281462.620 - mean_eps: 0.100 - ale.lives: 1.760\n",
            "\n",
            "Interval 121 (600000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0102\n",
            "5 episodes - episode_reward: 8.200 [3.000, 16.000] - loss: 337813.827 - mae: 11806548.429 - mean_q: 14110231.254 - mean_eps: 0.100 - ale.lives: 1.950\n",
            "\n",
            "Interval 122 (605000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1360000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0144\n",
            "6 episodes - episode_reward: 13.500 [4.000, 21.000] - loss: 674636.198 - mae: 17928038.868 - mean_q: 21388370.257 - mean_eps: 0.100 - ale.lives: 1.983\n",
            "\n",
            "Interval 123 (610000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0170\n",
            "5 episodes - episode_reward: 15.600 [9.000, 24.000] - loss: 1037222.477 - mae: 26987201.243 - mean_q: 32203842.443 - mean_eps: 0.100 - ale.lives: 1.964\n",
            "\n",
            "Interval 124 (615000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0142\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1370000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0142\n",
            "6 episodes - episode_reward: 13.167 [8.000, 29.000] - loss: 1020733.351 - mae: 29670532.504 - mean_q: 35407530.448 - mean_eps: 0.100 - ale.lives: 1.851\n",
            "\n",
            "Interval 125 (620000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0106\n",
            "6 episodes - episode_reward: 6.667 [0.000, 11.000] - loss: 1058392.433 - mae: 32528260.621 - mean_q: 38849383.835 - mean_eps: 0.100 - ale.lives: 2.021\n",
            "\n",
            "Interval 126 (625000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0130\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1380000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0130\n",
            "7 episodes - episode_reward: 9.857 [5.000, 18.000] - loss: 1517751.344 - mae: 42428538.134 - mean_q: 50648986.938 - mean_eps: 0.100 - ale.lives: 2.154\n",
            "\n",
            "Interval 127 (630000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0138\n",
            "6 episodes - episode_reward: 11.000 [6.000, 17.000] - loss: 2479316.139 - mae: 65418278.125 - mean_q: 77988122.582 - mean_eps: 0.100 - ale.lives: 1.957\n",
            "\n",
            "Interval 128 (635000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0114\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1390000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0114\n",
            "7 episodes - episode_reward: 9.286 [4.000, 16.000] - loss: 2908248.844 - mae: 84073107.315 - mean_q: 100284944.006 - mean_eps: 0.100 - ale.lives: 2.013\n",
            "\n",
            "Interval 129 (640000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0140\n",
            "6 episodes - episode_reward: 12.333 [9.000, 16.000] - loss: 4432359.543 - mae: 119200381.389 - mean_q: 142121150.042 - mean_eps: 0.100 - ale.lives: 1.991\n",
            "\n",
            "Interval 130 (645000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0150\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1400000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0150\n",
            "7 episodes - episode_reward: 10.429 [6.000, 16.000] - loss: 3462841.181 - mae: 111511493.920 - mean_q: 133125067.885 - mean_eps: 0.100 - ale.lives: 2.024\n",
            "\n",
            "Interval 131 (650000 steps performed)\n",
            "5000/5000 [==============================] - 153s 30ms/step - reward: 0.0168\n",
            "6 episodes - episode_reward: 12.000 [4.000, 19.000] - loss: 2860598.719 - mae: 73257947.210 - mean_q: 87309157.760 - mean_eps: 0.100 - ale.lives: 2.019\n",
            "\n",
            "Interval 132 (655000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0136\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1410000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0136\n",
            "6 episodes - episode_reward: 12.667 [9.000, 19.000] - loss: 1804964.471 - mae: 60460587.446 - mean_q: 72167531.171 - mean_eps: 0.100 - ale.lives: 2.061\n",
            "\n",
            "Interval 133 (660000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0146\n",
            "6 episodes - episode_reward: 12.833 [10.000, 15.000] - loss: 1857510.170 - mae: 60393886.115 - mean_q: 72133360.864 - mean_eps: 0.100 - ale.lives: 2.036\n",
            "\n",
            "Interval 134 (665000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0152\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1420000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 158s 32ms/step - reward: 0.0152\n",
            "7 episodes - episode_reward: 9.857 [4.000, 14.000] - loss: 1439391.084 - mae: 56046526.157 - mean_q: 67005677.846 - mean_eps: 0.100 - ale.lives: 2.015\n",
            "\n",
            "Interval 135 (670000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0170\n",
            "5 episodes - episode_reward: 17.200 [10.000, 24.000] - loss: 1547888.736 - mae: 58950327.062 - mean_q: 70431950.787 - mean_eps: 0.100 - ale.lives: 1.903\n",
            "\n",
            "Interval 136 (675000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0126\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1430000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0126\n",
            "6 episodes - episode_reward: 11.333 [2.000, 20.000] - loss: 1320400.395 - mae: 49775379.488 - mean_q: 59462953.536 - mean_eps: 0.100 - ale.lives: 1.853\n",
            "\n",
            "Interval 137 (680000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0118\n",
            "5 episodes - episode_reward: 10.600 [3.000, 17.000] - loss: 1959322.693 - mae: 66801190.384 - mean_q: 79785065.978 - mean_eps: 0.100 - ale.lives: 1.886\n",
            "\n",
            "Interval 138 (685000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0114\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1440000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0114\n",
            "6 episodes - episode_reward: 10.500 [2.000, 17.000] - loss: 1984075.944 - mae: 72083423.568 - mean_q: 86124308.877 - mean_eps: 0.100 - ale.lives: 1.799\n",
            "\n",
            "Interval 139 (690000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0156\n",
            "4 episodes - episode_reward: 15.750 [10.000, 30.000] - loss: 2951016.638 - mae: 94633330.726 - mean_q: 113009178.694 - mean_eps: 0.100 - ale.lives: 2.015\n",
            "\n",
            "Interval 140 (695000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0136\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1450000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0136\n",
            "6 episodes - episode_reward: 13.500 [4.000, 21.000] - loss: 4586993.277 - mae: 124405766.816 - mean_q: 148526567.974 - mean_eps: 0.100 - ale.lives: 2.025\n",
            "\n",
            "Interval 141 (700000 steps performed)\n",
            "5000/5000 [==============================] - 149s 30ms/step - reward: 0.0146\n",
            "7 episodes - episode_reward: 11.143 [2.000, 15.000] - loss: 3659954.627 - mae: 123448451.840 - mean_q: 147493527.283 - mean_eps: 0.100 - ale.lives: 1.952\n",
            "\n",
            "Interval 142 (705000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0144\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1460000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0144\n",
            "5 episodes - episode_reward: 13.200 [11.000, 17.000] - loss: 8202565.027 - mae: 193884404.294 - mean_q: 231270901.485 - mean_eps: 0.100 - ale.lives: 2.063\n",
            "\n",
            "Interval 143 (710000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0168\n",
            "6 episodes - episode_reward: 13.500 [7.000, 17.000] - loss: 14109650.687 - mae: 287196054.502 - mean_q: 342425248.243 - mean_eps: 0.100 - ale.lives: 2.135\n",
            "\n",
            "Interval 144 (715000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0140\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1470000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0140\n",
            "6 episodes - episode_reward: 13.167 [7.000, 17.000] - loss: 15332785.122 - mae: 361572334.797 - mean_q: 430759643.520 - mean_eps: 0.100 - ale.lives: 2.013\n",
            "\n",
            "Interval 145 (720000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0154\n",
            "5 episodes - episode_reward: 13.400 [11.000, 17.000] - loss: 77854511.268 - mae: 896032956.621 - mean_q: 1062417603.558 - mean_eps: 0.100 - ale.lives: 1.977\n",
            "\n",
            "Interval 146 (725000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0148\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1480000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0148\n",
            "6 episodes - episode_reward: 13.000 [11.000, 17.000] - loss: 94685494.861 - mae: 1484055741.542 - mean_q: 1767152424.141 - mean_eps: 0.100 - ale.lives: 2.129\n",
            "\n",
            "Interval 147 (730000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0144\n",
            "5 episodes - episode_reward: 14.400 [11.000, 17.000] - loss: 653612659.450 - mae: 3524816228.966 - mean_q: 4087680102.400 - mean_eps: 0.100 - ale.lives: 2.082\n",
            "\n",
            "Interval 148 (735000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0154\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1490000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0156\n",
            "6 episodes - episode_reward: 13.000 [11.000, 17.000] - loss: 1096294577.094 - mae: 6893870274.355 - mean_q: 8060157652.582 - mean_eps: 0.100 - ale.lives: 2.132\n",
            "\n",
            "Interval 149 (740000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0128\n",
            "5 episodes - episode_reward: 12.000 [11.000, 16.000] - loss: 1992098079.821 - mae: 12136601740.902 - mean_q: 14195755028.480 - mean_eps: 0.100 - ale.lives: 2.122\n",
            "\n",
            "Interval 150 (745000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0158\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1500000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0158\n",
            "6 episodes - episode_reward: 14.000 [11.000, 17.000] - loss: 2156231522.637 - mae: 19206471565.312 - mean_q: 22764967401.062 - mean_eps: 0.100 - ale.lives: 2.109\n",
            "\n",
            "Interval 151 (750000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0134\n",
            "5 episodes - episode_reward: 12.000 [11.000, 15.000] - loss: 1245601733.120 - mae: 24787120827.597 - mean_q: 29657881732.710 - mean_eps: 0.100 - ale.lives: 2.121\n",
            "\n",
            "Interval 152 (755000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0142\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1510000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0142\n",
            "6 episodes - episode_reward: 12.833 [11.000, 16.000] - loss: 937724767.667 - mae: 27860420866.867 - mean_q: 33296374145.024 - mean_eps: 0.100 - ale.lives: 2.152\n",
            "\n",
            "Interval 153 (760000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0160\n",
            "5 episodes - episode_reward: 14.400 [2.000, 22.000] - loss: 628951257.805 - mae: 19526419188.941 - mean_q: 23287999997.542 - mean_eps: 0.100 - ale.lives: 2.052\n",
            "\n",
            "Interval 154 (765000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0208\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1520000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0208\n",
            "7 episodes - episode_reward: 15.429 [9.000, 22.000] - loss: 471849301.517 - mae: 18150415503.360 - mean_q: 21690105413.632 - mean_eps: 0.100 - ale.lives: 2.008\n",
            "\n",
            "Interval 155 (770000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0110\n",
            "7 episodes - episode_reward: 8.714 [4.000, 12.000] - loss: 397511365.146 - mae: 14520421397.299 - mean_q: 17331948722.586 - mean_eps: 0.100 - ale.lives: 1.954\n",
            "\n",
            "Interval 156 (775000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0126\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1530000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 162s 32ms/step - reward: 0.0126\n",
            "5 episodes - episode_reward: 12.000 [2.000, 16.000] - loss: 547276068.941 - mae: 19145978269.696 - mean_q: 22842078179.328 - mean_eps: 0.100 - ale.lives: 2.038\n",
            "\n",
            "Interval 157 (780000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0122\n",
            "4 episodes - episode_reward: 14.500 [11.000, 16.000] - loss: 475234004.019 - mae: 18059822553.498 - mean_q: 21564053908.685 - mean_eps: 0.100 - ale.lives: 1.926\n",
            "\n",
            "Interval 158 (785000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0156\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1540000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0156\n",
            "7 episodes - episode_reward: 12.143 [5.000, 22.000] - loss: 695463176.678 - mae: 23291723885.773 - mean_q: 27804619289.395 - mean_eps: 0.100 - ale.lives: 2.035\n",
            "\n",
            "Interval 159 (790000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 13.833 [11.000, 19.000] - loss: 1016985861.709 - mae: 29519080697.037 - mean_q: 35247989722.317 - mean_eps: 0.100 - ale.lives: 2.189\n",
            "\n",
            "Interval 160 (795000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0086\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1550000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0086\n",
            "5 episodes - episode_reward: 8.200 [3.000, 17.000] - loss: 643762607.155 - mae: 18788860406.170 - mean_q: 22383269352.243 - mean_eps: 0.100 - ale.lives: 1.982\n",
            "\n",
            "Interval 161 (800000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0122\n",
            "7 episodes - episode_reward: 8.714 [2.000, 19.000] - loss: 446685999.565 - mae: 17426185188.966 - mean_q: 20799839404.032 - mean_eps: 0.100 - ale.lives: 1.955\n",
            "\n",
            "Interval 162 (805000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0188\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1560000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 162s 32ms/step - reward: 0.0188\n",
            "5 episodes - episode_reward: 17.600 [12.000, 21.000] - loss: 436482528.781 - mae: 16397426109.645 - mean_q: 19585451234.099 - mean_eps: 0.100 - ale.lives: 2.097\n",
            "\n",
            "Interval 163 (810000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0190\n",
            "6 episodes - episode_reward: 15.833 [9.000, 26.000] - loss: 449282605.824 - mae: 17243010372.403 - mean_q: 20587751766.426 - mean_eps: 0.100 - ale.lives: 2.176\n",
            "\n",
            "Interval 164 (815000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0148\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1570000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0148\n",
            "5 episodes - episode_reward: 12.800 [8.000, 18.000] - loss: 497002344.499 - mae: 18252103084.442 - mean_q: 21787383645.798 - mean_eps: 0.100 - ale.lives: 1.850\n",
            "\n",
            "Interval 165 (820000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0132\n",
            "5 episodes - episode_reward: 14.200 [10.000, 21.000] - loss: 506413846.554 - mae: 19078212530.176 - mean_q: 22771787540.070 - mean_eps: 0.100 - ale.lives: 1.890\n",
            "\n",
            "Interval 166 (825000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0146\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1580000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0146\n",
            "6 episodes - episode_reward: 13.833 [11.000, 17.000] - loss: 776131237.734 - mae: 25050940030.976 - mean_q: 29901903465.677 - mean_eps: 0.100 - ale.lives: 2.079\n",
            "\n",
            "Interval 167 (830000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0126\n",
            "5 episodes - episode_reward: 12.000 [11.000, 16.000] - loss: 857151829.120 - mae: 27743762271.437 - mean_q: 33115694284.800 - mean_eps: 0.100 - ale.lives: 2.130\n",
            "\n",
            "Interval 168 (835000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0166\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1590000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 161s 32ms/step - reward: 0.0166\n",
            "6 episodes - episode_reward: 13.833 [11.000, 18.000] - loss: 1229130615.859 - mae: 34401205554.381 - mean_q: 41081089938.227 - mean_eps: 0.100 - ale.lives: 1.920\n",
            "\n",
            "Interval 169 (840000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0128\n",
            "2 episodes - episode_reward: 26.500 [26.000, 27.000] - loss: 1346661243.136 - mae: 37920559880.602 - mean_q: 45262017750.630 - mean_eps: 0.100 - ale.lives: 1.841\n",
            "\n",
            "Interval 170 (845000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0158\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1600000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0158\n",
            "6 episodes - episode_reward: 15.167 [11.000, 17.000] - loss: 1471805594.726 - mae: 40634908622.848 - mean_q: 48545487450.931 - mean_eps: 0.100 - ale.lives: 2.138\n",
            "\n",
            "Interval 171 (850000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0186\n",
            "6 episodes - episode_reward: 15.500 [7.000, 25.000] - loss: 2269231090.586 - mae: 48331569004.544 - mean_q: 57694019321.856 - mean_eps: 0.100 - ale.lives: 1.912\n",
            "\n",
            "Interval 172 (855000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0150\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1610000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 162s 32ms/step - reward: 0.0150\n",
            "5 episodes - episode_reward: 14.800 [10.000, 18.000] - loss: 41592886968.320 - mae: 81733828221.338 - mean_q: 87279945603.482 - mean_eps: 0.100 - ale.lives: 2.127\n",
            "\n",
            "Interval 173 (860000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0132\n",
            "5 episodes - episode_reward: 12.000 [11.000, 16.000] - loss: 2296270376.346 - mae: 90068239679.488 - mean_q: 108310481600.512 - mean_eps: 0.100 - ale.lives: 2.098\n",
            "\n",
            "Interval 174 (865000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0130\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1620000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0130\n",
            "6 episodes - episode_reward: 12.167 [5.000, 17.000] - loss: 2123569748.173 - mae: 83182751468.749 - mean_q: 99302087065.600 - mean_eps: 0.100 - ale.lives: 2.118\n",
            "\n",
            "Interval 175 (870000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0134\n",
            "5 episodes - episode_reward: 12.000 [8.000, 20.000] - loss: 2175328025.190 - mae: 87788387303.424 - mean_q: 104836879764.685 - mean_eps: 0.100 - ale.lives: 2.045\n",
            "\n",
            "Interval 176 (875000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0148\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1630000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0148\n",
            "5 episodes - episode_reward: 14.400 [11.000, 18.000] - loss: 4740003594.240 - mae: 106374957642.547 - mean_q: 126973004238.029 - mean_eps: 0.100 - ale.lives: 2.054\n",
            "\n",
            "Interval 177 (880000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0194\n",
            "6 episodes - episode_reward: 17.500 [14.000, 21.000] - loss: 2921365761.843 - mae: 114079593339.290 - mean_q: 136361643586.355 - mean_eps: 0.100 - ale.lives: 2.004\n",
            "\n",
            "Interval 178 (885000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0164\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1640000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0164\n",
            "5 episodes - episode_reward: 14.800 [11.000, 19.000] - loss: 4464687276.646 - mae: 129102595404.595 - mean_q: 154494059026.842 - mean_eps: 0.100 - ale.lives: 2.076\n",
            "\n",
            "Interval 179 (890000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0132\n",
            "6 episodes - episode_reward: 13.167 [11.000, 19.000] - loss: 5055320872.653 - mae: 147060475651.686 - mean_q: 176130156986.368 - mean_eps: 0.100 - ale.lives: 2.068\n",
            "\n",
            "Interval 180 (895000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0146\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1650000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 161s 32ms/step - reward: 0.0146\n",
            "5 episodes - episode_reward: 13.800 [11.000, 19.000] - loss: 4294152164.762 - mae: 156722476181.094 - mean_q: 187487007722.701 - mean_eps: 0.100 - ale.lives: 2.121\n",
            "\n",
            "Interval 181 (900000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0150\n",
            "5 episodes - episode_reward: 13.800 [11.000, 17.000] - loss: 4151322222.182 - mae: 153132325496.422 - mean_q: 182829524123.648 - mean_eps: 0.100 - ale.lives: 2.047\n",
            "\n",
            "Interval 182 (905000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0124\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1660000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 161s 32ms/step - reward: 0.0124\n",
            "6 episodes - episode_reward: 11.000 [5.000, 16.000] - loss: 4102436256.768 - mae: 156959296258.048 - mean_q: 187353677103.104 - mean_eps: 0.100 - ale.lives: 1.993\n",
            "\n",
            "Interval 183 (910000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 13.833 [11.000, 17.000] - loss: 4973435629.978 - mae: 171910913549.926 - mean_q: 205619870826.496 - mean_eps: 0.100 - ale.lives: 2.085\n",
            "\n",
            "Interval 184 (915000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0140\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1670000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0140\n",
            "5 episodes - episode_reward: 13.400 [11.000, 16.000] - loss: 6461242863.206 - mae: 191934767746.253 - mean_q: 229882392150.016 - mean_eps: 0.100 - ale.lives: 2.124\n",
            "\n",
            "Interval 185 (920000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0150\n",
            "7 episodes - episode_reward: 11.429 [5.000, 18.000] - loss: 4790873235.046 - mae: 182819617806.746 - mean_q: 218334470301.286 - mean_eps: 0.100 - ale.lives: 2.032\n",
            "\n",
            "Interval 186 (925000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0132\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1680000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0132\n",
            "5 episodes - episode_reward: 12.200 [11.000, 16.000] - loss: 6211157356.954 - mae: 205984079203.533 - mean_q: 246268603583.693 - mean_eps: 0.100 - ale.lives: 2.168\n",
            "\n",
            "Interval 187 (930000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0160\n",
            "6 episodes - episode_reward: 13.333 [11.000, 17.000] - loss: 19147931917.926 - mae: 251187311345.664 - mean_q: 299115977952.461 - mean_eps: 0.100 - ale.lives: 2.135\n",
            "\n",
            "Interval 188 (935000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0146\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1690000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0146\n",
            "5 episodes - episode_reward: 13.600 [11.000, 17.000] - loss: 7610899613.286 - mae: 252684246450.176 - mean_q: 303103801373.491 - mean_eps: 0.100 - ale.lives: 2.095\n",
            "\n",
            "Interval 189 (940000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0158\n",
            "6 episodes - episode_reward: 14.000 [11.000, 17.000] - loss: 6013631199.232 - mae: 214583212244.992 - mean_q: 256328768107.315 - mean_eps: 0.100 - ale.lives: 2.126\n",
            "\n",
            "Interval 190 (945000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0136\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1700000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0136\n",
            "5 episodes - episode_reward: 13.000 [11.000, 16.000] - loss: 8399825651.712 - mae: 259230127187.558 - mean_q: 309609014991.258 - mean_eps: 0.100 - ale.lives: 2.101\n",
            "\n",
            "Interval 191 (950000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0154\n",
            "5 episodes - episode_reward: 14.600 [11.000, 18.000] - loss: 6622463738.470 - mae: 209469254415.155 - mean_q: 249917020346.778 - mean_eps: 0.100 - ale.lives: 2.063\n",
            "\n",
            "Interval 192 (955000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0168\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1710000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0168\n",
            "6 episodes - episode_reward: 15.333 [11.000, 20.000] - loss: 6108605248.102 - mae: 206037526708.224 - mean_q: 245891791611.494 - mean_eps: 0.100 - ale.lives: 2.064\n",
            "\n",
            "Interval 193 (960000 steps performed)\n",
            "5000/5000 [==============================] - 153s 31ms/step - reward: 0.0122\n",
            "5 episodes - episode_reward: 12.400 [4.000, 21.000] - loss: 5914762773.709 - mae: 187442918916.096 - mean_q: 223534421363.917 - mean_eps: 0.100 - ale.lives: 2.003\n",
            "\n",
            "Interval 194 (965000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0164\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1720000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0164\n",
            "6 episodes - episode_reward: 14.333 [9.000, 24.000] - loss: 7274075604.787 - mae: 221636991241.421 - mean_q: 264380531041.894 - mean_eps: 0.100 - ale.lives: 2.015\n",
            "\n",
            "Interval 195 (970000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0152\n",
            "4 episodes - episode_reward: 13.500 [11.000, 16.000] - loss: 7794159414.272 - mae: 232516935339.213 - mean_q: 277227136588.186 - mean_eps: 0.100 - ale.lives: 2.109\n",
            "\n",
            "Interval 196 (975000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0068\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1730000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 161s 32ms/step - reward: 0.0068\n",
            "7 episodes - episode_reward: 7.714 [3.000, 26.000] - loss: 6399401997.926 - mae: 182605772501.811 - mean_q: 217539612901.376 - mean_eps: 0.100 - ale.lives: 1.983\n",
            "\n",
            "Interval 197 (980000 steps performed)\n",
            "5000/5000 [==============================] - 150s 30ms/step - reward: 0.0076\n",
            "8 episodes - episode_reward: 5.000 [1.000, 12.000] - loss: 7307093470.003 - mae: 207777706265.805 - mean_q: 247757592172.954 - mean_eps: 0.100 - ale.lives: 2.020\n",
            "\n",
            "Interval 198 (985000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0062\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1740000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0062\n",
            "6 episodes - episode_reward: 4.833 [3.000, 8.000] - loss: 5630662860.800 - mae: 168175701760.410 - mean_q: 200660049461.248 - mean_eps: 0.100 - ale.lives: 2.138\n",
            "\n",
            "Interval 199 (990000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0152\n",
            "6 episodes - episode_reward: 12.167 [4.000, 17.000] - loss: 5511875774.874 - mae: 182922705318.707 - mean_q: 218177364964.147 - mean_eps: 0.100 - ale.lives: 2.052\n",
            "\n",
            "Interval 200 (995000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0152\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1750000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0152\n",
            "5 episodes - episode_reward: 14.200 [8.000, 19.000] - loss: 5174173256.294 - mae: 178406982628.147 - mean_q: 212792144625.664 - mean_eps: 0.100 - ale.lives: 1.833\n",
            "\n",
            "Interval 201 (1000000 steps performed)\n",
            "5000/5000 [==============================] - 154s 31ms/step - reward: 0.0148\n",
            "5 episodes - episode_reward: 15.200 [7.000, 27.000] - loss: 5967645174.579 - mae: 194369064783.053 - mean_q: 231824388482.662 - mean_eps: 0.100 - ale.lives: 2.021\n",
            "\n",
            "Interval 202 (1005000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0184\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1760000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0184\n",
            "5 episodes - episode_reward: 15.600 [14.000, 18.000] - loss: 10274747100.365 - mae: 261663948800.000 - mean_q: 312795783469.466 - mean_eps: 0.100 - ale.lives: 1.892\n",
            "\n",
            "Interval 203 (1010000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0176\n",
            "6 episodes - episode_reward: 16.833 [12.000, 23.000] - loss: 7844635604.992 - mae: 257483430992.282 - mean_q: 307751752027.341 - mean_eps: 0.100 - ale.lives: 2.128\n",
            "\n",
            "Interval 204 (1015000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0172\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1770000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 161s 32ms/step - reward: 0.0172\n",
            "6 episodes - episode_reward: 14.833 [12.000, 17.000] - loss: 10344022204.416 - mae: 281898361880.576 - mean_q: 336570456657.101 - mean_eps: 0.100 - ale.lives: 1.994\n",
            "\n",
            "Interval 205 (1020000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0180\n",
            "6 episodes - episode_reward: 15.833 [12.000, 20.000] - loss: 11781228436.275 - mae: 306264895691.162 - mean_q: 366492393170.534 - mean_eps: 0.100 - ale.lives: 1.801\n",
            "\n",
            "Interval 206 (1025000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0154\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1780000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0154\n",
            "6 episodes - episode_reward: 13.000 [11.000, 17.000] - loss: 20474040812.339 - mae: 346043115215.258 - mean_q: 413747147964.416 - mean_eps: 0.100 - ale.lives: 2.102\n",
            "\n",
            "Interval 207 (1030000 steps performed)\n",
            "5000/5000 [==============================] - 152s 30ms/step - reward: 0.0146\n",
            "5 episodes - episode_reward: 13.400 [11.000, 17.000] - loss: 30880834483.814 - mae: 404583046420.890 - mean_q: 483687492537.549 - mean_eps: 0.100 - ale.lives: 2.143\n",
            "\n",
            "Interval 208 (1035000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0138\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1790000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 160s 32ms/step - reward: 0.0138\n",
            "6 episodes - episode_reward: 11.500 [11.000, 12.000] - loss: 12528899376.333 - mae: 362811412683.162 - mean_q: 436453192105.984 - mean_eps: 0.100 - ale.lives: 2.172\n",
            "\n",
            "Interval 209 (1040000 steps performed)\n",
            "5000/5000 [==============================] - 151s 30ms/step - reward: 0.0176\n",
            "6 episodes - episode_reward: 15.500 [11.000, 20.000] - loss: 8878641072.128 - mae: 284260663741.645 - mean_q: 339772236523.110 - mean_eps: 0.100 - ale.lives: 2.006\n",
            "\n",
            "Interval 210 (1045000 steps performed)\n",
            "4997/5000 [============================>.] - ETA: 0s - reward: 0.0172\n",
            "[Checkpoint] Pesos guardados en: ./models\\modelo_D1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1800000.h5f\n",
            "\n",
            "[Checkpoint] Memoria guardada de forma segura en: ./models\\modelo_D1\\checkpoints\\memory.pkl\n",
            "5000/5000 [==============================] - 159s 32ms/step - reward: 0.0172\n",
            "done, took 31474.468 seconds\n"
          ]
        }
      ],
      "source": [
        "# ENTRENAR DQN\n",
        "if TRAIN_STEPS>0:\n",
        "  dqn.fit(env, callbacks=callbacks, nb_steps=TRAIN_STEPS-last_checkpoint_steps, log_interval=LOG_INTERVAL, visualize=False)\n",
        "  dqn.save_weights(weights_filename, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHYryKd1Gb2b",
        "outputId": "cf7d86ae-ab74-4723-8367-ffadc582e464",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 18.000, steps: 722\n",
            "Episode 2: reward: 18.000, steps: 708\n",
            "Episode 3: reward: 18.000, steps: 725\n",
            "Episode 4: reward: 18.000, steps: 710\n",
            "Episode 5: reward: 18.000, steps: 708\n",
            "Episode 6: reward: 18.000, steps: 720\n",
            "Episode 7: reward: 18.000, steps: 725\n",
            "Episode 8: reward: 18.000, steps: 718\n",
            "Episode 9: reward: 18.000, steps: 723\n",
            "Episode 10: reward: 18.000, steps: 722\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x25e4b07cb88>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing part to calculate the mean reward\n",
        "weights_filename =os.path.join(WEIGHTS_DIR, 'dqn_{}_weights_{}.h5f'.format(env_name, model_name))\n",
        "#weights_filename=latest_checkpoint\n",
        "#print(weights_filename)\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1ne1WCO5ISG"
      },
      "outputs": [],
      "source": [
        "# graph_training_csv(log_csv_path, MODEL_DIR, model_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv_rl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
