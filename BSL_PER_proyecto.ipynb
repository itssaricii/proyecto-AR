{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "*   Alumno 1: de Antón Santiago, Sara\n",
    "*   Alumno 2: Sánchez La O, Benjamín C.\n",
    "*   Alumno 3:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S_YDFwZ-JscI",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:36.425589Z",
     "start_time": "2025-07-02T20:14:36.416580Z"
    }
   },
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6n7MIefJ21i",
    "outputId": "d3205c1b-5605-41cb-8d66-b3b56546901b",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:36.517508Z",
     "start_time": "2025-07-02T20:14:36.504520Z"
    }
   },
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en el directorio: \n",
      "['.Backup_venv_tf253', '.git', '.gitignore', '.idea', '.ipynb_checkpoints', '.venv_tf253', 'best_episode.mp4', 'BSL_PER_proyecto.ipynb', 'BSL_PER_proyecto.py', 'BSL_proyecto.ipynb', 'graph.py', 'models', 'proyecto.ipynb', 'README.txt', 'SDAS_proyecto.ipynb', 'showMetrics.ipynb', 'showTrainingGraphs.ipynb', 'testAgent.py', 'upload_weights.ipynb', 'weights']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UbVRjvHCJ8UF",
    "outputId": "1af0ec8c-c8a2-493c-87b1-2c526d888910",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:50.076486Z",
     "start_time": "2025-07-02T20:14:36.661694Z"
    }
   },
   "source": [
    "# NO EJECUTAR EN SAGEMAKER (MIRAR EL README PARA CONFIGURAR EL ENTORNO)\n",
    "# AUNQUE DE FALLOS DE INSTALACIÓN LOS IMPORTS FUNCIONAN\n",
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.12  #2.8\n",
    "else:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.17.3 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from gym==0.17.3) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from gym==0.17.3) (1.19.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from gym==0.17.3) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from gym==0.17.3) (1.6.0)\n",
      "Requirement already satisfied: future in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/Kojoley/atari-py.git\n",
      "  Cloning https://github.com/Kojoley/atari-py.git to c:\\users\\benjamincarmelo.sanc\\appdata\\local\\temp\\pip-req-build-sqb2xav4\n",
      "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from atari-py==1.2.2) (1.19.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git 'C:\\Users\\benjamincarmelo.sanc\\AppData\\Local\\Temp\\pip-req-build-sqb2xav4'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet==1.5.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: future in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from pyglet==1.5.0) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: h5py==3.1.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from h5py==3.1.0) (1.19.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: Pillow==9.5.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (9.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: keras-rl2==1.0.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from keras-rl2==1.0.5) (2.5.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.19.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.45.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (75.3.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (8.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: Keras==2.2.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from Keras==2.2.4) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from Keras==2.2.4) (1.5.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from Keras==2.2.4) (1.15.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from Keras==2.2.4) (6.0.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from Keras==2.2.4) (3.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from Keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from Keras==2.2.4) (1.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow==2.5.3 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (2.5.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (1.19.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (0.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (3.20.3)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (0.45.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow==2.5.3) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (75.3.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.3) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.3) (8.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow==2.5.3) (2.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.3) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.3) (3.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch==2.0.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from torch==2.0.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from torch==2.0.1) (3.7.4.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from torch==2.0.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from torch==2.0.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from torch==2.0.1) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from jinja2->torch==2.0.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: agents==1.4.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from agents==1.4.0) (2.5.3)\n",
      "Requirement already satisfied: gym in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from agents==1.4.0) (0.17.3)\n",
      "Requirement already satisfied: ruamel.yaml in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from agents==1.4.0) (0.18.14)\n",
      "Requirement already satisfied: scipy in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from gym->agents==1.4.0) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from gym->agents==1.4.0) (1.19.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from gym->agents==1.4.0) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from gym->agents==1.4.0) (1.6.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from ruamel.yaml->agents==1.4.0) (0.2.8)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.20.3)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.45.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.34.1)\n",
      "Requirement already satisfied: future in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym->agents==1.4.0) (1.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (75.3.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->agents==1.4.0) (8.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (2.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:54.428932Z",
     "start_time": "2025-07-02T20:14:50.092346Z"
    }
   },
   "source": [
    "# Añadidas y compatible con las versiones anteriores\n",
    "if not IN_COLAB:\n",
    "  %pip install pandas==1.1.3\n",
    "  %pip install \"matplotlib>=3.1,<3.4\"\n",
    "  %pip install scipy==1.5.2\n",
    "  %pip install seaborn==0.11.1\n",
    "  %pip install imageio\n",
    "  %pip install imageio[ffmpeg]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.1.3 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from pandas==1.1.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from pandas==1.1.3) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from pandas==1.1.3) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from python-dateutil>=2.7.3->pandas==1.1.3) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib<3.4,>=3.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from matplotlib<3.4,>=3.1) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from matplotlib<3.4,>=3.1) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from matplotlib<3.4,>=3.1) (1.19.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from matplotlib<3.4,>=3.1) (9.5.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from matplotlib<3.4,>=3.1) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from matplotlib<3.4,>=3.1) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from python-dateutil>=2.1->matplotlib<3.4,>=3.1) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scipy==1.5.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from scipy==1.5.2) (1.19.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn==0.11.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (0.11.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from seaborn==0.11.1) (1.19.2)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from seaborn==0.11.1) (1.5.2)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from seaborn==0.11.1) (1.1.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from seaborn==0.11.1) (3.3.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from matplotlib>=2.2->seaborn==0.11.1) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from matplotlib>=2.2->seaborn==0.11.1) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from matplotlib>=2.2->seaborn==0.11.1) (9.5.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from matplotlib>=2.2->seaborn==0.11.1) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from matplotlib>=2.2->seaborn==0.11.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from pandas>=0.23->seaborn==0.11.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn==0.11.1) (1.15.0)\n",
      "Requirement already satisfied: imageio in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (2.35.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from imageio) (1.19.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from imageio) (9.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imageio[ffmpeg] in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (2.35.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from imageio[ffmpeg]) (1.19.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from imageio[ffmpeg]) (9.5.0)\n",
      "Requirement already satisfied: imageio-ffmpeg in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from imageio[ffmpeg]) (0.5.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from imageio[ffmpeg]) (7.0.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\benjamincarmelo.sanc\\pycharmprojects\\proyecto-ar\\.venv_tf253\\lib\\site-packages (from imageio-ffmpeg->imageio[ffmpeg]) (75.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "45BPUdIoB41o",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:54.460591Z",
     "start_time": "2025-07-02T20:14:54.447026Z"
    }
   },
   "source": [
    "\n",
    "# DEFINIR AL PRINCIPIO\n",
    "# BSL: Hace falta de nuevo?\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j3eRhgI-Gb2a",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:58.550570Z",
     "start_time": "2025-07-02T20:14:54.477495Z"
    }
   },
   "source": [
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Input\n",
    "from tensorflow.keras.optimizers import Adam, schedules\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory, Memory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "\n",
    "# AÑADIDO\n",
    "from tensorflow.keras.layers import Lambda, BatchNormalization\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from collections import namedtuple, deque\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jwOE6I_KGb2a",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:58.769536Z",
     "start_time": "2025-07-02T20:14:58.567026Z"
    }
   },
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9jGEZUcpGb2a",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:58.801534Z",
     "start_time": "2025-07-02T20:14:58.787539Z"
    }
   },
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WRL-ixOyPIo7",
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:58.848309Z",
     "start_time": "2025-07-02T20:14:58.835558Z"
    }
   },
   "source": [
    "# ROOT PATH PARA LOS MODELOS Y SUS PESOS\n",
    "if IN_COLAB:\n",
    "  mount='/content/gdrive'\n",
    "  drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
    "  MODELS_DIR=drive_root+\"/models\"\n",
    "else:\n",
    "  MODELS_DIR=\"./models\"\n",
    "\n",
    "def get_dirs(model_name=\"modelo1\"):\n",
    "    WEIGHTS_DIR = os.path.join(MODELS_DIR, model_name, \"weights\")\n",
    "    CHECKPOINTS_DIR = os.path.join(MODELS_DIR, model_name, \"checkpoints\")\n",
    "    MODEL_DIR = os.path.join(MODELS_DIR, model_name)\n",
    "    os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "    os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "    return MODEL_DIR, WEIGHTS_DIR, CHECKPOINTS_DIR\n",
    "\n",
    "def save_hyperparams(modelo):\n",
    "    hyper_file = os.path.join(MODELS_DIR, modelo, modelo + '.json')\n",
    "    with open(hyper_file, 'w') as f:\n",
    "        json.dump(hiperparametros, f, indent=4)\n",
    "    print(f\"[INFO] Hiperparámetros guardados en {hyper_file}\")\n",
    "\n",
    "\n",
    "def load_hyperparams(modelo):\n",
    "    hyper_file = os.path.join(MODELS_DIR, modelo, modelo + '.json')\n",
    "\n",
    "    os.makedirs(os.path.dirname(hyper_file), exist_ok=True)\n",
    "\n",
    "\t# HIPERPARÁMETROS POR DEFECTO\n",
    "    hiperparametros = {\n",
    "        \"MEMORY_SIZE\": 1000000,\n",
    "        \"WARMUP_STEPS\": 50000,\n",
    "        \"SCHEDULER_STEPS\": 1000000,\n",
    "        \"GAMMA\": 0.99,\n",
    "        \"MODEL_UPDATE\": 10000,\n",
    "        \"LEARNING_RATE\": 0.00025,\n",
    "        \"MODEL_CHECKPOINT_STEPS\": 25000,\n",
    "        \"TRAIN_STEPS\": 1750000,\n",
    "        \"LOG_INTERVAL\": 10000,\n",
    "        \"DELTA_CLIP\": 1.0\n",
    "    }\n",
    "\n",
    "    if not os.path.exists(hyper_file):\n",
    "        with open(hyper_file, 'w') as f:\n",
    "            json.dump(hiperparametros, f, indent=4)\n",
    "\n",
    "        print(f\"[INFO] Fichero de hiperparámetros no encontrado. Creado por defecto en: {hyper_file}\")\n",
    "        params = hiperparametros\n",
    "    else:\n",
    "        with open(hyper_file, 'r') as f:\n",
    "            params = json.load(f)\n",
    "\n",
    "    for key, value in params.items():\n",
    "        globals()[key] = value\n",
    "        hiperparametros[key] = value\n",
    "\n",
    "    print(f\"[INFO] Hiperparámetros cargados desde {hyper_file}\")\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qUOajl31fsW7",
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:58.879001Z",
     "start_time": "2025-07-02T20:14:58.864395Z"
    }
   },
   "source": [
    "# CALLBACK CUSTOM DEL LOGGER PARA PODER GUARDAR TODA LA INFO EN UN MISMO FICHERO YA QUE ANTES SE SOBREESCRIBÍA\n",
    "# BSL: AÑADIDAS NUEVAS MÉTRICAS AL LOGGER\n",
    "class EpisodeLoggerCSV(Callback):\n",
    "\tdef __init__(self, filepath, verbose=False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.filepath = filepath\n",
    "\t\tself.verbose = verbose\n",
    "\t\tself.fields = [\n",
    "\t\t\t'episode', 'episode_reward', 'nb_steps', 'duration',\n",
    "\t\t\t'loss', 'mae', 'mean_q', 'mean_eps', 'ale.lives',\n",
    "\t\t\t'reward_min', 'reward_max'\n",
    "\t\t]\n",
    "\t\tself.episode_count = 0\n",
    "\t\tself.global_step_count = 0  # New: Global step counter\n",
    "\t\tself.file = None\n",
    "\t\tself.writer = None\n",
    "\t\tself._reset_episode_stats()\n",
    "\n",
    "\tdef _reset_episode_stats(self):\n",
    "\t\tself.losses = []\n",
    "\t\tself.q_values = []\n",
    "\t\tself.maes = []\n",
    "\t\tself.epsilons = []\n",
    "\t\tself.lives = []\n",
    "\t\tself.reward_values = []\n",
    "\n",
    "\tdef on_train_begin(self, logs=None):\n",
    "\t\tfile_exists = os.path.exists(self.filepath)\n",
    "\n",
    "\t\t# Read the last episode number and nb_steps if the file exists\n",
    "\t\tif file_exists:\n",
    "\t\t\twith open(self.filepath, mode='r', newline='') as f:\n",
    "\t\t\t\treader = csv.DictReader(f)\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tlast_row = None\n",
    "\t\t\t\t\tfor row in reader:\n",
    "\t\t\t\t\t\tlast_row = row  # Iterate to the last row\n",
    "\t\t\t\t\tif last_row:\n",
    "\t\t\t\t\t\tif 'episode' in last_row:\n",
    "\t\t\t\t\t\t\tself.episode_count = int(last_row['episode'])\n",
    "\t\t\t\t\t\t\tprint(f\"Resuming from episode: {self.episode_count}\")\n",
    "\t\t\t\t\t\tif 'nb_steps' in last_row:  # New: Resume nb_steps\n",
    "\t\t\t\t\t\t\tself.global_step_count = int(last_row['nb_steps'])\n",
    "\t\t\t\t\t\t\tprint(f\"Resuming from global step: {self.global_step_count}\")\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(f\"Error reading last episode/steps from CSV: {e}. Starting counts from 0.\")\n",
    "\t\t\t\t\tself.episode_count = 0\n",
    "\t\t\t\t\tself.global_step_count = 0\n",
    "\n",
    "\t\tself.file = open(self.filepath, mode='a', newline='')\n",
    "\t\tself.writer = csv.DictWriter(self.file, fieldnames=self.fields)\n",
    "\n",
    "\t\tif not file_exists:\n",
    "\t\t\tself.writer.writeheader()\n",
    "\n",
    "\tdef on_step_end(self, step, logs=None):\n",
    "\t\tlogs = logs or {}\n",
    "\t\tself.global_step_count += 1  # Increment global step count\n",
    "\n",
    "\t\tif 'metrics' in logs and logs['metrics'] is not None:\n",
    "\t\t\tmetrics = logs['metrics']\n",
    "\t\t\tif len(metrics) >= 4:\n",
    "\t\t\t\tloss, mae, mean_q, mean_eps = metrics\n",
    "\t\t\t\tif not np.isnan(loss):\n",
    "\t\t\t\t\tself.losses.append(loss)\n",
    "\t\t\t\tif not np.isnan(mae):\n",
    "\t\t\t\t\tself.maes.append(mae)\n",
    "\t\t\t\tif not np.isnan(mean_q):\n",
    "\t\t\t\t\tself.q_values.append(mean_q)\n",
    "\t\t\t\tif not np.isnan(mean_eps):\n",
    "\t\t\t\t\tself.epsilons.append(mean_eps)\n",
    "\t\tif 'reward' in logs:\n",
    "\t\t\tself.reward_values.append(logs['reward'])\n",
    "\t\tif 'info' in logs and 'ale.lives' in logs['info']:\n",
    "\t\t\tself.lives.append(float(logs['info']['ale.lives']))\n",
    "\n",
    "\tdef on_episode_end(self, episode, logs=None):\n",
    "\t\tself.episode_count += 1\n",
    "\t\tlogs = logs or {}\n",
    "\n",
    "\t\trow = {\n",
    "\t\t\t'episode': self.episode_count,\n",
    "\t\t\t'episode_reward': logs.get('episode_reward'),\n",
    "\t\t\t'nb_steps': self.global_step_count,  # Use global_step_count\n",
    "\t\t\t'duration': logs.get('duration'),\n",
    "\t\t\t'loss': np.mean(self.losses) if self.losses else None,\n",
    "\t\t\t'mae': np.mean(self.maes) if self.maes else None,\n",
    "\t\t\t'mean_q': np.mean(self.q_values) if self.q_values else None,\n",
    "\t\t\t'mean_eps': np.mean(self.epsilons) if self.epsilons else None,\n",
    "\t\t\t'ale.lives': np.mean(self.lives) if self.lives else None,\n",
    "\t\t\t'reward_min': np.min(self.reward_values) if self.reward_values else None,\n",
    "\t\t\t'reward_max': np.max(self.reward_values) if self.reward_values else None\n",
    "\t\t}\n",
    "\n",
    "\t\tself.writer.writerow(row)\n",
    "\t\tself.file.flush()\n",
    "\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"[Log CSV] Episodio {row['episode']} - Recompensa: {row['episode_reward']} - \"\n",
    "\t\t\t\t  f\"Loss: {row['loss']}, MAE: {row['mae']}, Mean Q: {row['mean_q']} - Total Steps: {row['nb_steps']}\")\n",
    "\n",
    "\t\tself._reset_episode_stats()\n",
    "\n",
    "\tdef on_train_end(self, logs=None):\n",
    "\t\tif self.file:\n",
    "\t\t\tself.file.close()\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here's the fixed code cell with consistent indentation using spaces:\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:58.911001Z",
     "start_time": "2025-07-02T20:14:58.896012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CALLBACK CUSTOM PARA GUARDAR LOS CHECKPOINTS CON EL NOMBRE BIEN CUANDO SE REINICIA EL ENTRENAMIENTO Y LA MEMORIA\n",
    "# GUARDA MEJORES PESOS EN MEDIA MOVIL DE 10\n",
    "class AccumulatedCheckpoint(Callback):\n",
    "\tdef __init__(self, base_path, env_name, interval, initial_step=0):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.base_path = base_path\n",
    "\t\tself.weights_path = os.path.join(base_path, f'dqn_{env_name}_weights_{{step}}.h5f')\n",
    "\t\tself.memory_path = os.path.join(base_path, 'memory.pkl')\n",
    "\t\tself.policy_state_path = os.path.join(base_path, 'policy.json')\n",
    "\t\tself.interval = interval\n",
    "\t\tself.total_steps = initial_step\n",
    "\t\tprint(f\"Callback iniciado desde paso {self.total_steps}\")\n",
    "\t\tself.best_weights_path = os.path.join(base_path, f'dqn_{env_name}_best_weights.h5f')\n",
    "\t\tself.episode_rewards = deque(maxlen=10)\n",
    "\t\tself.best_moving_avg = float('-inf')\n",
    "\n",
    "\tdef on_step_end(self, step, logs={}):\n",
    "\t\tself.total_steps += 1\n",
    "\t\tif self.total_steps % self.interval == 0:\n",
    "\t\t\tbase = self.base_path.format(step=self.total_steps)\n",
    "\t\t\t\t\t\t# Guardar pesos del modelo\n",
    "\t\t\tweights_path = self.weights_path.format(step=self.total_steps)\n",
    "\t\t\tself.model.save_weights(weights_path, overwrite=True)\n",
    "\t\t\tprint(f\"\\n[Checkpoint] Pesos guardados en: {weights_path}\")\n",
    "\n",
    "\t\t\t# Guardar memoria de repetición\n",
    "\t\t\tmemory_path = self.memory_path\n",
    "\t\t\ttemp_path = memory_path + \".tmp\"\n",
    "\n",
    "\t\t\twith open(temp_path, \"wb\") as f:\n",
    "\t\t\t\tpickle.dump(self.model.memory, f)\n",
    "\n",
    "\t\t\t# Solo si guardar ha ido bien\n",
    "\t\t\tos.replace(temp_path, memory_path)\n",
    "\t\t\tprint(f\"\\n[Checkpoint] Memoria guardada de forma segura en: {memory_path}\")\n",
    "\n",
    "\tdef on_episode_end(self, episode, logs=None):\n",
    "\t\tlogs = logs or {}\n",
    "\t\treward = logs.get('episode_reward')\n",
    "\t\tif reward is not None:\n",
    "\t\t\tself.episode_rewards.append(reward)\n",
    "\t\t\tif len(self.episode_rewards) == 10:\n",
    "\t\t\t\tmoving_avg = sum(self.episode_rewards) / 10.0\n",
    "\t\t\t\tif moving_avg > self.best_moving_avg:\n",
    "\t\t\t\t\tself.best_moving_avg = moving_avg\n",
    "\t\t\t\t\tself.model.save_weights(os.path.join(WEIGHTS_DIR, 'dqn_{}_best_weights.h5f'.format(env_name)),\n",
    "\t\t\t\t\t\t\t\t\t\t\toverwrite=True)\n",
    "\t\t\t\t\tprint(f\"\\n🏆 Nueva mejor media móvil: {moving_avg:.2f}\")\n",
    "\t\t\t\t\tprint(f\"[Best] Pesos guardados en: {self.best_weights_path}\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:58.941622Z",
     "start_time": "2025-07-02T20:14:58.927179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FUNCIÓN PARA CARGAR EL ÚLTMO CHECKPOINT DETECTADO\n",
    "def load_last_checkpoint(dqn):\n",
    "  latest_checkpoint=None\n",
    "  # Buscar todos los checkpoints por su archivo .index\n",
    "  pattern = os.path.join(CHECKPOINTS_DIR, f'dqn_{env_name}_weights_*.h5f.index')\n",
    "  checkpoints = glob.glob(pattern)\n",
    "\n",
    "  # Valor por defecto si no se encuentra checkpoint\n",
    "  last_checkpoint_steps = 0\n",
    "\n",
    "  if checkpoints:\n",
    "      # Extraer el número de paso del nombre\n",
    "      def extract_step(filename):\n",
    "          try:\n",
    "              name = os.path.basename(filename)\n",
    "              step_part = name.split('_weights_')[1].replace('.h5f.index', '')\n",
    "              return int(step_part)\n",
    "          except:\n",
    "              return -1\n",
    "\n",
    "      # Seleccionar el checkpoint con mayor número de pasos\n",
    "      latest_index = max(checkpoints, key=extract_step)\n",
    "\n",
    "      # Quitar la extensión .index para obtener el nombre base\n",
    "      latest_checkpoint = latest_index.replace('.index', '')\n",
    "\n",
    "      print(f\"[DQN] Cargando último checkpoint: {latest_checkpoint}\")\n",
    "      dqn.load_weights(latest_checkpoint)\n",
    "\n",
    "      # Aquí extraemos los pasos acumulados\n",
    "      last_checkpoint_steps = extract_step(latest_index)\n",
    "  else:\n",
    "      print(\"[DQN] No se encontró ningún checkpoint, entrenamiento desde cero.\")\n",
    "  return dqn, last_checkpoint_steps,latest_checkpoint"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o2PywN6Wsef_",
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:58.972163Z",
     "start_time": "2025-07-02T20:14:58.958168Z"
    }
   },
   "source": [
    "# FUNCIÓN QUE PERMITE AJUSTAR LOS PARÁMETROS DE LA POLICY PARA QUE EL ENTRENAMIENTO SE REAUNDE POR DONDE TOCA\n",
    "# SI NO POR DEFECTO COMENZARÍA EN EL VALOR MÁXIMO DE EPSILON\n",
    "def adjust_policy_params(scheduler_steps, train_steps, current_step, value_max=1.0, value_min=0.1):\n",
    "    \"\"\"\n",
    "    Ajusta dinámicamente los parámetros de LinearAnnealedPolicy para reanudar la exploración desde el punto correcto.\n",
    "\n",
    "    Args:\n",
    "        scheduler_steps (int): Número de pasos que define la duración del decaimiento de eps.\n",
    "        train_steps (int): Número total de pasos planeados para el entrenamiento.\n",
    "        current_step (int): Paso actual o recuperado del checkpoint.\n",
    "        value_max (float): Valor inicial deseado de eps.\n",
    "        value_min (float): Valor mínimo deseado de eps.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (value_max_ajustado, value_min, scheduler_steps_restantes)\n",
    "    \"\"\"\n",
    "    if current_step >= scheduler_steps:\n",
    "        # Ya se alcanzó el mínimo, mantenerlo constante\n",
    "        return value_min, value_min, 1  # eps se queda fijo\n",
    "\n",
    "    # Calcular epsilon actual desde el punto alcanzado\n",
    "    frac = current_step / scheduler_steps\n",
    "    current_eps = value_max - (value_max - value_min) * frac\n",
    "\n",
    "    # Pasos restantes para completar el scheduler\n",
    "    steps_remaining = scheduler_steps - current_step\n",
    "    return current_eps, value_min, steps_remaining"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "88T-Q17-t1cw",
    "ExecuteTime": {
     "end_time": "2025-07-02T20:14:59.003163Z",
     "start_time": "2025-07-02T20:14:58.990165Z"
    }
   },
   "source": [
    "# FUNCIÓN PARA CARGAR LA MEMORIA Y EL NÚMERO DE PASOS DEL MODELO\n",
    "def get_memory_and_last_steps():\n",
    "  pattern = os.path.join(CHECKPOINTS_DIR, f'dqn_{env_name}_weights_*.h5f.index')\n",
    "  checkpoints = glob.glob(pattern)\n",
    "  last_checkpoint_steps = 0\n",
    "  memory = None\n",
    "\n",
    "  if checkpoints:\n",
    "      def extract_step(filename):\n",
    "          try:\n",
    "              name = os.path.basename(filename)\n",
    "              step_part = name.split('_weights_')[1].replace('.h5f.index', '')\n",
    "              return int(step_part)\n",
    "          except:\n",
    "              return -1\n",
    "\n",
    "      # Encontrar el último checkpoint\n",
    "      latest_index = max(checkpoints, key=extract_step)\n",
    "      latest_checkpoint = latest_index.replace('.index', '')\n",
    "      last_checkpoint_steps = extract_step(latest_index)\n",
    "      print(f\"Último step de checkpoint: {last_checkpoint_steps}\")\n",
    "\n",
    "      # Cargar memoria\n",
    "      memory_path = os.path.join(CHECKPOINTS_DIR, \"memory.pkl\")\n",
    "      if os.path.exists(memory_path):\n",
    "          with open(memory_path, \"rb\") as f:\n",
    "              memory = pickle.load(f)\n",
    "          print(f\"Memoria cargada desde: {memory_path}\")\n",
    "      else:\n",
    "          print(\"No se encontró memoria para este checkpoint.\")\n",
    "\n",
    "  else:\n",
    "      print(\"No se encontró ningún checkpoint, entrenamiento desde cero.\")\n",
    "  return memory, last_checkpoint_steps"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:56:35.678153Z",
     "start_time": "2025-07-01T17:56:35.664609Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def check_cuda():\n",
    "\tprint(\"TensorFlow version:\", tf.__version__)\n",
    "\tprint(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "\tprint(\"Built with GPU support:\", tf.test.is_built_with_gpu_support())\n",
    "\tprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# check_cuda()"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Variantes DQN\n",
    "\n",
    "Una vez alcanzado el objetivo de los 20 puntos que propone la práctica, se ha decido explorar algunas variantes de DQN en una serie de 3 experimentos, :\n",
    "\n",
    "- Dueling-DQN (DDQN)\n",
    "- Double Dueling-DQN (DDDQN)\n",
    "- Double Dueling-DQN + Prioritized Experience Replay (DDDQN+PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73g2U2Ik5ISA",
    "tags": []
   },
   "source": "## Dueling-DQN (DDQN)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXkQ9km05ISB",
    "tags": []
   },
   "source": [
    "### Red neuronal basada en el paper: [https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581)\n",
    "\n",
    "**Dueling Network Architectures for Deep Reinforcement Learning**\n",
    "**Autores:** Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas.\n",
    "**Año de publicación:** 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xmRuF8ndGcs"
   },
   "source": [
    "El término \"Duel DQN\" o \"Dueling DQN\" (DQN con redes en duelo) se refiere a una arquitectura específica de la red neuronal dentro del marco de DQN. No son \"redes separadas\" en el sentido de la red online y la red target, sino una **modificación a la capa de salida de una única red Q** que luego se duplica para formar la red online y la red target.\n",
    "\n",
    "La idea principal de Dueling DQN es que la función Q se puede descomponer en dos componentes:\n",
    "\n",
    "1.  **Valor del estado (V(s))**: Cuánto de bueno es estar en un estado dado, independientemente de la acción.\n",
    "2.  **Ventaja de la acción (A(s, a))**: Cuánto mejor (o peor) es tomar una acción específica en ese estado en comparación con el promedio de las acciones.\n",
    "\n",
    "Así, la función Q se estima como:\n",
    "\n",
    "$Q(s, a; \\theta, \\alpha, \\beta) = V(s; \\theta, \\beta) + (A(s, a; \\theta, \\alpha) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s, a'; \\theta, \\alpha))$\n",
    "\n",
    "Donde:\n",
    "* $\\theta$: Parámetros de las capas convolucionales (comunes a ambos flujos).\n",
    "* $\\alpha$: Parámetros de la capa densa de ventaja.\n",
    "* $\\beta$: Parámetros de la capa densa de valor.\n",
    "* $|\\mathcal{A}|$: Número de acciones.\n",
    "\n",
    "Esto permite que la red aprenda la importancia de los estados por separado de la importancia de cada acción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ibl40JjrJgAV"
   },
   "source": [
    "### Descripción del modelo Dueling DQN\n",
    "\n",
    "El modelo construido sigue la arquitectura Dueling DQN utilizando la API funcional de Keras. Comienza con una entrada de cuatro frames consecutivos del entorno, representados como imágenes de 84x84 píxeles, que se reorganizan con `Permute` para adaptarse al formato de canales de TensorFlow. A continuación, se procesan mediante tres capas convolucionales con activaciones ReLU y normalización por lotes (BatchNormalization), lo que permite a la red extraer representaciones espaciales de alto nivel. La salida convolucional se aplana y se divide en dos flujos independientes: uno para estimar el valor del estado $V(s)$ y otro para calcular las ventajas $A(s,a)$ asociadas a cada acción posible. Ambos flujos utilizan capas densas con 512 unidades y activaciones ReLU. Finalmente, los valores $Q(s,a)$ se obtienen combinando el valor y las ventajas mediante una capa `Lambda` que aplica la fórmula de dueling: $Q(s,a) = V(s) + (A(s,a) - \\text{mean}(A(s,a)))$. Esta separación mejora la estabilidad del entrenamiento en entornos donde las diferencias entre acciones son sutiles, como ocurre en Space Invaders. El modelo resultante es compacto y eficiente, adaptado al aprendizaje por refuerzo profundo con imágenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "np4nwhGdJ9xO"
   },
   "source": [
    "1. Implementación de la red neuronal"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T20:15:44.613123Z",
     "start_time": "2025-07-02T20:15:44.598646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def getDuelingDQN(model_name=\"ddqn\"):\n",
    "    input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE  # (4, 84, 84)\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Permute((2, 3, 1))(inputs)\n",
    "\n",
    "    # Capas convolucionales\n",
    "    # Conv1\n",
    "    x = Convolution2D(32, (8, 8), strides=(4, 4), name='Conv1')(x)\n",
    "    x = BatchNormalization(name='BN1')(x)\n",
    "    x = Activation('relu', name='ReLU1')(x)\n",
    "\n",
    "    # Conv2\n",
    "    x = Convolution2D(64, (4, 4), strides=(2, 2), name='Conv2')(x)\n",
    "    x = BatchNormalization(name='BN2')(x)\n",
    "    x = Activation('relu', name='ReLU2')(x)\n",
    "\n",
    "    # Conv3\n",
    "    x = Convolution2D(64, (3, 3), strides=(1, 1), name='Conv3')(x)\n",
    "    x = BatchNormalization(name='BN3')(x)\n",
    "    x = Activation('relu', name='ReLU3')(x)\n",
    "\n",
    "    # Aplanar la salida de las capas convolucionales antes de las capas densas\n",
    "    flattened_output = Flatten(name='Flatten_Layer')(x)\n",
    "\n",
    "    # --- Ramificación Dueling DQN ---\n",
    "\n",
    "    # 1. Stream para la estimación del Valor (V(s))\n",
    "    value_stream = Dense(512, name='Value_Dense')(flattened_output)\n",
    "    value_stream = Activation('relu', name='Value_ReLU')(value_stream)\n",
    "    value = Dense(1, activation='linear', name='Value_Output')(value_stream)  # Salida de valor escalar\n",
    "\n",
    "    # 2. Stream para la estimación de la Ventaja (A(s,a))\n",
    "    advantage_stream = Dense(512, name='Advantage_Dense')(flattened_output)\n",
    "    advantage_stream = Activation('relu', name='Advantage_ReLU')(advantage_stream)\n",
    "    advantage = Dense(nb_actions, activation='linear', name='Advantage_Output')(advantage_stream)  # Salida para cada acción\n",
    "\n",
    "    # Combinación de Valor y Ventaja para obtener Q(s,a)\n",
    "    output_q_values = Lambda(\n",
    "        lambda a: a[0] + (a[1] - K.mean(a[1], axis=1, keepdims=True)),\n",
    "        output_shape=(nb_actions,), name='Q_Value_Combined'\n",
    "    )([value, advantage])\n",
    "\n",
    "    # Crea el modelo final con las entradas y salidas definidas\n",
    "    model = Model(inputs=inputs, outputs=output_q_values, name=model_name)\n",
    "\n",
    "\t# Muestra un resumen de la arquitectura de la red\n",
    "    print(f\"\\nResumen de la Red Dueling DQN: {model_name}\")\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T20:17:41.244210Z",
     "start_time": "2025-07-02T20:17:41.101519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"ddqn.v5.0\"\n",
    "model = getDuelingDQN(model_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen de la Red Dueling DQN: ddqn.v5.2\n",
      "Model: \"ddqn.v5.2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 4, 84, 84)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 84, 84, 4)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 20, 20, 32)   8224        permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "BN1 (BatchNormalization)        (None, 20, 20, 32)   128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ReLU1 (Activation)              (None, 20, 20, 32)   0           BN1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv2D)                  (None, 9, 9, 64)     32832       ReLU1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BN2 (BatchNormalization)        (None, 9, 9, 64)     256         Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ReLU2 (Activation)              (None, 9, 9, 64)     0           BN2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv2D)                  (None, 7, 7, 64)     36928       ReLU2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BN3 (BatchNormalization)        (None, 7, 7, 64)     256         Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ReLU3 (Activation)              (None, 7, 7, 64)     0           BN3[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "Flatten_Layer (Flatten)         (None, 3136)         0           ReLU3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Value_Dense (Dense)             (None, 512)          1606144     Flatten_Layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Advantage_Dense (Dense)         (None, 512)          1606144     Flatten_Layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Value_ReLU (Activation)         (None, 512)          0           Value_Dense[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Advantage_ReLU (Activation)     (None, 512)          0           Advantage_Dense[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Value_Output (Dense)            (None, 1)            513         Value_ReLU[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Advantage_Output (Dense)        (None, 6)            3078        Advantage_ReLU[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Q_Value_Combined (Lambda)       (None, 6)            0           Value_Output[0][0]               \n",
      "                                                                 Advantage_Output[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 3,294,503\n",
      "Trainable params: 3,294,183\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTIIRs1bJ9xO"
   },
   "source": "2. Implementación de la solución DDQN"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Los siguientes hiperparámetros pensamos que son un buen punto de partida para entrenar un agente Dueling-DQN en el entorno de Space Invaders, que es bastante complejo (con imágenes como observaciones y muchos enemigos en pantalla).\n",
    "\n",
    "Usamos una memoria de 50.000 pasos para que el agente tenga variedad de experiencias antes de aprender, y lo dejamos \"mirar\" durante 10.000 pasos (warmup) antes de empezar a entrenar. Se ha establecido `gamma` a 0.99 porque queremos que el agente valore las recompensas a largo plazo, no solo lo inmediato.\n",
    "Se ha configurado un LR muy bajo (0.00001) y se actualiza el modelo target cada 5000 pasos para procurar estabilidad en el entrenamiento.\n",
    "\n",
    "No entrenamos en cada paso del juego, sino cada 3, lo que ahorra recursos y hace el entrenamiento más suave.\n",
    "La política de exploración empieza en 1.0 (totalmente aleatorio) y va bajando hasta 0.1 durante un 25% del entreamiento, lo cual creemos que será una exploración más que suficiente."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aYOJ-MCYy_X",
    "outputId": "1da50b1e-b2a7-48da-b5e2-7b57b3ac3a9a",
    "ExecuteTime": {
     "end_time": "2025-07-02T20:17:45.895582Z",
     "start_time": "2025-07-02T20:17:45.876581Z"
    }
   },
   "source": [
    "# GENERACIÓN O CARGA MODELO\n",
    "MODEL_DIR, WEIGHTS_DIR, CHECKPOINTS_DIR = get_dirs(model_name)\n",
    "load_hyperparams(model_name)\n",
    "\n",
    "# Añado los hiperparametros para el modelo dueling\n",
    "hiperparametros = {\n",
    "    \"MEMORY_SIZE\": 50000,               # Tamaño del buffer de memoria de repetición\n",
    "    \"WARMUP_STEPS\": 10000,              # Pasos antes de comenzar a entrenar (para llenar la memoria)\n",
    "    \"SCHEDULER_STEPS\": 250000,          # Pasos totales considerados en programación de aprendizaje/exploración\n",
    "    \"GAMMA\": 0.99,                      # Factor de descuento para recompensas futuras\n",
    "    \"MODEL_UPDATE\": 5000,               # Frecuencia de actualización del modelo objetivo (target network)\n",
    "    \"LEARNING_RATE\": 0.00001,           # Tasa de aprendizaje del optimizador\n",
    "    \"MODEL_CHECKPOINT_STEPS\": 10000,    # Frecuencia con la que se guardan los pesos del modelo\n",
    "    \"TRAIN_STEPS\": 1000000,             # Número total de pasos de entrenamiento\n",
    "    \"TRAIN_INTERVAL\": 3,                # Se entrena cada 3 pasos del entorno\n",
    "    \"LOG_INTERVAL\": 10000,              # Frecuencia de registro/log de métricas de entrenamiento\n",
    "    \"DELTA_CLIP\": 1.0,                  # Límite del error temporal (TD) para la pérdida Huber\n",
    "    \"ENABLE_DOUBLE_DQN\": False,         # Si se activa, usa Double DQN para reducir sobreestimación\n",
    "    \"ENABLE_DUELING_NETWORK\": False,    # Se ha implementado, en el modelo, no es necesario activarla.\n",
    "    \"EPSILON_START\": 1.0,               # Valor inicial de epsilon (exploración total)\n",
    "    \"EPSILON_MIN\": 0.1,                 # Valor mínimo de epsilon (exploración residual)\n",
    "    \"BATCH_SIZE\": 32                    # Tamaño del lote de entrenamiento por paso\n",
    "}\n",
    "\n",
    "\n",
    "# Guardar cambios\n",
    "save_hyperparams(model_name)\n",
    "\n",
    "# Volver a cargar para crear las variables globales nuevas\n",
    "load_hyperparams(model_name)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fichero de hiperparámetros no encontrado. Creado por defecto en: ./models\\ddqn.v5.2\\ddqn.v5.2.json\n",
      "[INFO] Hiperparámetros cargados desde ./models\\ddqn.v5.2\\ddqn.v5.2.json\n",
      "[INFO] Hiperparámetros guardados en ./models\\ddqn.v5.2\\ddqn.v5.2.json\n",
      "[INFO] Hiperparámetros cargados desde ./models\\ddqn.v5.2\\ddqn.v5.2.json\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xjm2NtxkJ9xP",
    "outputId": "690bf78b-087c-479a-a18e-acafdb8e6b9b",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-07-02T20:17:51.184980Z",
     "start_time": "2025-07-02T20:17:51.165458Z"
    }
   },
   "source": [
    "# DEFINICIÓN DE LA POLICY\n",
    "memory = None\n",
    "memory, last_checkpoint_steps=get_memory_and_last_steps()\n",
    "\n",
    "value_max, value_min, new_scheduler_steps = adjust_policy_params(\n",
    "    scheduler_steps=SCHEDULER_STEPS,\n",
    "    train_steps=TRAIN_STEPS,\n",
    "    current_step=last_checkpoint_steps\n",
    ")\n",
    "if not memory:\n",
    "  memory = SequentialMemory(limit=MEMORY_SIZE, window_length=WINDOW_LENGTH)\n",
    "  print(\"Memoria inicializada de 0\")\n",
    "\n",
    "print(f\"Valores de la policy: value_min={value_min}, value_max={value_max}, scheduler_steps={new_scheduler_steps}\")\n",
    "\n",
    "processor = AtariProcessor()\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
    "                              value_max=EPSILON_START,\n",
    "                              value_min=EPSILON_MIN,\n",
    "                              value_test=.05,\n",
    "                              nb_steps=new_scheduler_steps)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontró ningún checkpoint, entrenamiento desde cero.\n",
      "Memoria inicializada de 0\n",
      "Valores de la policy: value_min=0.1, value_max=1.0, scheduler_steps=250000\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E9mNDvVvJ9xP",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-07-02T20:17:56.496438Z",
     "start_time": "2025-07-02T20:17:54.766377Z"
    }
   },
   "source": [
    "dqn = DQNAgent(model=model,\n",
    "                     nb_actions=nb_actions,\n",
    "                     policy=policy,\n",
    "                     memory=memory,\n",
    "                     processor=processor,\n",
    "                     nb_steps_warmup=WARMUP_STEPS,\n",
    "                     gamma=GAMMA,\n",
    "                     target_model_update=MODEL_UPDATE,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     train_interval=4,\n",
    "                     delta_clip=DELTA_CLIP,\n",
    "                     enable_double_dqn=ENABLE_DOUBLE_DQN,\n",
    "                     enable_dueling_network=ENABLE_DUELING_NETWORK)\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Parche para evitar el error 'get_updates' que ya no existe\n",
    "    def patched_get_updates(self, loss, params):\n",
    "        return []\n",
    "    Adam.get_updates = patched_get_updates\n",
    "\n",
    "# Compilo con el agente el modelo\n",
    "dqn.compile(\n",
    "    Adam(learning_rate=hiperparametros[\"LEARNING_RATE\"]),\n",
    "    metrics=['mae']\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vY5KFvrgJ9xP",
    "ExecuteTime": {
     "end_time": "2025-07-02T20:18:00.308448Z",
     "start_time": "2025-07-02T20:18:00.303443Z"
    }
   },
   "source": [
    "weights_filename = os.path.join(WEIGHTS_DIR, 'dqn_{}_weights_{}.h5f'.format(env_name, model_name))\n",
    "checkpoint_weights_filename = os.path.join(CHECKPOINTS_DIR, 'dqn_' + env_name + '_weights_{step}.h5f')\n",
    "log_filename =os.path.join(MODEL_DIR, 'dqn_{}_log_{}.json'.format(env_name, model_name))\n",
    "log_csv_path = os.path.join(MODEL_DIR, f'{model_name}_training_log.csv')"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQe1Y5n5J9xP",
    "outputId": "21e29bcb-4c32-4c0e-d072-c369f03065d9",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-07-02T20:18:02.481962Z",
     "start_time": "2025-07-02T20:18:02.467424Z"
    }
   },
   "source": [
    "# CARGAR PESOS DEL ÚLTIMO CHECKPOINT SI EXISTE\n",
    "dqn, last_checkpoint_steps,latest_checkpoint = load_last_checkpoint(dqn)\n",
    "# CREAR CALLBACKS CUSTOMIZADOS QUE GUARDAN BIEN LOS CHECKPOINTS Y LOGS DEL TRAINING\n",
    "checkpoint_callback = AccumulatedCheckpoint(\n",
    "      base_path=CHECKPOINTS_DIR,\n",
    "      env_name=env_name,\n",
    "      interval=MODEL_CHECKPOINT_STEPS,\n",
    "      initial_step=last_checkpoint_steps\n",
    "  )\n",
    "callbacks = [checkpoint_callback]\n",
    "dqn.step = last_checkpoint_steps\n",
    "callbacks += [EpisodeLoggerCSV(log_csv_path)]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DQN] No se encontró ningún checkpoint, entrenamiento desde cero.\n",
      "Callback iniciado desde paso 0\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T20:18:13.375678Z",
     "start_time": "2025-07-02T20:18:06.542592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Hiperparámetros de\", model_name)\n",
    "print(json.dumps(hiperparametros, indent=4))\n",
    "\n",
    "if TRAIN_STEPS>0:\n",
    "  dqn.fit(env, callbacks=callbacks, nb_steps=TRAIN_STEPS-last_checkpoint_steps, log_interval=LOG_INTERVAL, visualize=False)\n",
    "  dqn.save_weights(weights_filename, overwrite=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiperparámetros de ddqn.v5.2\n",
      "{\n",
      "    \"MEMORY_SIZE\": 50000,\n",
      "    \"WARMUP_STEPS\": 10000,\n",
      "    \"SCHEDULER_STEPS\": 250000,\n",
      "    \"GAMMA\": 0.99,\n",
      "    \"MODEL_UPDATE\": 5000,\n",
      "    \"LEARNING_RATE\": 1e-05,\n",
      "    \"MODEL_CHECKPOINT_STEPS\": 10000,\n",
      "    \"TRAIN_STEPS\": 1000000,\n",
      "    \"TRAIN_INTERVAL\": 3,\n",
      "    \"LOG_INTERVAL\": 10000,\n",
      "    \"DELTA_CLIP\": 1.0,\n",
      "    \"ENABLE_DOUBLE_DQN\": false,\n",
      "    \"ENABLE_DUELING_NETWORK\": false,\n",
      "    \"EPSILON_START\": 1.0,\n",
      "    \"EPSILON_MIN\": 0.1,\n",
      "    \"BATCH_SIZE\": 32\n",
      "}\n",
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjamincarmelo.sanc\\PycharmProjects\\proyecto-AR\\.venv_tf253\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  905/10000 [=>............................] - ETA: 26s - reward: 0.0122done, took 6.715 seconds\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "class SlowTestCallback(Callback):\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        time.sleep(0.005)  # Ajusta el tiempo (en segundos) según quieras\n",
    "\n",
    "# Carga de pesos\n",
    "_, _,latest_checkpoint= load_last_checkpoint(dqn)\n",
    "dqn.load_weights(latest_checkpoint)\n",
    "# dqn.load_weights(\"./models/ddqn.v5.3/weights/dqn_SpaceInvaders-v0_best_weights.h5f\")\n",
    "history=dqn.test(env, nb_episodes=10, visualize=True, callbacks=[SlowTestCallback()])\n",
    "episode_rewards = history.history['episode_reward']\n",
    "average_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "print(f\"Recompensa media sobre 10 episodios: {average_reward}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Double-DDQN"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Red neuronal basada en el paper: [https://arxiv.org/abs/1509.06461](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    "**Deep Reinforcement Learning with Double Q-learning**\n",
    "**Autores:** Hado van Hasselt, Arthur Guez, David Silver.\n",
    "**Año de publicación:** 2015\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "El término **\"Double DQN\"** no es una arquitectura distinta de red neuronal como en el caso de Dueling DQN, sino a una **modificación del algoritmo de aprendizaje** que busca reducir el sesgo por sobreestimación de los valores Q, presente en el DQN clásico.\n",
    "\n",
    "En DQN estándar, la actualización de la red Q utiliza el máximo valor de Q estimado por la red *target* para todas las acciones posibles en el siguiente estado: $y^{\\text{DQN}} = r + \\gamma \\cdot \\max_{a'} Q_{\\text{target}}(s', a')$\n",
    "\n",
    "Esto puede llevar a una **sobreestimación sistemática**, ya que se utiliza la misma red para seleccionar y evaluar la acción.\n",
    "\n",
    "Double DQN corrige este problema separando esos dos roles. Se utiliza la **red online** para seleccionar la acción y la **red target** para evaluarla:$y^{\\text{DoubleDQN}} = r + \\gamma \\cdot Q_{\\text{target}}(s', \\arg\\max_{a'} Q_{\\text{online}}(s', a'))$\n",
    "\n",
    "De esta forma:\n",
    "\n",
    "- La **red online** selecciona la acción $a'$ que maximiza $Q_{\\text{online}}(s', a')$.\n",
    "- La **red target** evalúa esa acción seleccionada: $Q_{\\text{target}}(s', a')$.\n",
    "\n",
    "Este cambio reduce el sesgo sin introducir una carga computacional significativa, y mejora la estabilidad del entrenamiento en muchos entornos, especialmente aquellos con alta aleatoriedad o recompensas escasas como es el caso."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Descripción del modelo Double DQN\n",
    "\n",
    "El modelo construido para este experimento emplea la misma arquitectura base utilizada en el modelo Dueling DQN, pero a diferencia de este, no se separan los flujos de valor y ventaja; el foco de mejora está en el algoritmo de entrenamiento.\n",
    "\n",
    "La diferencia fundamental con el modelo DQN clásico está en cómo se calcula la **actualización del valor objetivo (target)** durante el aprendizaje. En lugar de usar el valor máximo estimado por la red target (como en DQN estándar), **Double DQN** utiliza la red online para seleccionar la acción con mejor valor estimado, y luego la red target para evaluar el valor de esa acción. Esta separación ayuda a mitigar el sesgo por sobreestimación común en DQN."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. Implementación de la red neuronal"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_name = \"ddqn.v5.1\"\n",
    "model = getDuelingDQN(model_name)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. Implementación de la solución Double-DDQN"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En este segundo experimento usando las redes \"dueling\", hemos entrenado un agente **Double Dueling DQN** en el entorno de **Space Invaders**, utilizando exactamente los mismos hiperparámetros que en el experimento anterior con Dueling DQN, salvo por la activación explícita del mecanismo *Double DQN*.\n",
    "\n",
    "Esta variante tiene como objetivo mitigar la sobreestimación de los valores Q, una limitación común en el DQN tradicional, mediante una separación más rigurosa entre la selección y evaluación de acciones durante la actualización. Al mantener constantes el resto de los parámetros, buscamos observar el impacto específico del componente *Double* sobre la estabilidad y calidad del aprendizaje en un entorno visualmente complejo como es Space Invaders.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:57:14.648135Z",
     "start_time": "2025-07-01T17:57:14.628132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GENERACIÓN O CARGA MODELO\n",
    "MODEL_DIR, WEIGHTS_DIR, CHECKPOINTS_DIR = get_dirs(model_name)\n",
    "load_hyperparams(model_name)\n",
    "\n",
    "# Añado los hiperparametros para el modelo dueling\n",
    "hiperparametros = {\n",
    "    \"MEMORY_SIZE\": 50000,               # Tamaño del buffer de memoria de repetición\n",
    "    \"WARMUP_STEPS\": 10000,              # Pasos antes de comenzar a entrenar (para llenar la memoria)\n",
    "    \"SCHEDULER_STEPS\": 250000,          # Pasos totales considerados en programación de aprendizaje/exploración\n",
    "    \"GAMMA\": 0.99,                      # Factor de descuento para recompensas futuras\n",
    "    \"MODEL_UPDATE\": 5000,               # Frecuencia de actualización del modelo objetivo (target network)\n",
    "    \"LEARNING_RATE\": 0.00001,           # Tasa de aprendizaje del optimizador\n",
    "    \"MODEL_CHECKPOINT_STEPS\": 10000,    # Frecuencia con la que se guardan los pesos del modelo\n",
    "    \"TRAIN_STEPS\": 1000000,             # Número total de pasos de entrenamiento\n",
    "    \"TRAIN_INTERVAL\": 3,                # Se entrena cada 3 pasos del entorno\n",
    "    \"LOG_INTERVAL\": 10000,              # Frecuencia de registro/log de métricas de entrenamiento\n",
    "    \"DELTA_CLIP\": 1.0,                  # Límite del error temporal (TD) para la pérdida Huber\n",
    "    \"ENABLE_DOUBLE_DQN\": True,          # Si se activa, usa Double DQN para reducir sobreestimación\n",
    "    \"ENABLE_DUELING_NETWORK\": False,    # Se ha implementado, en el modelo, no es necesario activarla.\n",
    "    \"EPSILON_START\": 1.0,               # Valor inicial de epsilon (exploración total)\n",
    "    \"EPSILON_MIN\": 0.1,                 # Valor mínimo de epsilon (exploración residual)\n",
    "    \"BATCH_SIZE\": 32                    # Tamaño del lote de entrenamiento por paso\n",
    "}\n",
    "\n",
    "\n",
    "# Guardar cambios\n",
    "save_hyperparams(model_name)\n",
    "\n",
    "# Volver a cargar para crear las variables globales nuevas\n",
    "load_hyperparams(model_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fichero de hiperparámetros no encontrado. Creado por defecto en: ./models\\ddqn.v5.1\\ddqn.v5.1.json\n",
      "[INFO] Hiperparámetros cargados desde ./models\\ddqn.v5.1\\ddqn.v5.1.json\n",
      "[INFO] Hiperparámetros guardados en ./models\\ddqn.v5.1\\ddqn.v5.1.json\n",
      "[INFO] Hiperparámetros cargados desde ./models\\ddqn.v5.1\\ddqn.v5.1.json\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:57:19.137539Z",
     "start_time": "2025-07-01T17:57:19.123933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DEFINICIÓN DE LA POLICY\n",
    "memory = None\n",
    "memory, last_checkpoint_steps=get_memory_and_last_steps()\n",
    "\n",
    "value_max, value_min, new_scheduler_steps = adjust_policy_params(\n",
    "    scheduler_steps=SCHEDULER_STEPS,\n",
    "    train_steps=TRAIN_STEPS,\n",
    "    current_step=last_checkpoint_steps\n",
    ")\n",
    "if not memory:\n",
    "  memory = SequentialMemory(limit=MEMORY_SIZE, window_length=WINDOW_LENGTH)\n",
    "  print(\"Memoria inicializada de 0\")\n",
    "\n",
    "print(f\"Valores de la policy: value_min={value_min}, value_max={value_max}, scheduler_steps={new_scheduler_steps}\")\n",
    "\n",
    "processor = AtariProcessor()\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
    "                              value_max=EPSILON_START,\n",
    "                              value_min=EPSILON_MIN,\n",
    "                              value_test=.05,\n",
    "                              nb_steps=new_scheduler_steps)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontró ningún checkpoint, entrenamiento desde cero.\n",
      "Memoria inicializada de 0\n",
      "Valores de la policy: value_min=0.1, value_max=1.0, scheduler_steps=250000\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:58:53.573987Z",
     "start_time": "2025-07-01T17:58:52.033117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dqn = DQNAgent(model=model,\n",
    "                     nb_actions=nb_actions,\n",
    "                     policy=policy,\n",
    "                     memory=memory,\n",
    "                     processor=processor,\n",
    "                     nb_steps_warmup=WARMUP_STEPS,\n",
    "                     gamma=GAMMA,\n",
    "                     target_model_update=MODEL_UPDATE,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     train_interval=4,\n",
    "                     delta_clip=DELTA_CLIP,\n",
    "                     enable_double_dqn=ENABLE_DOUBLE_DQN,\n",
    "                     enable_dueling_network=ENABLE_DUELING_NETWORK)\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Parche para evitar el error 'get_updates' que ya no existe\n",
    "    def patched_get_updates(self, loss, params):\n",
    "        return []\n",
    "    Adam.get_updates = patched_get_updates\n",
    "\n",
    "# Compilo con el agente el modelo\n",
    "dqn.compile(\n",
    "    Adam(learning_rate=hiperparametros[\"LEARNING_RATE\"]),\n",
    "    metrics=['mae']\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:59:00.601778Z",
     "start_time": "2025-07-01T17:59:00.593769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights_filename = os.path.join(WEIGHTS_DIR, 'dqn_{}_weights_{}.h5f'.format(env_name, model_name))\n",
    "checkpoint_weights_filename = os.path.join(CHECKPOINTS_DIR, 'dqn_' + env_name + '_weights_{step}.h5f')\n",
    "log_filename =os.path.join(MODEL_DIR, 'dqn_{}_log_{}.json'.format(env_name, model_name))\n",
    "log_csv_path = os.path.join(MODEL_DIR, f'{model_name}_training_log.csv')"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:59:03.290721Z",
     "start_time": "2025-07-01T17:59:03.277754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CARGAR PESOS DEL ÚLTIMO CHECKPOINT SI EXISTE\n",
    "dqn, last_checkpoint_steps,latest_checkpoint = load_last_checkpoint(dqn)\n",
    "# CREAR CALLBACKS CUSTOMIZADOS QUE GUARDAN BIEN LOS CHECKPOINTS Y LOGS DEL TRAINING\n",
    "checkpoint_callback = AccumulatedCheckpoint(\n",
    "      base_path=CHECKPOINTS_DIR,\n",
    "      env_name=env_name,\n",
    "      interval=MODEL_CHECKPOINT_STEPS,\n",
    "      initial_step=last_checkpoint_steps\n",
    "  )\n",
    "callbacks = [checkpoint_callback]\n",
    "dqn.step = last_checkpoint_steps\n",
    "callbacks += [EpisodeLoggerCSV(log_csv_path)]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DQN] No se encontró ningún checkpoint, entrenamiento desde cero.\n",
      "Callback iniciado desde paso 0\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T00:18:19.954303Z",
     "start_time": "2025-07-01T17:59:08.038939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Hiperparámetros de\", model_name)\n",
    "print(json.dumps(hiperparametros, indent=4))\n",
    "\n",
    "if TRAIN_STEPS>0:\n",
    "  dqn.fit(env, callbacks=callbacks, nb_steps=TRAIN_STEPS-last_checkpoint_steps, log_interval=LOG_INTERVAL, visualize=False)\n",
    "  dqn.save_weights(weights_filename, overwrite=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiperparámetros de ddqn.v5.1\n",
      "{\n",
      "    \"MEMORY_SIZE\": 50000,\n",
      "    \"WARMUP_STEPS\": 10000,\n",
      "    \"SCHEDULER_STEPS\": 250000,\n",
      "    \"GAMMA\": 0.99,\n",
      "    \"MODEL_UPDATE\": 5000,\n",
      "    \"LEARNING_RATE\": 1e-05,\n",
      "    \"MODEL_CHECKPOINT_STEPS\": 10000,\n",
      "    \"TRAIN_STEPS\": 1000000,\n",
      "    \"TRAIN_INTERVAL\": 3,\n",
      "    \"LOG_INTERVAL\": 10000,\n",
      "    \"DELTA_CLIP\": 1.0,\n",
      "    \"ENABLE_DOUBLE_DQN\": true,\n",
      "    \"ENABLE_DUELING_NETWORK\": false,\n",
      "    \"EPSILON_START\": 1.0,\n",
      "    \"EPSILON_MIN\": 0.1,\n",
      "    \"BATCH_SIZE\": 32\n",
      "}\n",
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjamincarmelo.sanc\\PycharmProjects\\proyecto-AR\\.venv_tf253\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6265/10000 [=================>............] - ETA: 11s - reward: 0.0148\n",
      "🏆 Nueva mejor media móvil: 9.30\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 7035/10000 [====================>.........] - ETA: 9s - reward: 0.0152\n",
      "🏆 Nueva mejor media móvil: 10.10\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 7690/10000 [======================>.......] - ETA: 7s - reward: 0.0150\n",
      "🏆 Nueva mejor media móvil: 10.40\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 8385/10000 [========================>.....] - ETA: 5s - reward: 0.0149\n",
      "🏆 Nueva mejor media móvil: 10.70\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9989/10000 [============================>.] - ETA: 0s - reward: 0.0148\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_10000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 38s 3ms/step - reward: 0.0148\n",
      "15 episodes - episode_reward: 9.467 [4.000, 22.000] - ale.lives: 2.252\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "  549/10000 [>.............................] - ETA: 4:08 - reward: 0.0164\n",
      "🏆 Nueva mejor media móvil: 11.10\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 1449/10000 [===>..........................] - ETA: 3:22 - reward: 0.0152\n",
      "🏆 Nueva mejor media móvil: 11.70\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9357/10000 [===========================>..] - ETA: 15s - reward: 0.0158\n",
      "🏆 Nueva mejor media móvil: 12.20\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0156\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_20000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 242s 24ms/step - reward: 0.0156\n",
      "13 episodes - episode_reward: 11.846 [4.000, 21.000] - loss: 0.039 - mae: 0.235 - mean_q: 0.370 - mean_eps: 0.946 - ale.lives: 2.091\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0133\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_30000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 247s 25ms/step - reward: 0.0133\n",
      "16 episodes - episode_reward: 8.438 [3.000, 17.000] - loss: 0.027 - mae: 0.402 - mean_q: 0.630 - mean_eps: 0.910 - ale.lives: 2.115\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0134\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_40000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 246s 25ms/step - reward: 0.0134\n",
      "15 episodes - episode_reward: 8.600 [4.000, 17.000] - loss: 0.023 - mae: 0.577 - mean_q: 0.830 - mean_eps: 0.874 - ale.lives: 2.246\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      " 8873/10000 [=========================>....] - ETA: 28s - reward: 0.0126\n",
      "🏆 Nueva mejor media móvil: 12.30\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0133\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_50000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 254s 25ms/step - reward: 0.0133\n",
      "11 episodes - episode_reward: 12.273 [6.000, 22.000] - loss: 0.023 - mae: 0.779 - mean_q: 1.067 - mean_eps: 0.838 - ale.lives: 2.191\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "  877/10000 [=>............................] - ETA: 3:27 - reward: 0.0091\n",
      "🏆 Nueva mejor media móvil: 12.40\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0131\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_60000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 246s 25ms/step - reward: 0.0132\n",
      "11 episodes - episode_reward: 11.545 [5.000, 23.000] - loss: 0.022 - mae: 1.010 - mean_q: 1.338 - mean_eps: 0.802 - ale.lives: 1.834\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0147\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_70000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 254s 25ms/step - reward: 0.0147\n",
      "16 episodes - episode_reward: 10.062 [4.000, 17.000] - loss: 0.022 - mae: 1.135 - mean_q: 1.480 - mean_eps: 0.766 - ale.lives: 2.073\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      " 8949/10000 [=========================>....] - ETA: 25s - reward: 0.0153\n",
      "🏆 Nueva mejor media móvil: 13.30\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0150\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_80000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 250s 25ms/step - reward: 0.0150\n",
      "12 episodes - episode_reward: 12.000 [2.000, 17.000] - loss: 0.021 - mae: 1.249 - mean_q: 1.610 - mean_eps: 0.730 - ale.lives: 2.051\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      " 3513/10000 [=========>....................] - ETA: 2:41 - reward: 0.0188\n",
      "🏆 Nueva mejor media móvil: 13.60\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0142\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_90000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 254s 25ms/step - reward: 0.0142\n",
      "13 episodes - episode_reward: 10.846 [3.000, 22.000] - loss: 0.020 - mae: 1.454 - mean_q: 1.851 - mean_eps: 0.694 - ale.lives: 2.005\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0154\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_100000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 238s 24ms/step - reward: 0.0154\n",
      "13 episodes - episode_reward: 11.769 [5.000, 20.000] - loss: 0.020 - mae: 1.527 - mean_q: 1.934 - mean_eps: 0.658 - ale.lives: 2.121\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0142\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_110000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 240s 24ms/step - reward: 0.0142\n",
      "15 episodes - episode_reward: 9.533 [3.000, 17.000] - loss: 0.018 - mae: 1.626 - mean_q: 2.047 - mean_eps: 0.622 - ale.lives: 2.042\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0139\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_120000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 232s 23ms/step - reward: 0.0139\n",
      "15 episodes - episode_reward: 9.133 [4.000, 17.000] - loss: 0.018 - mae: 1.639 - mean_q: 2.056 - mean_eps: 0.586 - ale.lives: 2.083\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0133\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_130000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 235s 24ms/step - reward: 0.0133\n",
      "15 episodes - episode_reward: 9.333 [2.000, 20.000] - loss: 0.018 - mae: 1.706 - mean_q: 2.133 - mean_eps: 0.550 - ale.lives: 2.137\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0150\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_140000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 235s 24ms/step - reward: 0.0150\n",
      "13 episodes - episode_reward: 11.231 [7.000, 24.000] - loss: 0.017 - mae: 1.736 - mean_q: 2.164 - mean_eps: 0.514 - ale.lives: 2.059\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0151\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_150000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 241s 24ms/step - reward: 0.0151\n",
      "13 episodes - episode_reward: 11.308 [4.000, 23.000] - loss: 0.016 - mae: 1.743 - mean_q: 2.168 - mean_eps: 0.478 - ale.lives: 2.051\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0145\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_160000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 231s 23ms/step - reward: 0.0145\n",
      "16 episodes - episode_reward: 9.375 [5.000, 20.000] - loss: 0.016 - mae: 1.812 - mean_q: 2.249 - mean_eps: 0.442 - ale.lives: 2.202\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      " 9561/10000 [===========================>..] - ETA: 10s - reward: 0.0168\n",
      "🏆 Nueva mejor media móvil: 14.90\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0167\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_170000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 234s 23ms/step - reward: 0.0167\n",
      "13 episodes - episode_reward: 12.769 [5.000, 26.000] - loss: 0.016 - mae: 1.884 - mean_q: 2.335 - mean_eps: 0.406 - ale.lives: 1.993\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0138\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_180000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 229s 23ms/step - reward: 0.0138\n",
      "16 episodes - episode_reward: 8.938 [1.000, 20.000] - loss: 0.017 - mae: 1.965 - mean_q: 2.430 - mean_eps: 0.370 - ale.lives: 2.118\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0151\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_190000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0151\n",
      "15 episodes - episode_reward: 9.600 [3.000, 20.000] - loss: 0.017 - mae: 2.039 - mean_q: 2.518 - mean_eps: 0.334 - ale.lives: 2.053\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0133\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_200000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0133\n",
      "16 episodes - episode_reward: 8.812 [3.000, 21.000] - loss: 0.016 - mae: 2.149 - mean_q: 2.648 - mean_eps: 0.298 - ale.lives: 2.145\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0162\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_210000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0162\n",
      "12 episodes - episode_reward: 13.167 [3.000, 20.000] - loss: 0.017 - mae: 2.177 - mean_q: 2.680 - mean_eps: 0.262 - ale.lives: 1.981\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0155\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_220000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 227s 23ms/step - reward: 0.0155\n",
      "13 episodes - episode_reward: 11.615 [5.000, 19.000] - loss: 0.016 - mae: 2.202 - mean_q: 2.708 - mean_eps: 0.226 - ale.lives: 1.894\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0152\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_230000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 236s 24ms/step - reward: 0.0152\n",
      "13 episodes - episode_reward: 11.154 [4.000, 24.000] - loss: 0.017 - mae: 2.281 - mean_q: 2.802 - mean_eps: 0.190 - ale.lives: 2.151\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0145\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_240000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 227s 23ms/step - reward: 0.0145\n",
      "16 episodes - episode_reward: 9.812 [4.000, 17.000] - loss: 0.016 - mae: 2.320 - mean_q: 2.848 - mean_eps: 0.154 - ale.lives: 1.906\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      " 6841/10000 [===================>..........] - ETA: 1:12 - reward: 0.0197\n",
      "🏆 Nueva mejor media móvil: 15.10\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0181\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_250000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 229s 23ms/step - reward: 0.0181\n",
      "14 episodes - episode_reward: 13.000 [5.000, 29.000] - loss: 0.016 - mae: 2.302 - mean_q: 2.825 - mean_eps: 0.118 - ale.lives: 2.092\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0139\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_260000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0139\n",
      "12 episodes - episode_reward: 11.000 [2.000, 20.000] - loss: 0.016 - mae: 2.281 - mean_q: 2.799 - mean_eps: 0.100 - ale.lives: 1.973\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0162\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_270000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0162\n",
      "16 episodes - episode_reward: 10.375 [4.000, 21.000] - loss: 0.016 - mae: 2.270 - mean_q: 2.785 - mean_eps: 0.100 - ale.lives: 2.010\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0147\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_280000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0147\n",
      "14 episodes - episode_reward: 10.714 [5.000, 20.000] - loss: 0.015 - mae: 2.254 - mean_q: 2.765 - mean_eps: 0.100 - ale.lives: 2.048\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      " 8381/10000 [========================>.....] - ETA: 36s - reward: 0.0173\n",
      "🏆 Nueva mejor media móvil: 15.40\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9085/10000 [==========================>...] - ETA: 20s - reward: 0.0171\n",
      "🏆 Nueva mejor media móvil: 15.70\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0169\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_290000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 229s 23ms/step - reward: 0.0169\n",
      "11 episodes - episode_reward: 15.000 [8.000, 30.000] - loss: 0.016 - mae: 2.269 - mean_q: 2.782 - mean_eps: 0.100 - ale.lives: 1.909\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0164\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_300000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 232s 23ms/step - reward: 0.0164\n",
      "14 episodes - episode_reward: 11.000 [1.000, 23.000] - loss: 0.015 - mae: 2.306 - mean_q: 2.829 - mean_eps: 0.100 - ale.lives: 2.045\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      " 5781/10000 [================>.............] - ETA: 1:34 - reward: 0.0195\n",
      "🏆 Nueva mejor media móvil: 15.80\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0180\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_310000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0180\n",
      "15 episodes - episode_reward: 12.867 [5.000, 24.000] - loss: 0.015 - mae: 2.316 - mean_q: 2.840 - mean_eps: 0.100 - ale.lives: 2.073\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0176\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_320000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 226s 23ms/step - reward: 0.0176\n",
      "13 episodes - episode_reward: 12.538 [3.000, 22.000] - loss: 0.016 - mae: 2.321 - mean_q: 2.847 - mean_eps: 0.100 - ale.lives: 2.022\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0155\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_330000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 229s 23ms/step - reward: 0.0155\n",
      "14 episodes - episode_reward: 11.714 [6.000, 21.000] - loss: 0.016 - mae: 2.405 - mean_q: 2.947 - mean_eps: 0.100 - ale.lives: 2.111\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0159\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_340000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 22ms/step - reward: 0.0159\n",
      "16 episodes - episode_reward: 9.688 [2.000, 22.000] - loss: 0.016 - mae: 2.427 - mean_q: 2.972 - mean_eps: 0.100 - ale.lives: 2.123\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0173\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_350000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0173\n",
      "15 episodes - episode_reward: 12.267 [4.000, 35.000] - loss: 0.015 - mae: 2.368 - mean_q: 2.898 - mean_eps: 0.100 - ale.lives: 2.144\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0184\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_360000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0184\n",
      "12 episodes - episode_reward: 14.833 [5.000, 28.000] - loss: 0.016 - mae: 2.375 - mean_q: 2.907 - mean_eps: 0.100 - ale.lives: 2.195\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0164\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_370000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0164\n",
      "13 episodes - episode_reward: 13.000 [5.000, 25.000] - loss: 0.015 - mae: 2.409 - mean_q: 2.946 - mean_eps: 0.100 - ale.lives: 2.035\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0156\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_380000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 22ms/step - reward: 0.0156\n",
      "14 episodes - episode_reward: 10.857 [3.000, 22.000] - loss: 0.015 - mae: 2.378 - mean_q: 2.907 - mean_eps: 0.100 - ale.lives: 2.034\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0174\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_390000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0174\n",
      "13 episodes - episode_reward: 13.231 [5.000, 25.000] - loss: 0.015 - mae: 2.430 - mean_q: 2.968 - mean_eps: 0.100 - ale.lives: 2.024\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0164\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_400000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 22ms/step - reward: 0.0164\n",
      "13 episodes - episode_reward: 12.769 [3.000, 27.000] - loss: 0.015 - mae: 2.472 - mean_q: 3.017 - mean_eps: 0.100 - ale.lives: 2.163\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0174\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_410000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 227s 23ms/step - reward: 0.0174\n",
      "14 episodes - episode_reward: 12.357 [3.000, 29.000] - loss: 0.016 - mae: 2.484 - mean_q: 3.030 - mean_eps: 0.100 - ale.lives: 2.115\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0154\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_420000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 22ms/step - reward: 0.0154\n",
      "16 episodes - episode_reward: 9.562 [5.000, 21.000] - loss: 0.015 - mae: 2.458 - mean_q: 2.995 - mean_eps: 0.100 - ale.lives: 2.013\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0157\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_430000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0157\n",
      "13 episodes - episode_reward: 12.308 [7.000, 19.000] - loss: 0.015 - mae: 2.492 - mean_q: 3.035 - mean_eps: 0.100 - ale.lives: 2.070\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0167\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_440000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 222s 22ms/step - reward: 0.0167\n",
      "13 episodes - episode_reward: 13.154 [5.000, 24.000] - loss: 0.015 - mae: 2.500 - mean_q: 3.042 - mean_eps: 0.100 - ale.lives: 1.990\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0157\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_450000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0157\n",
      "11 episodes - episode_reward: 11.909 [4.000, 19.000] - loss: 0.015 - mae: 2.493 - mean_q: 3.032 - mean_eps: 0.100 - ale.lives: 1.974\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0187\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_460000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 0.0187\n",
      "14 episodes - episode_reward: 14.929 [3.000, 30.000] - loss: 0.014 - mae: 2.536 - mean_q: 3.083 - mean_eps: 0.100 - ale.lives: 1.943\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0175\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_470000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0175\n",
      "15 episodes - episode_reward: 11.400 [4.000, 24.000] - loss: 0.015 - mae: 2.567 - mean_q: 3.117 - mean_eps: 0.100 - ale.lives: 1.998\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0177\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_480000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 22ms/step - reward: 0.0177\n",
      "15 episodes - episode_reward: 11.467 [3.000, 22.000] - loss: 0.014 - mae: 2.563 - mean_q: 3.111 - mean_eps: 0.100 - ale.lives: 1.950\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0163\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_490000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 22ms/step - reward: 0.0163\n",
      "16 episodes - episode_reward: 10.688 [3.000, 21.000] - loss: 0.015 - mae: 2.534 - mean_q: 3.075 - mean_eps: 0.100 - ale.lives: 2.058\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0173\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_500000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0173\n",
      "15 episodes - episode_reward: 11.600 [3.000, 22.000] - loss: 0.014 - mae: 2.497 - mean_q: 3.029 - mean_eps: 0.100 - ale.lives: 2.057\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      " 8041/10000 [=======================>......] - ETA: 43s - reward: 0.0208\n",
      "🏆 Nueva mejor media móvil: 17.10\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9205/10000 [==========================>...] - ETA: 17s - reward: 0.0204\n",
      "🏆 Nueva mejor media móvil: 18.40\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0200\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_510000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0200\n",
      "11 episodes - episode_reward: 17.455 [7.000, 29.000] - loss: 0.015 - mae: 2.415 - mean_q: 2.927 - mean_eps: 0.100 - ale.lives: 2.034\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      " 1133/10000 [==>...........................] - ETA: 3:18 - reward: 0.0203\n",
      "🏆 Nueva mejor media móvil: 19.00\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 2213/10000 [=====>........................] - ETA: 2:53 - reward: 0.0217\n",
      "🏆 Nueva mejor media móvil: 19.10\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0184\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_520000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0184\n",
      "12 episodes - episode_reward: 15.500 [6.000, 26.000] - loss: 0.015 - mae: 2.418 - mean_q: 2.931 - mean_eps: 0.100 - ale.lives: 2.008\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0182\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_530000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0183\n",
      "14 episodes - episode_reward: 13.571 [4.000, 24.000] - loss: 0.015 - mae: 2.472 - mean_q: 2.995 - mean_eps: 0.100 - ale.lives: 1.901\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0191\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_540000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 230s 23ms/step - reward: 0.0191\n",
      "11 episodes - episode_reward: 16.636 [7.000, 30.000] - loss: 0.015 - mae: 2.539 - mean_q: 3.075 - mean_eps: 0.100 - ale.lives: 2.044\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0191\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_550000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 0.0191\n",
      "14 episodes - episode_reward: 13.571 [4.000, 23.000] - loss: 0.015 - mae: 2.620 - mean_q: 3.174 - mean_eps: 0.100 - ale.lives: 1.952\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      " 8253/10000 [=======================>......] - ETA: 41s - reward: 0.0218\n",
      "🏆 Nueva mejor media móvil: 19.20\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0212\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_560000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 238s 24ms/step - reward: 0.0212\n",
      "12 episodes - episode_reward: 17.667 [7.000, 31.000] - loss: 0.015 - mae: 2.630 - mean_q: 3.187 - mean_eps: 0.100 - ale.lives: 2.022\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0191\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_570000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 0.0191\n",
      "14 episodes - episode_reward: 14.071 [5.000, 25.000] - loss: 0.015 - mae: 2.621 - mean_q: 3.175 - mean_eps: 0.100 - ale.lives: 2.062\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0193\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_580000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0193\n",
      "15 episodes - episode_reward: 12.867 [5.000, 23.000] - loss: 0.015 - mae: 2.624 - mean_q: 3.176 - mean_eps: 0.100 - ale.lives: 2.112\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0189\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_590000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0189\n",
      "11 episodes - episode_reward: 15.000 [7.000, 34.000] - loss: 0.016 - mae: 2.577 - mean_q: 3.119 - mean_eps: 0.100 - ale.lives: 2.035\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0183\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_600000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0183\n",
      "14 episodes - episode_reward: 14.071 [6.000, 31.000] - loss: 0.015 - mae: 2.567 - mean_q: 3.106 - mean_eps: 0.100 - ale.lives: 2.239\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0191\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_610000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0191\n",
      "13 episodes - episode_reward: 15.385 [5.000, 34.000] - loss: 0.015 - mae: 2.568 - mean_q: 3.109 - mean_eps: 0.100 - ale.lives: 2.127\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0169\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_620000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0169\n",
      "13 episodes - episode_reward: 13.231 [5.000, 26.000] - loss: 0.015 - mae: 2.589 - mean_q: 3.133 - mean_eps: 0.100 - ale.lives: 2.133\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0164\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_630000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0164\n",
      "11 episodes - episode_reward: 14.727 [5.000, 22.000] - loss: 0.016 - mae: 2.630 - mean_q: 3.184 - mean_eps: 0.100 - ale.lives: 2.050\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0177\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_640000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0177\n",
      "15 episodes - episode_reward: 12.200 [4.000, 35.000] - loss: 0.015 - mae: 2.641 - mean_q: 3.198 - mean_eps: 0.100 - ale.lives: 2.100\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0195\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_650000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 235s 24ms/step - reward: 0.0195\n",
      "14 episodes - episode_reward: 12.429 [0.000, 26.000] - loss: 0.015 - mae: 2.623 - mean_q: 3.174 - mean_eps: 0.100 - ale.lives: 2.189\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0186\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_660000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0187\n",
      "14 episodes - episode_reward: 14.571 [8.000, 22.000] - loss: 0.015 - mae: 2.609 - mean_q: 3.157 - mean_eps: 0.100 - ale.lives: 2.002\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0204\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_670000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0204\n",
      "10 episodes - episode_reward: 17.500 [9.000, 30.000] - loss: 0.014 - mae: 2.582 - mean_q: 3.126 - mean_eps: 0.100 - ale.lives: 2.006\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "  861/10000 [=>............................] - ETA: 3:24 - reward: 0.0139\n",
      "🏆 Nueva mejor media móvil: 19.90\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0181\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_680000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0181\n",
      "13 episodes - episode_reward: 15.692 [6.000, 45.000] - loss: 0.015 - mae: 2.592 - mean_q: 3.136 - mean_eps: 0.100 - ale.lives: 1.887\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0203\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_690000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 15.769 [11.000, 21.000] - loss: 0.015 - mae: 2.556 - mean_q: 3.093 - mean_eps: 0.100 - ale.lives: 2.026\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0191\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_700000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0191\n",
      "13 episodes - episode_reward: 14.000 [6.000, 21.000] - loss: 0.015 - mae: 2.496 - mean_q: 3.019 - mean_eps: 0.100 - ale.lives: 2.208\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      " 8321/10000 [=======================>......] - ETA: 37s - reward: 0.0220\n",
      "🏆 Nueva mejor media móvil: 20.00\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0213\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_710000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 226s 23ms/step - reward: 0.0213\n",
      "11 episodes - episode_reward: 19.636 [13.000, 32.000] - loss: 0.016 - mae: 2.516 - mean_q: 3.043 - mean_eps: 0.100 - ale.lives: 1.905\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      " 1229/10000 [==>...........................] - ETA: 3:16 - reward: 0.0187\n",
      "🏆 Nueva mejor media móvil: 20.30\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0204\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_720000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0204\n",
      "14 episodes - episode_reward: 15.571 [6.000, 27.000] - loss: 0.015 - mae: 2.539 - mean_q: 3.069 - mean_eps: 0.100 - ale.lives: 1.899\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0206\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_730000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0206\n",
      "13 episodes - episode_reward: 15.231 [6.000, 23.000] - loss: 0.015 - mae: 2.508 - mean_q: 3.027 - mean_eps: 0.100 - ale.lives: 2.129\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0197\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_740000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 236s 24ms/step - reward: 0.0197\n",
      "13 episodes - episode_reward: 13.923 [5.000, 29.000] - loss: 0.015 - mae: 2.520 - mean_q: 3.039 - mean_eps: 0.100 - ale.lives: 2.067\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0181\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_750000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0181\n",
      "12 episodes - episode_reward: 16.417 [4.000, 30.000] - loss: 0.015 - mae: 2.556 - mean_q: 3.081 - mean_eps: 0.100 - ale.lives: 2.068\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0212\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_760000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0212\n",
      "13 episodes - episode_reward: 16.615 [8.000, 25.000] - loss: 0.015 - mae: 2.583 - mean_q: 3.115 - mean_eps: 0.100 - ale.lives: 2.149\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0199\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_770000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 22ms/step - reward: 0.0199\n",
      "14 episodes - episode_reward: 13.786 [7.000, 29.000] - loss: 0.015 - mae: 2.569 - mean_q: 3.096 - mean_eps: 0.100 - ale.lives: 2.070\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0194\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_780000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0194\n",
      "11 episodes - episode_reward: 16.909 [8.000, 33.000] - loss: 0.016 - mae: 2.553 - mean_q: 3.078 - mean_eps: 0.100 - ale.lives: 2.179\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0209\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_790000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 227s 23ms/step - reward: 0.0209\n",
      "13 episodes - episode_reward: 17.154 [7.000, 27.000] - loss: 0.015 - mae: 2.543 - mean_q: 3.065 - mean_eps: 0.100 - ale.lives: 2.136\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0201\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_800000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0201\n",
      "14 episodes - episode_reward: 14.286 [7.000, 27.000] - loss: 0.015 - mae: 2.508 - mean_q: 3.025 - mean_eps: 0.100 - ale.lives: 2.125\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0200\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_810000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 229s 23ms/step - reward: 0.0200\n",
      "12 episodes - episode_reward: 16.250 [9.000, 26.000] - loss: 0.015 - mae: 2.488 - mean_q: 3.001 - mean_eps: 0.100 - ale.lives: 2.245\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0192\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_820000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 250s 25ms/step - reward: 0.0192\n",
      "12 episodes - episode_reward: 16.167 [7.000, 25.000] - loss: 0.015 - mae: 2.523 - mean_q: 3.042 - mean_eps: 0.100 - ale.lives: 2.023\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0196\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_830000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 259s 26ms/step - reward: 0.0196\n",
      "14 episodes - episode_reward: 13.643 [5.000, 28.000] - loss: 0.015 - mae: 2.506 - mean_q: 3.020 - mean_eps: 0.100 - ale.lives: 1.906\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0209\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_840000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 232s 23ms/step - reward: 0.0209\n",
      "12 episodes - episode_reward: 17.583 [8.000, 36.000] - loss: 0.015 - mae: 2.477 - mean_q: 2.985 - mean_eps: 0.100 - ale.lives: 2.172\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      " 4073/10000 [===========>..................] - ETA: 2:14 - reward: 0.0194\n",
      "🏆 Nueva mejor media móvil: 21.50\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 4517/10000 [============>.................] - ETA: 2:06 - reward: 0.0195\n",
      "🏆 Nueva mejor media móvil: 21.60\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 5297/10000 [==============>...............] - ETA: 1:49 - reward: 0.0193\n",
      "🏆 Nueva mejor media móvil: 21.70\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 6249/10000 [=================>............] - ETA: 1:32 - reward: 0.0190\n",
      "🏆 Nueva mejor media móvil: 22.00\n",
      "[Best] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_best_weights.h5f\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0192\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_850000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 246s 25ms/step - reward: 0.0192\n",
      "11 episodes - episode_reward: 18.273 [9.000, 26.000] - loss: 0.015 - mae: 2.514 - mean_q: 3.028 - mean_eps: 0.100 - ale.lives: 2.039\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0178\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_860000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0178\n",
      "13 episodes - episode_reward: 13.846 [4.000, 23.000] - loss: 0.015 - mae: 2.589 - mean_q: 3.120 - mean_eps: 0.100 - ale.lives: 2.203\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0211\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_870000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0211\n",
      "13 episodes - episode_reward: 16.231 [7.000, 29.000] - loss: 0.014 - mae: 2.603 - mean_q: 3.135 - mean_eps: 0.100 - ale.lives: 2.046\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0186\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_880000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0186\n",
      "12 episodes - episode_reward: 14.250 [7.000, 23.000] - loss: 0.015 - mae: 2.633 - mean_q: 3.172 - mean_eps: 0.100 - ale.lives: 2.067\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0163\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_890000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0163\n",
      "12 episodes - episode_reward: 14.333 [7.000, 23.000] - loss: 0.015 - mae: 2.680 - mean_q: 3.229 - mean_eps: 0.100 - ale.lives: 2.093\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0198\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_900000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0198\n",
      "14 episodes - episode_reward: 14.571 [5.000, 26.000] - loss: 0.016 - mae: 2.626 - mean_q: 3.163 - mean_eps: 0.100 - ale.lives: 2.076\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0207\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_910000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0207\n",
      "10 episodes - episode_reward: 19.600 [10.000, 28.000] - loss: 0.015 - mae: 2.568 - mean_q: 3.093 - mean_eps: 0.100 - ale.lives: 2.249\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0194\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_920000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 0.0194\n",
      "13 episodes - episode_reward: 14.462 [8.000, 22.000] - loss: 0.015 - mae: 2.560 - mean_q: 3.084 - mean_eps: 0.100 - ale.lives: 2.000\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0190\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_930000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 228s 23ms/step - reward: 0.0191\n",
      "12 episodes - episode_reward: 15.583 [8.000, 26.000] - loss: 0.015 - mae: 2.535 - mean_q: 3.055 - mean_eps: 0.100 - ale.lives: 2.071\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0183\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_940000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0183\n",
      "13 episodes - episode_reward: 15.154 [4.000, 33.000] - loss: 0.015 - mae: 2.484 - mean_q: 2.992 - mean_eps: 0.100 - ale.lives: 2.104\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0200\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_950000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 235s 23ms/step - reward: 0.0200\n",
      "13 episodes - episode_reward: 14.923 [5.000, 24.000] - loss: 0.015 - mae: 2.482 - mean_q: 2.991 - mean_eps: 0.100 - ale.lives: 1.899\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0185\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_960000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0185\n",
      "13 episodes - episode_reward: 14.923 [6.000, 31.000] - loss: 0.014 - mae: 2.459 - mean_q: 2.962 - mean_eps: 0.100 - ale.lives: 2.020\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0203\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_970000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 225s 23ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 15.846 [4.000, 27.000] - loss: 0.014 - mae: 2.489 - mean_q: 3.000 - mean_eps: 0.100 - ale.lives: 2.123\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0200\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_980000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0200\n",
      "11 episodes - episode_reward: 16.727 [8.000, 32.000] - loss: 0.015 - mae: 2.549 - mean_q: 3.071 - mean_eps: 0.100 - ale.lives: 2.164\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0184\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_990000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 223s 22ms/step - reward: 0.0184\n",
      "11 episodes - episode_reward: 17.818 [10.000, 25.000] - loss: 0.015 - mae: 2.570 - mean_q: 3.099 - mean_eps: 0.100 - ale.lives: 2.231\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      " 9997/10000 [============================>.] - ETA: 0s - reward: 0.0211\n",
      "[Checkpoint] Pesos guardados en: ./models\\ddqn.v5.1\\checkpoints\\dqn_SpaceInvaders-v0_weights_1000000.h5f\n",
      "\n",
      "[Checkpoint] Memoria guardada de forma segura en: ./models\\ddqn.v5.1\\checkpoints\\memory.pkl\n",
      "10000/10000 [==============================] - 224s 22ms/step - reward: 0.0211\n",
      "done, took 22751.834 seconds\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T07:04:37.813710Z",
     "start_time": "2025-07-02T07:04:37.800712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SlowTestCallback(Callback):\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        time.sleep(0.0025)\n",
    "\n",
    "class CaptureFramesCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.all_frames = []              # Lista de listas: frames por episodio\n",
    "        self.episode_frames = []\n",
    "        self.episode_rewards = []\n",
    "        self.current_reward = 0\n",
    "\n",
    "    def on_episode_begin(self, episode, logs={}):\n",
    "        self.episode_frames = []\n",
    "        self.current_reward = 0\n",
    "\n",
    "    def on_step_end(self, step, logs={}):\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        self.episode_frames.append(frame)\n",
    "        self.current_reward += logs.get('reward', 0)\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        self.all_frames.append(self.episode_frames)\n",
    "        self.episode_rewards.append(self.current_reward)\n",
    "\n",
    "def test(agent=dqn, episodes=10, weights='full', video=False):\n",
    "    # Carga de pesos\n",
    "    if weights == 'last':\n",
    "        _, _, latest_checkpoint = load_last_checkpoint(dqn)\n",
    "        dqn.load_weights(latest_checkpoint)\n",
    "    elif weights == 'best':\n",
    "        dqn.load_weights(f\"./models/{model_name}/weights/dqn_SpaceInvaders-v0_best_weights.h5f\")\n",
    "    else:\n",
    "        dqn.load_weights(f\"./models/{model_name}/weights/dqn_SpaceInvaders-v0_weights_{model_name}.h5f\")\n",
    "\n",
    "    # Instancia de callbacks\n",
    "    capture_cb = CaptureFramesCallback()\n",
    "    slow_cb = SlowTestCallback()\n",
    "\n",
    "    # Ejecuta test una sola vez, grabando y ralentizando\n",
    "    history = dqn.test(env, nb_episodes=episodes, visualize=False, callbacks=[capture_cb, slow_cb])\n",
    "\n",
    "    # Encuentra el mejor episodio\n",
    "    best_idx = int(np.argmax(capture_cb.episode_rewards))\n",
    "    average_reward = np.mean(capture_cb.episode_rewards)\n",
    "    best_reward = capture_cb.episode_rewards[best_idx]\n",
    "    print(f\"Recompensa media sobre 10 episodios: {average_reward}\")\n",
    "    print(f\"Mejor episodio: #{best_idx + 1} con recompensa: {best_reward}\")\n",
    "\n",
    "    # Guarda el MP4 del mejor episodio\n",
    "    if video:\n",
    "        mp4_path = os.path.join(MODEL_DIR, f'{model_name}_best_test_episode.mp4')\n",
    "        with imageio.get_writer(mp4_path, fps=30, codec='libx264', macro_block_size=1) as writer:\n",
    "            for frame in capture_cb.all_frames[best_idx]:\n",
    "                writer.append_data(frame)\n",
    "\n",
    "        print(f\"Video del mejor episodio guardado en {mp4_path}\")\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T07:04:29.417691Z",
     "start_time": "2025-07-02T07:02:14.841997Z"
    }
   },
   "cell_type": "code",
   "source": "test(agent=dqn, episodes=10, video=False)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 11.000, steps: 492\n",
      "Episode 2: reward: 24.000, steps: 1018\n",
      "Episode 3: reward: 27.000, steps: 1111\n",
      "Episode 4: reward: 29.000, steps: 1154\n",
      "Episode 5: reward: 16.000, steps: 727\n",
      "Episode 6: reward: 13.000, steps: 688\n",
      "Episode 7: reward: 14.000, steps: 762\n",
      "Episode 8: reward: 21.000, steps: 1059\n",
      "Episode 9: reward: 8.000, steps: 510\n",
      "Episode 10: reward: 22.000, steps: 1123\n",
      "Recompensa media sobre 10 episodios: 18.5\n",
      "Mejor episodio: #4 con recompensa: 29.0\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJ99jw3yktPw"
   },
   "source": "## Double-DDQN + Prioritized Expierence Replay (PER) + LR con PolynomialDecay"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Red neuronal basada en el paper: [https://arxiv.org/abs/1511.05952](https://arxiv.org/abs/1511.05952)\n",
    "\n",
    "**Prioritized Experience Replay (PER)**\n",
    "**Autores:** Tom Schaul, John Quan, Ioannis Antonoglou, David Silver.\n",
    "**Año de publicación:** 2015"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "El método **Prioritized Experience Replay** introduce una **mejora en la estrategia de muestreo de experiencias** en el buffer de replay. En lugar de seleccionar transiciones de manera uniforme, como en el replay clásico, PER asigna mayor probabilidad de ser seleccionadas a aquellas transiciones que tienen **mayor error de predicción temporal (TD error)**. El razonamiento detrás de esta idea es que **no todas las experiencias son igualmente útiles para el aprendizaje**. Aquellas con mayor error TD indican que la red aún no ha aprendido adecuadamente de ellas, por lo tanto, deberían ser muestreadas con mayor frecuencia.\n",
    "\n",
    "Para implementar esta estrategia, se calcula una prioridad $p_i$ para cada transición $i$ basada en su TD error: $p_i = |\\delta_i| + \\epsilon$, donde $\\delta_i$ es el error TD y $\\epsilon$ es un pequeño valor positivo que asegura que todas las transiciones tengan una probabilidad distinta de cero. La probabilidad de muestreo de cada transición se define como: $P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}$, donde $\\alpha$ controla hasta qué punto se prioriza (si $\\alpha = 0$, se recupera el muestreo uniforme).\n",
    "\n",
    "Para **compensar el sesgo introducido por este muestreo no uniforme**, se aplican **pesos de importancia** (importance sampling weights) durante la actualización de la red, definidos como:\n",
    "$w_i = \\left( \\frac{1}{N \\cdot P(i)} \\right)^\\beta$, donde $N$ es el tamaño del buffer de replay y $\\beta$ regula el grado de corrección (se incrementa progresivamente durante el entrenamiento).\n",
    "\n",
    "Lo que se pretende introduciendo este mecanismo es:\n",
    "\n",
    "- Acelerar el aprendizaje al centrarse en experiencias más informativas.\n",
    "- Mantener una exploración adecuada del espacio de experiencias.\n",
    "- Mejorar el rendimiento en entornos donde las recompensas son escasas o la dinámica es compleja.\n",
    "\n",
    "Según la listeratura, PER ha demostrado ser especialmente útil cuando se combina con DQN, Double DQN o Dueling DQN.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Descripción del modelo Double Dueling DQN + PER + LR Polynomial Decay\n",
    "\n",
    "El modelo desarrollado para este experimento combina la arquitectura **Double Dueling DQN** con la técnica de **Prioritized Experience Replay (PER)** para mejorar la eficiencia del aprendizaje. La estrategia de muestreo está basada en la implementación disponible en el siguiente repositorio: [Howuhh/prioritized_experience_replay](https://github.com/Howuhh/prioritized_experience_replay), adaptada para integrarse con el stack de librerías utilizado en este notebook.\n",
    "\n",
    "Esta implementación se compone de dos clases principales:\n",
    "\n",
    "- **`SumTree`**: una estructura de árbol binario que permite seleccionar y actualizar muestras de experiencia con una probabilidad proporcional a su prioridad, todo ello en tiempo logarítmico. Cada nodo del árbol almacena la suma de prioridades de sus nodos hijos.\n",
    "\n",
    "- **`PERMemory`**: gestiona el almacenamiento de experiencias en forma de tuplas (estado, acción, recompensa, terminal, siguiente estado), utilizando el `SumTree` para asignar mayor probabilidad de muestreo a aquellas transiciones con mayor error temporal de diferencia (**TD error**).\n",
    "\n",
    "Con este enfoque se pretende que el agente se centre en el aprendizaje en experiencias más relevantes, ajustando las prioridades dinámicamente tras cada paso de entrenamiento mediante el método `update_priorities`.\n",
    "\n",
    "También se ha añadido una estrategia de decaimiento de la tasa de aprendizaje basada en la función `PolynomialDecay` de TensorFlow. Esta técnica permite reducir progresivamente la tasa de aprendizaje durante el entrenamiento, comenzando con un valor inicial (`initial_learning_rate`) y descendiendo de forma lineal (con `power=1.0`) hasta un valor final (`end_learning_rate`) a lo largo de un número determinado de pasos (`decay_steps`).\n",
    "\n",
    "El uso de un decaimiento lineal ayuda a estabilizar el proceso de entrenamiento: se favorece una exploración más agresiva al inicio del aprendizaje (tasa alta), y una convergencia más precisa en fases posteriores (tasa baja), lo cual es especialmente beneficioso en entornos de alto ruido o complejidad como el entorno que propone esta práctica.\n",
    "\n",
    "Esta estrategia está inspirada en prácticas comunes en entrenamiento de redes neuronales profundas, y ha sido utilizada en múltiples trabajos de referencia, incluyendo:\n",
    "\n",
    "- **Mnih et al. (2015)** – *Human-level control through deep reinforcement learning* [[arXiv:1312.5602](https://arxiv.org/abs/1312.5602)]\n",
    "- **Hessel et al. (2018)** – *Rainbow: Combining Improvements in Deep Reinforcement Learning* [[arXiv:1710.02298](https://arxiv.org/abs/1710.02298)]\n",
    "\n",
    "La implementación concreta sigue las recomendaciones de TensorFlow para control fino del `learning rate`, utilizando la clase [`tf.keras.optimizers.schedules.PolynomialDecay`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PolynomialDecay)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. Implementación de la red neuronal"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_name = \"ddqn.v5.3\"\n",
    "model = getDuelingDQN(model_name)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. Implementación de la solución Double-DDQN + PER + LR Polynomial Decay"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En esta ocasión se ha decidido variar algunos hiperparámetros con respecto al anterior experimento que se comentan a continuación:\n",
    "\n",
    "- Se aumentó el tamaño del buffer de memoria de 50,000 a 1,000,000 experiencias y el período de calentamiento (warmup) de 10,000 a 50,000 pasos para almacenar y aprender de una mayor variedad de experiencias.\n",
    "- Se extendió la duración total de entrenamiento de 1,000,000 a 1,750,000 pasos, así como el número de pasos considerados en la programación de aprendizaje (scheduler steps), pasando de 250,000 a 1,000,000, para aprovechar mejor el mayor volumen de datos.\n",
    "- Se incrementó la tasa de aprendizaje inicial de 0.00001 a 0.00025 y se activó un decaimiento lineal programado para acelerar el aprendizaje inicial y mejorar la estabilidad a lo largo del entrenamiento.\n",
    "- Se redujo la frecuencia de entrenamiento por paso, pasando de entrenar cada 3 pasos a cada 4 pasos, y se aumentó la frecuencia de actualización de la red objetivo (target network) de cada 5,000 a cada 10,000 pasos para favorecer la estabilidad y optimizar el uso de recursos en un entrenamiento más largo y complejo.\n",
    "- Se activaron mecanismos adicionales como Prioritized Experience Replay (PER) para priorizar las experiencias más relevantes y mejorar la eficiencia del aprendizaje.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. Implementaciones de las clases para PER"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.write = 0\n",
    "        self.n_entries = 0\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        self.write = (self.write + 1) % self.capacity\n",
    "        self.n_entries = min(self.n_entries + 1, self.capacity)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PERMemory(Memory):\n",
    "    Experience = namedtuple('Experience', ['state0', 'action', 'reward', 'terminal1', 'state1'])\n",
    "\n",
    "    def __init__(self, capacity, env_obs_shape, window_length=4, alpha=0.6, epsilon=1e-6):\n",
    "        super(PERMemory, self).__init__(window_length=window_length)\n",
    "        self.capacity = capacity\n",
    "        self.env_obs_shape = env_obs_shape\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "        self.observations = np.zeros((capacity,) + self.env_obs_shape, dtype=np.uint8)\n",
    "        self.actions = np.zeros((capacity,), dtype=np.int32)\n",
    "        self.rewards = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.terminals = np.zeros((capacity,), dtype=np.bool)\n",
    "        self.pos = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        experience = (self.pos, observation, action, reward, terminal)\n",
    "        priority = self.max_priority\n",
    "        self.tree.add(priority, experience)\n",
    "\n",
    "        self.observations[self.pos] = observation\n",
    "        self.actions[self.pos] = action\n",
    "        self.rewards[self.pos] = reward\n",
    "        self.terminals[self.pos] = terminal\n",
    "\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "        if self.size < self.capacity:\n",
    "            self.size += 1\n",
    "\n",
    "    def _get_state(self, idx):\n",
    "        if self.size == 0:\n",
    "            return np.zeros((self.window_length,) + self.env_obs_shape, dtype=np.uint8)\n",
    "\n",
    "        indices = []\n",
    "        for i in reversed(range(self.window_length)):\n",
    "            cur_idx = (idx - i) % self.size\n",
    "            if i >= self.size:\n",
    "                indices.append(0)\n",
    "            else:\n",
    "                indices.append(cur_idx)\n",
    "\n",
    "        # Corte de episodios si hay un terminal en el medio\n",
    "        for i in range(1, self.window_length):\n",
    "            terminal_value = self.terminals[indices[-i]]\n",
    "            if isinstance(terminal_value, np.ndarray):\n",
    "                if terminal_value.size == 1:\n",
    "                    terminal_value = terminal_value.item()\n",
    "                else:\n",
    "                    terminal_value = terminal_value.any()\n",
    "            if terminal_value:\n",
    "                indices = indices[-i+1:] + [indices[-1]] * (i - 1)\n",
    "                break\n",
    "\n",
    "        # Recolectar los frames y apilarlos como array 3D\n",
    "        frames = [self.observations[i] for i in indices]\n",
    "\n",
    "        # Asegura que siempre haya self.window_length frames\n",
    "        while len(frames) < self.window_length:\n",
    "            frames.insert(0, np.zeros(self.env_obs_shape, dtype=np.uint8))\n",
    "\n",
    "        return np.array(frames, dtype=np.uint8)\n",
    "\n",
    "\n",
    "    def get_recent_state(self, observation):\n",
    "        state = list(self.recent_observations)\n",
    "        while len(state) < self.window_length - 1:\n",
    "            state.insert(0, np.zeros(self.env_obs_shape, dtype=np.uint8))\n",
    "        state.append(observation)\n",
    "        return np.array(state)\n",
    "\n",
    "    def sample(self, batch_size, **kwargs):\n",
    "        if self.size < self.window_length:\n",
    "            raise ValueError(\"Not enough entries to sample\")\n",
    "\n",
    "        batch = []\n",
    "        segment = self.tree.total() / batch_size\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            s = np.random.uniform(segment * i, segment * (i + 1))\n",
    "            idx, p, data = self.tree.get(s)\n",
    "            pos = data[0]\n",
    "\n",
    "            state0 = self._get_state((pos - 1) % self.size)\n",
    "            state1 = self._get_state(pos)\n",
    "\n",
    "            # Asegurarse que ambos son arrays del mismo dtype y forma\n",
    "            state0 = np.array(state0, dtype=np.uint8)\n",
    "            state1 = np.array(state1, dtype=np.uint8)\n",
    "\n",
    "            action = self.actions[pos]\n",
    "            reward = self.rewards[pos]\n",
    "            terminal1 = self.terminals[pos]\n",
    "\n",
    "            experience = self.Experience(state0, action, reward, terminal1, state1)\n",
    "            batch.append(experience)\n",
    "\n",
    "\t\t# Verificación de formas y tipos\n",
    "        for i, e in enumerate(batch):\n",
    "            if not isinstance(e.state0, np.ndarray) or e.state0.shape != (self.window_length,) + self.env_obs_shape:\n",
    "                print(f\"[ERROR] state0 en idx {i}: shape {e.state0.shape}, tipo: {type(e.state0)}\")\n",
    "            if not isinstance(e.state1, np.ndarray) or e.state1.shape != (self.window_length,) + self.env_obs_shape:\n",
    "                print(f\"[ERROR] state1 en idx {i}: shape {e.state1.shape}, tipo: {type(e.state1)}\")\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def update_priorities(self, idxs, errors):\n",
    "            print(f\"[DEBUG] idxs: {idxs}, errors: {errors}\")\n",
    "            for idx, error in zip(idxs, errors):\n",
    "                p = (np.abs(error) + self.epsilon) ** self.alpha\n",
    "                self.tree.update(idx, p)\n",
    "                self.max_priority = max(self.max_priority, p)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PERUpdateCallback(Callback):\n",
    "    def __init__(self, memory):\n",
    "        self.memory = memory\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        # Actualizar los últimos TD-errors e índices en la memoria\n",
    "        if hasattr(self.memory, 'last_indices') and hasattr(self.memory, 'last_td_errors'):\n",
    "            self.memory.update_priorities(self.memory.last_indices, self.memory.last_td_errors)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROI-mt2jk5za",
    "outputId": "1bcb5d10-58a3-4ac3-9790-1777e6401035"
   },
   "source": [
    "# GENERACIÓN O CARGA MODELO\n",
    "MODEL_DIR, WEIGHTS_DIR, CHECKPOINTS_DIR = get_dirs(model_name)\n",
    "load_hyperparams(model_name)\n",
    "\n",
    "# Añado los hiperparametros para el modelo dueling\n",
    "hiperparametros = {\n",
    "    \"MEMORY_SIZE\": 1000000,\n",
    "    \"WARMUP_STEPS\": 50000,\n",
    "    \"SCHEDULER_STEPS\": 1000000,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"MODEL_UPDATE\": 10000,\n",
    "    \"LEARNING_RATE\": 0.00025,\n",
    "    \"MODEL_CHECKPOINT_STEPS\": 50000,\n",
    "    \"TRAIN_STEPS\": 1750000,\n",
    "    \"TRAIN_INTERVAL\": 4,\n",
    "    \"LOG_INTERVAL\": 10000,\n",
    "    \"DELTA_CLIP\": 1.0,\n",
    "    \"ENABLE_DOUBLE_DQN\": True,\n",
    "    \"ENABLE_DUELING_NETWORK\": False,\n",
    "    \"EPSILON_START\": 1.0,\n",
    "    \"EPSILON_MIN\": 0.1,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "\t\"ENABLE_LR_SCHD\": True,\n",
    "\t\"LR_SCHD_INITIAL_LR\": 0.00025,\n",
    "\t\"ENABLE_PER_MEMORY\": True\n",
    "}\n",
    "\n",
    "# Guardar cambios\n",
    "save_hyperparams(model_name)\n",
    "\n",
    "# Volver a cargar para crear las variables globales nuevas\n",
    "load_hyperparams(model_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCXRFjA6pcgr",
    "outputId": "b5290c0c-3eff-442c-99e4-3c4366c6ef9a"
   },
   "source": [
    "# DEFINICIÓN DE LA POLICY\n",
    "memory = None\n",
    "memory, last_checkpoint_steps=get_memory_and_last_steps()\n",
    "\n",
    "value_max, value_min, new_scheduler_steps = adjust_policy_params(\n",
    "    scheduler_steps=SCHEDULER_STEPS,\n",
    "    train_steps=TRAIN_STEPS,\n",
    "    current_step=last_checkpoint_steps,\n",
    "    value_max=EPSILON_START,\n",
    "    value_min=EPSILON_MIN\n",
    ")\n",
    "\n",
    "if not memory:\n",
    "    if ENABLE_PER_MEMORY:\n",
    "        memory = PERMemory(capacity=MEMORY_SIZE, window_length=WINDOW_LENGTH, env_obs_shape=INPUT_SHAPE)\n",
    "    else:\n",
    "        memory = SequentialMemory(limit=MEMORY_SIZE, window_length=WINDOW_LENGTH)\n",
    "    print(\"Memoria inicializada de 0\")\n",
    "\n",
    "\n",
    "print(f\"Valores de la policy: value_min={value_min}, value_max={value_max}, scheduler_steps={new_scheduler_steps}\")\n",
    "\n",
    "processor = AtariProcessor()\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
    "                              value_max=EPSILON_START,\n",
    "                              value_min=EPSILON_MIN,\n",
    "                              value_test=.05,\n",
    "                              nb_steps=new_scheduler_steps)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bWEJijiophgL"
   },
   "source": [
    "# Agente\n",
    "dqn = DQNAgent(model=model,\n",
    "                     nb_actions=nb_actions,\n",
    "                     policy=policy,\n",
    "                     memory=memory,\n",
    "                     processor=processor, # Aquí se le pasa el procesador inalterado\n",
    "                     nb_steps_warmup=WARMUP_STEPS,\n",
    "                     gamma=GAMMA,\n",
    "                     target_model_update=MODEL_UPDATE,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     train_interval=TRAIN_INTERVAL, # Usar TRAIN_INTERVAL desde hiperparametros\n",
    "                     delta_clip=DELTA_CLIP,\n",
    "                     enable_double_dqn=ENABLE_DOUBLE_DQN,\n",
    "                     enable_dueling_network=ENABLE_DUELING_NETWORK)\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Parche para evitar el error 'get_updates' que ya no existe\n",
    "    def patched_get_updates(self, loss, params):\n",
    "        return []\n",
    "    Adam.get_updates = patched_get_updates"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. Politica de decaimiento de la tasa de aprendizaje"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if ENABLE_LR_SCHD:\n",
    "\n",
    "  lr_schedule = schedules.PolynomialDecay(\n",
    "      initial_learning_rate=LR_SCHD_INITIAL_LR,\n",
    "      decay_steps=TRAIN_STEPS,\n",
    "      end_learning_rate=LEARNING_RATE,\n",
    "      power=1.0  # Decaimiento lineal\n",
    "  )\n",
    "\n",
    "  dqn.compile(\n",
    "      Adam(learning_rate=lr_schedule),\n",
    "      metrics=['mae']\n",
    "      )\n",
    "\n",
    "  print(\"Learning Rate Scheduler activado\")\n",
    "else:\n",
    "\tdqn.compile(\n",
    "      Adam(learning_rate=LEARNING_RATE),\n",
    "      metrics=['mae']\n",
    "      )\n",
    "\tprint(\"Learning Rate Scheduler desactivado\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uhuKAF1UqLV1"
   },
   "source": [
    "weights_filename = os.path.join(WEIGHTS_DIR, 'dqn_{}_weights_{}.h5f'.format(env_name, model_name))\n",
    "checkpoint_weights_filename = os.path.join(CHECKPOINTS_DIR, 'dqn_' + env_name + '_weights_{step}.h5f')\n",
    "log_filename =os.path.join(MODEL_DIR, 'dqn_{}_log_{}.json'.format(env_name, model_name))\n",
    "log_csv_path = os.path.join(MODEL_DIR, f'{model_name}_training_log.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CARGAR PESOS DEL ÚLTIMO CHECKPOINT SI EXISTE\n",
    "#dqn, last_checkpoint_steps = load_last_checkpoint(dqn)\n",
    "dqn, last_checkpoint_steps,latest_checkpoint = load_last_checkpoint(dqn)\n",
    "# CREAR CALLBACKS CUSTOMIZADOS QUE GUARDAN BIEN LOS CHECKPOINTS Y LOGS DEL TRAINING\n",
    "checkpoint_callback = AccumulatedCheckpoint(\n",
    "      base_path=CHECKPOINTS_DIR,\n",
    "      env_name=env_name,\n",
    "      interval=MODEL_CHECKPOINT_STEPS,\n",
    "      initial_step=last_checkpoint_steps\n",
    "  )\n",
    "\n",
    "dqn.step = last_checkpoint_steps\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [checkpoint_callback]\n",
    "callbacks += [EpisodeLoggerCSV(filepath=log_csv_path, verbose=False)]\n",
    "callbacks += [PERUpdateCallback(memory)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QkbL3l1dqShn",
    "outputId": "2268777e-b173-4e9a-f72b-ffab3ff21b96"
   },
   "source": [
    "print(\"Hiperparámetros de\", model_name)\n",
    "print(json.dumps(hiperparametros, indent=4))\n",
    "\n",
    "if TRAIN_STEPS>0:\n",
    "  dqn.fit(env, callbacks=callbacks, nb_steps=TRAIN_STEPS-last_checkpoint_steps, log_interval=LOG_INTERVAL, visualize=False)\n",
    "  dqn.save_weights(weights_filename, overwrite=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a, b,latest_checkpoint= load_last_checkpoint(dqn)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Carga de pesos\n",
    "_, _,latest_checkpoint= load_last_checkpoint(dqn)\n",
    "dqn.load_weights(latest_checkpoint)\n",
    "# dqn.load_weights(\"./models/ddqn.v5.3/weights/dqn_SpaceInvaders-v0_best_weights.h5f\")\n",
    "history=dqn.test(env, nb_episodes=10, visualize=True)\n",
    "episode_rewards = history.history['episode_reward']\n",
    "average_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "print(f\"Recompensa media sobre 10 episodios: {average_reward}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8 (TF 2.5.3 + GPU)",
   "language": "python",
   "name": "venv_tf253"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
